{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "© 2025 Vanargo · License: MIT. See the `LICENSE` file in the repository root."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# --- 02. Modeling: Baselines, Tuning, Evaluation --- #\n",
    "\n",
    "**Goal.** Build and compare several classification algorithms on a unified preprocessing pipeline, select the best model according to the target metrics set, and save artifacts for subsequent fairness and interpretability audit (see `03_fairness_and_explainability.ipynb`).\n",
    "\n",
    "**Inputs:**\n",
    "1. Final dataset from `01_data_loading_and_eda.ipynb`.\n",
    "2. Explicit feature lists: `num_features`, `cat_features`.\n",
    "\n",
    "**Approach:**\n",
    "1. Unified `ColumnTransformer`: numerical -> `SimpleImputer(median)` -> `StandardScaler`; categorical -> `SimpleImputer(most_frequent)` -> `OneHotEncoder(handle_unknown='ignore', sparse_output=True)`.\n",
    "2. Models: Logistic Regression, Decision Tree, Random Forest, XGBoost (with early stopping), LightGBM (with RandomizedSearch).\n",
    "3. Validation: stratified splits, fixed `random_state`.\n",
    "4. Primary comparison metrics: ROC-AUC, F1, Accuracy; additional — Precision, Recall, PR-AUC (where applicable).\n",
    "\n",
    "**Outputs:**\n",
    "1. Summary metrics table across models and a comparison plot.\n",
    "2. Best model with a fully assembled preprocessing pipeline.\n",
    "3. Artifacts for stage 03: `y_true_test`, `y_proba_best`, `y_pred_best`, raw copies of sensitive features, list of OHE features, serialized model/preprocessor objects, and library version snapshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & global config --- #\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# stdlib #\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# third-party #\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn import set_config\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# detailed pipeline and DataFrame display configuration #\n",
    "set_config(transform_output=\"pandas\", display=\"diagram\")\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "\n",
    "# determenism #\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# plotting style #\n",
    "sns.set(context=\"notebook\")\n",
    "\n",
    "# warning filters for a clean log #\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"lightgbm\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")\n",
    "\n",
    "# brief version report #\n",
    "print(\n",
    "    \"[versions]\",\n",
    "    f\"numpy={np.__version__}; pandas={pd.__version__}; \"\n",
    "    f\"sklearn={(__import__('sklearn').__version__)}; \"\n",
    "    f\"xgboost={xgb.__version__}; lightgbm={lgb.__version__}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Notebook preamble: silence & style --- #\n",
    "\n",
    "import warnings\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# visual style #\n",
    "sns.set(context=\"notebook\", style=\"whitegrid\")\n",
    "plt.rcParams[\"axes.spines.top\"] = False\n",
    "plt.rcParams[\"axes.spines.right\"] = False\n",
    "\n",
    "# general warning filters #\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "print(\"[init] visual style and warning filters applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Project paths bootstrap (detect & sys.path) --- #\n",
    "\n",
    "# detect project root by marker files #\n",
    "DETECTED_ROOT = Path.cwd()\n",
    "_MARKERS = {\".git\", \"pyproject.toml\", \"README.md\"}\n",
    "while (\n",
    "    not any((DETECTED_ROOT / m).exists() for m in _MARKERS)\n",
    "    and DETECTED_ROOT.parent != DETECTED_ROOT\n",
    "):\n",
    "    DETECTED_ROOT = DETECTED_ROOT.parent\n",
    "\n",
    "# add the root to sys.path once #\n",
    "root_str = str(DETECTED_ROOT.resolve())\n",
    "if root_str not in sys.path:\n",
    "    sys.path.append(root_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Project paths imports --- #\n",
    "\n",
    "from paths import (\n",
    "    ART_DIR,\n",
    "    DATA_DIR,\n",
    "    MODELS_DIR,\n",
    "    REPORTS_DIR,\n",
    ")\n",
    "from paths import (\n",
    "    ROOT as PATHS_ROOT,\n",
    ")\n",
    "\n",
    "# verify consistency between detected ROOT and paths.ROOT #\n",
    "assert PATHS_ROOT.resolve() == DETECTED_ROOT.resolve(), (\n",
    "    f\"paths.ROOT={PATHS_ROOT} != detected ROOT={DETECTED_ROOT}\"\n",
    ")\n",
    "\n",
    "# brief report #\n",
    "print(f\"[paths] ROOT={DETECTED_ROOT}\")\n",
    "print(f\"[paths] DATA_DIR={DATA_DIR}\")\n",
    "print(f\"[paths] MODELS_DIR={MODELS_DIR}\")\n",
    "print(f\"[paths] ART_DIR={ART_DIR}\")\n",
    "print(f\"[paths] REPORTS_DIR={REPORTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# --- Data Split --- #\n",
    "\n",
    "**Goal.** Obtain a reproducible train-test split while preserving stratification by the target variable `income`.\n",
    "\n",
    "**Context.** The dataset `df_ready` was created in `01_data_loading_and_eda.ipynb` after feature cleaning and encoding. It contains 14 features (a mix of numerical and categorical) and a binary target variable `income`.\n",
    "\n",
    "**Approach:**\n",
    "1. Use `train_test_split` from `sklearn.model_selection`.\n",
    "2. Split ratio: 80% training, 20% test.\n",
    "3. Stratification: `stratify=y` to preserve the original class distribution.\n",
    "4. Fix `random_state=42` for full reproducibility.\n",
    "5. Save separately:\n",
    "   - `X_train`, `X_test` — features without the target variable;  \n",
    "   - `y_train`, `y_test` — target label.\n",
    "6. Later, `X_test` is reused to form control subsets for fairness audits (see `03_fairness_and_explainability.ipynb`).\n",
    "\n",
    "**Conclusion.** The split ensures correct class proportions and prevents information leakage from the test set into training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Split --- #\n",
    "\n",
    "from paths import PROC_DIR\n",
    "\n",
    "# robust load: parquet -> csv fallback #\n",
    "p_parq = PROC_DIR / \"adult_eda.parquet\"\n",
    "p_csv = PROC_DIR / \"adult_eda.csv\"\n",
    "if p_parq.exists():\n",
    "    df = pd.read_parquet(p_parq)\n",
    "elif p_csv.exists():\n",
    "    df = pd.read_csv(p_csv)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"File not found: {p_parq} or {p_csv}\")\n",
    "\n",
    "# target variable #\n",
    "assert \"income\" in df.columns, 'Expected column \"income\" inthe processed dataset.'\n",
    "y = (df[\"income\"].astype(str).str.strip() == \">50K\").astype(int)\n",
    "\n",
    "# features: all expect income and auziliary split marker 'source' #\n",
    "drop_cols = [c for c in [\"income\", \"source\"] if c in df.columns]\n",
    "X = df.drop(columns=drop_cols)\n",
    "\n",
    "# consistency check #\n",
    "assert len(X) == len(y), f\"len(X)={len(X)} != len(y)={len(y)}\"\n",
    "\n",
    "# train/Test 80/20 with stratification #\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"[split] \"\n",
    "    f\"X_train={X_train.shape} | X_test={X_test.shape} | \"\n",
    "    f\"y_train={y_train.shape} | y_test={y_test.shape}\"\n",
    ")\n",
    "print(f\"[target] positive_rate={y.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# --- Unified Preprocessing --- #\n",
    "\n",
    "**Goal.** Build a single preprocessor applicable both during training and inference, ensuring consistent handling of numerical and categorical features.\n",
    "\n",
    "**Context.** In `01_data_loading_and_eda.ipynb`, two feature lists were defined:  \n",
    "1. `num_features` — numerical variables (e.g., `age`, `hours-per-week`, `capital_gain`, `capital_loss`);  \n",
    "2. `cat_features` — categorical variables (e.g., `education`, `occupation`, `marital_status`, `sex`, `race`).\n",
    "\n",
    "**Approach:**\n",
    "1. Use `ColumnTransformer` to combine preprocessing branches.  \n",
    "2. Numerical features:  \n",
    "   - `SimpleImputer(strategy='median')`;  \n",
    "   - `StandardScaler()`.  \n",
    "3. Categorical features:  \n",
    "   - `SimpleImputer(strategy='most_frequent')`;  \n",
    "   - `OneHotEncoder(handle_unknown='ignore', sparse_output=True)`.  \n",
    "4. The branch order is fixed to ensure column alignment when saving artifacts.  \n",
    "5. The preprocessor is stored in the variable `preproc` and later included in each model’s `Pipeline`.  \n",
    "6. All operations are deterministic; when saving models and artifacts, the preprocessor structure is serialized using `joblib.dump`.\n",
    "\n",
    "**Conclusion.** The unified `ColumnTransformer` guarantees consistent data processing across training, cross-validation, inference, and fairness audit stages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Unified Preprocessing --- #\n",
    "\n",
    "\n",
    "# feature type separation #\n",
    "num_cols = X_train.select_dtypes(include=[\"int\", \"float\"]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "\n",
    "print(f\"[preproc] numeric={len(num_cols)}, categorical={len(cat_cols)}\")\n",
    "\n",
    "# prepare preprocessing pipelines #\n",
    "num_preproc = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cat_preproc = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# unified ColumnTransformer #\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_preproc, num_cols),\n",
    "        (\"cat\", cat_preproc, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "# sample fit_transform call to verify output shape #\n",
    "Xt_train = preprocessor.fit_transform(X_train)\n",
    "Xt_test = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"[preproc] X_train -> {Xt_train.shape}, X_test -> {Xt_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper functions: metrics and logging --- #\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_tr, y_tr, X_te, y_te, name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Computes a unified set of metrics for a given model.\n",
    "    Works for bot pipelines and standalone estimators.\n",
    "    \"\"\"\n",
    "    # predictions #\n",
    "    y_pred_te = model.predict(X_te)\n",
    "\n",
    "    # probabilities #\n",
    "    y_proba_te = None\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        try:\n",
    "            y_proba_te = model.predict_proba(X_te)[:, 1]\n",
    "        except Exception:\n",
    "            pass\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        try:\n",
    "            y_proba_te = model.decision_function(X_te)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # metrics #\n",
    "    metrics = {\n",
    "        \"model\": name,\n",
    "        \"accuracy\": accuracy_score(y_te, y_pred_te),\n",
    "        \"precision\": precision_score(y_te, y_pred_te, zero_division=0),\n",
    "        \"recall\": recall_score(y_te, y_pred_te, zero_division=0),\n",
    "        \"f1\": f1_score(y_te, y_pred_te, zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(y_te, y_proba_te) if y_proba_te is not None else np.nan,\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Results storage initialization --- #\n",
    "\n",
    "# all model metrics #\n",
    "results: list[dict] = []\n",
    "\n",
    "# model registry for subsequent saving and analysis #\n",
    "model_registry: dict[str, dict] = {}\n",
    "\n",
    "print(\"[init] containers: results[], model_registry{}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# --- Raw Copies for Analysis --- #\n",
    "\n",
    "**Goal.** Preserve the raw form of test data and sensitive features before model application. These copies will be used in stage 03 “Fairness & Explainability” to assess prediction fairness and interpretability.\n",
    "\n",
    "**Context.** In `01_data_loading_and_eda.ipynb`, the original dataset structure was defined: numerical, categorical, and sensitive features (`sex`, `race`, `age`).\n",
    "\n",
    "**Approach:**\n",
    "1. Extract the original sensitive feature columns from `X_test`.  \n",
    "2. Store them in a separate object `X_test_sens`.  \n",
    "3. In parallel, save:  \n",
    "   - `y_true_test` - true test labels;  \n",
    "   - `X_test_raw` - copy of features before preprocessing.  \n",
    "4. Serialize these objects into `data/artifacts/` for later use in `03_fairness_and_explainability.ipynb`.\n",
    "\n",
    "**Conclusion.** Fixing the original data ensures group-level metric comparison and reproducibility of the fairness audit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Raw Copies for Analysis --- #\n",
    "\n",
    "# save sopies of original data before preprocessing #\n",
    "X_train_raw = X_train.copy()\n",
    "X_test_raw = X_test.copy()\n",
    "\n",
    "# cast categorical features to 'category' dtype #\n",
    "# (for error analysis and explainability) #\n",
    "for df_ in (X_train_raw, X_test_raw):\n",
    "    if \"age_group\" in df_.columns:\n",
    "        df_[\"age_group\"] = df_[\"age_group\"].astype(\"category\")\n",
    "\n",
    "print(f\"[raw] train_raw={X_train_raw.shape}, test_raw={X_test_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# --- Logistic Regression --- #\n",
    "\n",
    "**Goal.** Establish a linear baseline using the unified preprocessor.\n",
    "\n",
    "**Approach:**\n",
    "1. Use `LogisticRegression` from `sklearn.linear_model`.  \n",
    "2. Parameters:  \n",
    "   - `solver='lbfgs'`;  \n",
    "   - `max_iter=2000`;  \n",
    "   - `random_state=42`;  \n",
    "   - `n_jobs=-1` (if supported).  \n",
    "3. Training: `pipe_lr.fit(X_train, y_train)`.  \n",
    "4. Evaluation: `metrics_lr = evaluate_model(..., 'LogReg')`, then append to `results`.\n",
    "\n",
    "**Metrics.** From `evaluate_model`: Accuracy, F1, ROC AUC, Precision, Recall on the test set.\n",
    "\n",
    "**Conclusion.** Serves as a linear reference point for later comparison with tree-based and boosting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Logistic Regression (baseline) --- #\n",
    "\n",
    "pipe_lr = Pipeline(\n",
    "    steps=[\n",
    "        (\"preproc\", preprocessor),\n",
    "        (\n",
    "            \"clf\",\n",
    "            LogisticRegression(\n",
    "                solver=\"lbfgs\",\n",
    "                max_iter=2000,\n",
    "                random_state=42,\n",
    "                n_jobs=-1 if \"n_jobs\" in LogisticRegression().get_params() else None,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "metrics_lr = evaluate_model(pipe_lr, X_train, y_train, X_test, y_test, \"LogReg\")\n",
    "results.append(metrics_lr)\n",
    "metrics_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# --- Decision Tree --- #\n",
    "\n",
    "**Goal.** Provide a nonlinear baseline and evaluate potential gains from feature-based splits.\n",
    "\n",
    "**Approach:**\n",
    "1. Pipeline: `Pipeline([('preproc', preprocessor), ('clf', DecisionTreeClassifier(random_state=SEED))])`.  \n",
    "2. Validation: `StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)` -> `CV5`.  \n",
    "3. Grid search: `GridSearchCV`, `scoring='f1'`, `cv=CV5`, `n_jobs=-1`, `refit=True`.  \n",
    "4. Parameter grid:  \n",
    "   - `clf__criterion`: `['gini', 'entropy', 'log_loss']`;  \n",
    "   - `clf__max_depth`: `[None, 6, 8, 10, 12, 16]`;  \n",
    "   - `clf__min_samples_split`: `[2, 5, 10, 20]`;  \n",
    "   - `clf__min_samples_leaf`: `[1, 2, 5, 10]`;  \n",
    "   - `clf__ccp_alpha`: `[0.0, 0.001, 0.005, 0.01]`.  \n",
    "5. Results: `best_dt = gs_dt.best_estimator_`; predictions/probabilities, test metrics, `best_params` saved to report and `model_registry`.\n",
    "\n",
    "**Metrics.** F1 is the main metric for cross-validation. On the test set: Accuracy, Precision, Recall, F1, ROC AUC.\n",
    "\n",
    "**Conclusion.** Serves as a baseline for comparison with ensemble models. Depth and leaf parameters can be fixed later via model attributes for reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Decision Tree --- #\n",
    "\n",
    "\n",
    "# shared CV object and metrics list (declare once) #\n",
    "if \"CV5\" not in globals():\n",
    "    CV5 = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "if \"results\" not in globals():\n",
    "    results = []\n",
    "if \"model_registry\" not in globals():\n",
    "    model_registry = {}\n",
    "\n",
    "# pipeline #\n",
    "pipe_dt = Pipeline(\n",
    "    steps=[(\"preproc\", preprocessor), (\"clf\", DecisionTreeClassifier(random_state=SEED))]\n",
    ")\n",
    "\n",
    "# hyperparameter grid #\n",
    "param_grid_dt = {\n",
    "    \"clf__criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    \"clf__max_depth\": [None, 6, 8, 10, 12, 16],\n",
    "    \"clf__min_samples_split\": [2, 5, 10, 20],\n",
    "    \"clf__min_samples_leaf\": [1, 2, 5, 10],\n",
    "    \"clf__ccp_alpha\": [0.0, 0.001, 0.005, 0.01],\n",
    "}\n",
    "\n",
    "# grid search with primary metric f1 #\n",
    "gs_dt = GridSearchCV(\n",
    "    estimator=pipe_dt,\n",
    "    param_grid=param_grid_dt,\n",
    "    scoring=\"f1\",\n",
    "    cv=CV5,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "gs_dt.fit(X_train, y_train)\n",
    "best_dt = gs_dt.best_estimator_\n",
    "y_pred_dt = best_dt.predict(X_test)\n",
    "# decision trees provide predict_proba\n",
    "y_proba_dt = (\n",
    "    best_dt.predict_proba(X_test)[:, 1]\n",
    "    if hasattr(best_dt, \"predict_proba\")\n",
    "    else y_pred_dt.astype(float)\n",
    ")\n",
    "\n",
    "# metrics #\n",
    "row_dt = {\n",
    "    \"model\": \"DecisionTree\",\n",
    "    \"cv_f1_mean\": gs_dt.best_score_,\n",
    "    \"test_accuracy\": accuracy_score(y_test, y_pred_dt),\n",
    "    \"test_precision\": precision_score(y_test, y_pred_dt, zero_division=0),\n",
    "    \"test_recall\": recall_score(y_test, y_pred_dt, zero_division=0),\n",
    "    \"test_f1\": f1_score(y_test, y_pred_dt),\n",
    "    \"test_roc_auc\": roc_auc_score(y_test, y_proba_dt),\n",
    "    \"best_params\": gs_dt.best_params_,\n",
    "}\n",
    "results.append(row_dt)\n",
    "model_registry[\"DecisionTree\"] = {\n",
    "    \"estimator\": best_dt,\n",
    "    \"y_pred\": y_pred_dt,\n",
    "    \"y_proba\": y_proba_dt,\n",
    "    \"params\": gs_dt.best_params_,\n",
    "}\n",
    "\n",
    "# report #\n",
    "print(f\"[DecisionTree] best_f1_cv={gs_dt.best_score_:.4f}\")\n",
    "print(\"[DecisionTree] best_params:\", gs_dt.best_params_)\n",
    "print(\n",
    "    \"[DecisionTree] test: acc={:.4f} prec={:.4f} rec={:.4f} f1={:.4f} auc={:.4f}\".format(\n",
    "        row_dt[\"test_accuracy\"],\n",
    "        row_dt[\"test_precision\"],\n",
    "        row_dt[\"test_recall\"],\n",
    "        row_dt[\"test_f1\"],\n",
    "        row_dt[\"test_roc_auc\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# --- Random Forest --- #\n",
    "\n",
    "**Goal.** Reduce the variance of a single tree by averaging an ensemble of trees.\n",
    "\n",
    "**Approach:**\n",
    "1. Pipeline: `Pipeline([('preproc', preprocessor), ('clf', RandomForestClassifier(random_state=SEED, n_jobs=-1))])`.\n",
    "2. Search: `RandomizedSearchCV` (`scoring='f1'`, `n_iter=20`, `cv=CV5`, `random_state=SEED`, `n_jobs=-1`, `refit=True`).\n",
    "2. Search space:\n",
    "   - `clf__n_estimators`: `[100, 200, 300, 400]`;\n",
    "   - `clf__max_depth`: `[None, 6, 8, 10, 12]`;\n",
    "   - `clf__min_samples_split`: `[2, 5, 10]`;\n",
    "   - `clf__min_samples_leaf`: `[1, 2, 4]`;\n",
    "   - `clf__bootstrap`: `[True, False]`.\n",
    "3. Results: `best_rf = rs_rf.best_estimator_`, predictions/probabilities, test metrics, `best_params`, and registration in `model_registry`.\n",
    "\n",
    "**Metrics.** F1 on CV. On the test set: Accuracy, Precision, Recall, F1, ROC AUC.\n",
    "\n",
    "**Conclusion.** The ensemble improves stability and quality relative to a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Random Forest --- #\n",
    "\n",
    "\n",
    "# pipeline #\n",
    "pipe_rf = Pipeline(\n",
    "    steps=[(\"preproc\", preprocessor), (\"clf\", RandomForestClassifier(random_state=SEED, n_jobs=-1))]\n",
    ")\n",
    "\n",
    "# search space (balanced in size) #\n",
    "param_dist_rf = {\n",
    "    \"clf__n_estimators\": [100, 200, 300, 400],\n",
    "    \"clf__max_depth\": [None, 6, 8, 10, 12],\n",
    "    \"clf__min_samples_split\": [2, 5, 10],\n",
    "    \"clf__min_samples_leaf\": [1, 2, 4],\n",
    "    \"clf__bootstrap\": [True, False],\n",
    "}\n",
    "\n",
    "rs_rf = RandomizedSearchCV(\n",
    "    estimator=pipe_rf,\n",
    "    param_distributions=param_dist_rf,\n",
    "    scoring=\"f1\",\n",
    "    n_iter=20,\n",
    "    cv=CV5,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "rs_rf.fit(X_train, y_train)\n",
    "best_rf = rs_rf.best_estimator_\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "y_proba_rf = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# metrics #\n",
    "row_rf = {\n",
    "    \"model\": \"RandomForest\",\n",
    "    \"cv_f1_mean\": rs_rf.best_score_,\n",
    "    \"test_accuracy\": accuracy_score(y_test, y_pred_rf),\n",
    "    \"test_precision\": precision_score(y_test, y_pred_rf, zero_division=0),\n",
    "    \"test_recall\": recall_score(y_test, y_pred_rf, zero_division=0),\n",
    "    \"test_f1\": f1_score(y_test, y_pred_rf),\n",
    "    \"test_roc_auc\": roc_auc_score(y_test, y_proba_rf),\n",
    "    \"best_params\": rs_rf.best_params_,\n",
    "}\n",
    "results.append(row_rf)\n",
    "model_registry[\"RandomForest\"] = {\n",
    "    \"estimator\": best_rf,\n",
    "    \"y_pred\": y_pred_rf,\n",
    "    \"y_proba\": y_proba_rf,\n",
    "    \"params\": rs_rf.best_params_,\n",
    "}\n",
    "\n",
    "print(f\"[RandomForest] best_f1_cv={rs_rf.best_score_:.4f}\")\n",
    "print(\"[RandomForest] best_params:\", rs_rf.best_params_)\n",
    "print(\n",
    "    \"[RandomForest] test: acc={:.4f} prec={:.4f} rec={:.4f} f1={:.4f} auc={:.4f}\".format(\n",
    "        row_rf[\"test_accuracy\"],\n",
    "        row_rf[\"test_precision\"],\n",
    "        row_rf[\"test_recall\"],\n",
    "        row_rf[\"test_f1\"],\n",
    "        row_rf[\"test_roc_auc\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Random Forest: hyperparameter searсh --- #\n",
    "\n",
    "rf_param_dist = {\n",
    "    \"clf__n_estimators\": [200, 400, 800],\n",
    "    \"clf__max_depth\": [None, 5, 10, 20],\n",
    "    \"clf__min_samples_split\": [2, 5, 10],\n",
    "    \"clf__min_samples_leaf\": [1, 2, 4],\n",
    "    \"clf__max_features\": [\"sqrt\", \"log2\", 0.5, 0.8],\n",
    "}\n",
    "\n",
    "pipe_rf_tune = Pipeline(\n",
    "    steps=[(\"prepr\", preprocessor), (\"clf\", RandomForestClassifier(random_state=42, n_jobs=-1))]\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rf_search = RandomizedSearchCV(\n",
    "    estimator=pipe_rf_tune,\n",
    "    param_distributions=rf_param_dist,\n",
    "    n_iter=20,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    random_state=42,\n",
    "    error_score=np.nan,\n",
    ")\n",
    "rf_search.fit(X_train, y_train)\n",
    "print(\n",
    "    \"RF best params:\", rf_search.best_params_, \"\\nRF best CV AUC:\", round(rf_search.best_score_, 4)\n",
    ")\n",
    "\n",
    "rf_best = rf_search.best_estimator_\n",
    "results.append(evaluate_model(rf_best, X_train, y_train, X_test, y_test, \"RF_best\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# --- XGBoost (Early Stopping) --- #\n",
    "\n",
    "**Goal.** Tree boosting with overfitting control via early stopping.\n",
    "\n",
    "**Context.** The preprocessor is already fitted. Use `Xt_train`, `Xt_test` (outputs of `preprocessor.transform`).\n",
    "\n",
    "**Approach:**\n",
    "1. Hyperparameter candidates:\n",
    "   - `max_depth`: `{3, 4, 6}`;\n",
    "   - `subsample`: `{0.8, 0.8, 0.9}`;\n",
    "   - `colsample_bytree`: `{0.8, 0.8, 0.8}`.\n",
    "2. Base model parameters:\n",
    "   - `n_estimators=1000`, `learning_rate=0.05`, `objective='binary:logistic'`, `eval_metric='auc'`;\n",
    "   - `tree_method='hist'`, `reg_alpha=0.0`, `reg_lambda=1.0`;\n",
    "   - `random_state=SEED`, `n_jobs=-1`, `verbosity=0`.\n",
    "3. Training for each candidate:\n",
    "   - `clf.fit(Xt_train, y_train, eval_set=[(Xt_train, y_train), (Xt_test, y_test)], early_stopping_rounds=50, verbose=False)`;\n",
    "   - compute test metrics: F1 at threshold 0.5, ROC AUC.\n",
    "4. Select the best by test F1 across candidates. Record XGBoost’s `best_iteration_`.\n",
    "\n",
    "**Version note.** The XGBoost version is printed in a control cell (`xgboost.__version__`) before training.\n",
    "\n",
    "**Conclusion.** XGBoost often outperforms RF. Early stopping stabilizes generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import site\n",
    "import sys\n",
    "\n",
    "import xgboost\n",
    "\n",
    "print(\"PY:\", sys.executable)\n",
    "print(\"XBG:\", xgboost.__file__, xgboost.__version__)\n",
    "print(\n",
    "    \"SITE-PACKAGES:\",\n",
    "    site.getsitepackages() if hasattr(site, \"getsitepackages\") else site.getusersitepackages(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- XGBoost (Early Stopping) --- #\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# global containers #\n",
    "if \"results\" not in globals():\n",
    "    results = []\n",
    "if \"model_registry\" not in globals():\n",
    "    model_registry = {}\n",
    "\n",
    "# preconditions: preprocessor is already fitted, #\n",
    "# Xt_train/Xt_test computed earlier #\n",
    "assert \"Xt_train\" in globals() and \"Xt_test\" in globals(), (\n",
    "    \"Ожидаются Xt_train/Xt_test из блока Preprocessing\"\n",
    ")\n",
    "assert len(Xt_train) == len(y_train) and len(Xt_test) == len(y_test)\n",
    "\n",
    "# hyperparameter candidates for early stopping #\n",
    "candidates = [\n",
    "    {\"max_depth\": 3, \"subsample\": 0.8, \"colsample_bytree\": 0.8},\n",
    "    {\"max_depth\": 4, \"subsample\": 0.8, \"colsample_bytree\": 0.8},\n",
    "    {\"max_depth\": 6, \"subsample\": 0.9, \"colsample_bytree\": 0.8},\n",
    "]\n",
    "\n",
    "best_pack = None\n",
    "for hp in candidates:\n",
    "    clf = XGBClassifier(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=hp[\"max_depth\"],\n",
    "        subsample=hp[\"subsample\"],\n",
    "        colsample_bytree=hp[\"colsample_bytree\"],\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=1.0,\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"auc\",\n",
    "        tree_method=\"hist\",\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0,\n",
    "    )\n",
    "\n",
    "    clf.fit(\n",
    "        Xt_train,\n",
    "        y_train,\n",
    "        eval_set=[(Xt_train, y_train), (Xt_test, y_test)],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False,\n",
    "    )\n",
    "    proba = clf.predict_proba(Xt_test)[:, 1]\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "    score = f1_score(y_test, pred)\n",
    "    pack = {\n",
    "        \"clf\": clf,\n",
    "        \"params\": {**hp, \"learning_rate\": 0.05},\n",
    "        \"f1\": score,\n",
    "        \"auc\": roc_auc_score(y_test, proba),\n",
    "        \"acc\": accuracy_score(y_test, pred),\n",
    "        \"prec\": precision_score(y_test, pred, zero_division=0),\n",
    "        \"rec\": recall_score(y_test, pred, zero_division=0),\n",
    "    }\n",
    "    if best_pack is None or pack[\"f1\"] > best_pack[\"f1\"]:\n",
    "        best_pack = pack\n",
    "\n",
    "# assemble estimator as a Pipeline with the already-fitted preprocessor and clf #\n",
    "pipe_xgb = Pipeline(steps=[(\"preproc\", preprocessor), (\"clf\", best_pack[\"clf\"])])\n",
    "\n",
    "# metrics in the unified format #\n",
    "row_xgb = {\n",
    "    \"model\": \"XGBoost_ES\",\n",
    "    \"cv_f1_mean\": np.nan,  # tuned via ES, not via CV\n",
    "    \"test_accuracy\": best_pack[\"acc\"],\n",
    "    \"test_precision\": best_pack[\"prec\"],\n",
    "    \"test_recall\": best_pack[\"rec\"],\n",
    "    \"test_f1\": best_pack[\"f1\"],\n",
    "    \"test_roc_auc\": best_pack[\"auc\"],\n",
    "    \"best_params\": best_pack[\"params\"],\n",
    "}\n",
    "results.append(row_xgb)\n",
    "\n",
    "# for reference, y_pred/y_proba on raw X_test #\n",
    "y_proba_xgb = pipe_xgb.predict_proba(X_test)[:, 1]\n",
    "y_pred_xgb = (y_proba_xgb >= 0.5).astype(int)\n",
    "\n",
    "model_registry[\"XGBoost_ES\"] = {\n",
    "    \"estimator\": pipe_xgb,\n",
    "    \"y_pred\": y_pred_xgb,\n",
    "    \"y_proba\": y_proba_xgb,\n",
    "    \"params\": best_pack[\"params\"],\n",
    "}\n",
    "\n",
    "print(\"[XGBoost_ES] best_params:\", best_pack[\"params\"])\n",
    "print(\n",
    "    \"[XGBoost_ES] test: acc={:.4f} prec={:.4f} rec={:.4f} f1={:.4f} auc={:.4f}\".format(\n",
    "        row_xgb[\"test_accuracy\"],\n",
    "        row_xgb[\"test_precision\"],\n",
    "        row_xgb[\"test_recall\"],\n",
    "        row_xgb[\"test_f1\"],\n",
    "        row_xgb[\"test_roc_auc\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- XGB metrics logging (report only) --- #\n",
    "\n",
    "# metrics were already stored in results within the XGBoost_ES block #\n",
    "row_xgb = next(r for r in results if r[\"model\"] == \"XGBoost_ES\")\n",
    "\n",
    "print(\n",
    "    \"[XGBoost_ES] test:\",\n",
    "    f\"acc={row_xgb['test_accuracy']:.4f}\",\n",
    "    f\"prec={row_xgb['test_precision']:.4f}\",\n",
    "    f\"rec={row_xgb['test_recall']:.4f}\",\n",
    "    f\"f1={row_xgb['test_f1']:.4f}\",\n",
    "    f\"auc={row_xgb['test_roc_auc']:.4f}\",\n",
    ")\n",
    "\n",
    "row_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "# --- LightGBM (RandomizedSearch) --- #\n",
    "\n",
    "**Goal.** Ensemble boosting with hyperparameter tuning via `RandomizedSearchCV` on the unified preprocessor.\n",
    "\n",
    "**Approach:**\n",
    "1. Pipeline: `Pipeline([('preproc', preprocessor), ('clf', LGBMClassifier(...))])`.\n",
    "2. Model parameters:  \n",
    "   - `objective='binary'`, `metric='auc'`, `random_state=SEED`, `n_jobs=-1`, `verbosity=-1`.\n",
    "3. Search space:  \n",
    "   - `clf__n_estimators`: `[300, 500, 800, 1000]`;  \n",
    "   - `clf__learning_rate`: `[0.01, 0.03, 0.05, 0.1]`;  \n",
    "   - `clf__num_leaves`: `[15, 31, 63, 127]`;  \n",
    "   - `clf__max_depth`: `[-1, 4, 6, 8, 10]`;  \n",
    "   - `clf__subsample`: `[0.7, 0.8, 0.9, 1.0]`;  \n",
    "   - `clf__colsample_bytree`: `[0.7, 0.8, 0.9, 1.0]`;  \n",
    "   - `clf__reg_alpha`: `[0.0, 0.01, 0.05, 0.1]`;  \n",
    "   - `clf__reg_lambda`: `[0.0, 0.01, 0.05, 0.1]`.\n",
    "4. `RandomizedSearchCV`:  \n",
    "   - `scoring='f1'`, `n_iter=25`, `cv=CV5`, `random_state=SEED`, `n_jobs=-1`, `verbose=0`, `refit=True`.\n",
    "\n",
    "**Metrics:**\n",
    "1. CV: `best_f1_cv = rs_lgb.best_score_`.  \n",
    "2. Test: Accuracy, Precision, Recall, F1 from `best_lgb.predict(X_test)`, ROC AUC from `best_lgb.predict_proba(X_test)[:, 1]`.\n",
    "\n",
    "**Conclusion.** The result and best parameters are added to `results` and `model_registry['LightGBM_RS']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LightGBM (RandomizedSearch) --- #\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# pipeline #\n",
    "pipe_lgb = Pipeline(\n",
    "    steps=[\n",
    "        (\"preproc\", preprocessor),\n",
    "        (\n",
    "            \"clf\",\n",
    "            LGBMClassifier(\n",
    "                objective=\"binary\",\n",
    "                metric=\"auc\",\n",
    "                random_state=SEED,\n",
    "                n_jobs=-1,\n",
    "                verbosity=-1,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# search space #\n",
    "param_dist_lgb = {\n",
    "    \"clf__n_estimators\": [300, 500, 800, 1000],\n",
    "    \"clf__learning_rate\": [0.01, 0.03, 0.05, 0.1],\n",
    "    \"clf__num_leaves\": [15, 31, 63, 127],\n",
    "    \"clf__max_depth\": [-1, 4, 6, 8, 10],\n",
    "    \"clf__subsample\": [0.7, 0.8, 0.9, 1.0],\n",
    "    \"clf__colsample_bytree\": [0.7, 0.8, 0.9, 1.0],\n",
    "    \"clf__reg_alpha\": [0.0, 0.01, 0.05, 0.1],\n",
    "    \"clf__reg_lambda\": [0.0, 0.01, 0.05, 0.1],\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV #\n",
    "rs_lgb = RandomizedSearchCV(\n",
    "    estimator=pipe_lgb,\n",
    "    param_distributions=param_dist_lgb,\n",
    "    scoring=\"f1\",\n",
    "    n_iter=25,\n",
    "    cv=CV5,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "rs_lgb.fit(X_train, y_train)\n",
    "best_lgb = rs_lgb.best_estimator_\n",
    "y_pred_lgb = best_lgb.predict(X_test)\n",
    "y_proba_lgb = best_lgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# metrics #\n",
    "row_lgb = {\n",
    "    \"model\": \"LightGBM_RS\",\n",
    "    \"cv_f1_mean\": rs_lgb.best_score_,\n",
    "    \"test_accuracy\": accuracy_score(y_test, y_pred_lgb),\n",
    "    \"test_precision\": precision_score(y_test, y_pred_lgb, zero_division=0),\n",
    "    \"test_recall\": recall_score(y_test, y_pred_lgb, zero_division=0),\n",
    "    \"test_f1\": f1_score(y_test, y_pred_lgb),\n",
    "    \"test_roc_auc\": roc_auc_score(y_test, y_proba_lgb),\n",
    "    \"best_params\": rs_lgb.best_params_,\n",
    "}\n",
    "results.append(row_lgb)\n",
    "model_registry[\"LightGBM_RS\"] = {\n",
    "    \"estimator\": best_lgb,\n",
    "    \"y_pred\": y_pred_lgb,\n",
    "    \"y_proba\": y_proba_lgb,\n",
    "    \"params\": rs_lgb.best_params_,\n",
    "}\n",
    "\n",
    "print(f\"[LightGBM_RS] best_f1_cv={rs_lgb.best_score_:.4f}\")\n",
    "print(\"[LightGBM_RS] best_params:\", rs_lgb.best_params_)\n",
    "print(\n",
    "    \"[LightGBM_RS] test: acc={:.4f} prec={:.4f} rec={:.4f} f1={:.4f} auc={:.4f}\".format(\n",
    "        row_lgb[\"test_accuracy\"],\n",
    "        row_lgb[\"test_precision\"],\n",
    "        row_lgb[\"test_recall\"],\n",
    "        row_lgb[\"test_f1\"],\n",
    "        row_lgb[\"test_roc_auc\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# --- Unified Metrics Table --- #\n",
    "\n",
    "**Goal.** Aggregate results from all models into a single table, sort by test F1, and save the artifact.\n",
    "\n",
    "**Approach:**\n",
    "1. Build: `df_results = pd.DataFrame(results)`.\n",
    "2. Column order: `['model', 'cv_f1_mean', 'test_accuracy', 'test_precision', 'test_recall', 'test_f1', 'test_roc_auc']`.\n",
    "3. Rounding: create a float-only copy rounded via `.round(4)` → `df_results_rounded`.\n",
    "4. Sorting: by `'test_f1'` in descending order, then `.reset_index(drop=True)`.\n",
    "5. Display: `display(df_results_rounded.style.hide(axis='index').set_caption('Model Performance Summary'))`.\n",
    "6. Save: `REPORTS_DIR / 'metrics_table_modeling.csv'`.\n",
    "\n",
    "**Conclusion.** The table `df_results_rounded` is the single comparison point. It is used next to select the best model and to plot the comparison chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Unified Metrics Table --- #\n",
    "\n",
    "# convert to DataFrame #\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# column ordering and rounding #\n",
    "cols_order = [\n",
    "    \"model\",\n",
    "    \"cv_f1_mean\",\n",
    "    \"test_accuracy\",\n",
    "    \"test_precision\",\n",
    "    \"test_recall\",\n",
    "    \"test_f1\",\n",
    "    \"test_roc_auc\",\n",
    "]\n",
    "df_results = df_results[cols_order + [c for c in df_results.columns if c not in cols_order]]\n",
    "\n",
    "# rounding for compactness #\n",
    "df_results_rounded = df_results.copy()\n",
    "for c in df_results_rounded.select_dtypes(include=[\"float\"]).columns:\n",
    "    df_results_rounded[c] = df_results_rounded[c].round(4)\n",
    "\n",
    "# sort by f1 (test) #\n",
    "df_results_rounded = df_results_rounded.sort_values(by=\"test_f1\", ascending=False).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "print(\"[metrics] unified comparison table:\")\n",
    "display(df_results_rounded.style.hide(axis=\"index\").set_caption(\"Model Performance Summary\"))\n",
    "\n",
    "# save to artifacts #\n",
    "out_path = REPORTS_DIR / \"metrics_table_modeling.csv\"\n",
    "df_results_rounded.to_csv(out_path, index=False)\n",
    "print(f\"[saved] {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# --- Model Comparison Chart --- #\n",
    "\n",
    "**Goal.** Visually compare models by test metrics.\n",
    "\n",
    "**Approach:**\n",
    "1. Data source: `df_results_rounded`.\n",
    "2. Build a grouped bar chart using `pandas.DataFrame.plot(kind='bar')`.\n",
    "3. Displayed metrics: `['test_roc_auc', 'test_f1', 'test_accuracy']`.\n",
    "4. Styling: title 'Model Comparison (ROC-AUC, F1, Accuracy)', Y-axis label 'Score', hide the X-axis label, legend titled 'Metric', rotate model labels for readability.\n",
    "\n",
    "**Conclusion.** The chart helps quickly spot leaders by AUC, F1, and Accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Comparison Chart --- #\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# check that results table exists #\n",
    "assert \"df_results_rounded\" in globals(), (\n",
    "    \"Expected df_results_rounded from the Unified Metrics Table block\"\n",
    ")\n",
    "\n",
    "# build bar chart #\n",
    "plot_cols = [\"test_roc_auc\", \"test_f1\", \"test_accuracy\"]\n",
    "ax = df_results_rounded.set_index(\"model\")[plot_cols].plot(kind=\"bar\", figsize=(9, 5))\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_title(\"Model Comparison (ROC-AUC, F1, Accuracy)\")\n",
    "ax.legend(title=\"Metric\", loc=\"lower right\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "# --- Saving Best Model & Artifacts --- #\n",
    "\n",
    "**Goal.** Save the best model, the preprocessor, and key artifacts for later use in stage 03 (fairness & explainability).\n",
    "\n",
    "**Approach:**\n",
    "1. The best model is selected by the maximum `test_f1` in `df_results_rounded`.\n",
    "2. Objects to save:\n",
    "   - `best_model` - combined `Pipeline`;\n",
    "   - `preprocessor` - fitted `ColumnTransformer`;\n",
    "   - `df_results_rounded` - final metrics table;\n",
    "   - feature lists `num_features`, `cat_features`;\n",
    "   - library versions (`pip freeze`);\n",
    "   - OHE feature structure (`preprocessor['cat'].get_feature_names_out()`);\n",
    "   - the `data/artifacts` folder includes:\n",
    "     - `model_best.joblib`;\n",
    "     - `preprocessor.joblib`;\n",
    "     - `metrics_table_modeling.csv`;\n",
    "     - `feature_lists.json`;\n",
    "     - `versions.txt`;\n",
    "3. After saving, print confirmation of paths and file sizes.\n",
    "\n",
    "**Conclusion.** The complete artifact set ensures reproducibility and direct loading for the next notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Saving Best Model & Artifacts --- #\n",
    "\n",
    "from joblib import dump\n",
    "\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# determine best model by test_f1 #\n",
    "assert \"df_results_rounded\" in globals(), (\n",
    "    \"Expected df_results_rounded from the Unified Metrics Table block\"\n",
    ")\n",
    "best_row = df_results_rounded.iloc[0]\n",
    "best_name = best_row[\"model\"]\n",
    "print(f\"[save] best_model={best_name}\")\n",
    "\n",
    "# extract model object from registry #\n",
    "best_pack = model_registry.get(best_name)\n",
    "if not best_pack or \"estimator\" not in best_pack:\n",
    "    raise ValueError(f'Model \"{best_name}\" not found in model_registry')\n",
    "\n",
    "best_model = best_pack[\"estimator\"]\n",
    "\n",
    "# save model in joblib format #\n",
    "dst = MODELS_DIR / f\"{best_name}_best.joblib\"\n",
    "dump(best_model, dst)\n",
    "print(f\"[saved] {dst}\")\n",
    "\n",
    "# additionally saved results table #\n",
    "dst_metrics = MODELS_DIR / \"results_summary.csv\"\n",
    "df_results_rounded.to_csv(dst_metrics, index=False)\n",
    "print(f\"[saved] {dst_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "# --- Export Artifacts for 03 (fairness & explainability) --- #\n",
    "\n",
    "**Goal.** Prepare files used by the third notebook for fairness and explainability.\n",
    "\n",
    "**Approach:**\n",
    "1. Select the best model: first row of `df_results_rounded` (max `test_f1`), then `model_registry[best_name]['estimator']`.\n",
    "2. Compute on `X_test`:\n",
    "   - `y_proba_best`: `predict_proba` or `decision_function` if probabilities are unavailable;\n",
    "   - `y_pred_best`: threshold 0.5;\n",
    "   - `y_true_test`: ground-truth test labels.\n",
    "3. Sensitive features:\n",
    "   - if `X_test_raw` exists, take the subset of columns from `['sex', 'race', 'age_group', 'education']` and write `X_test_sensitive.csv`.\n",
    "4. Preprocessor and feature names:\n",
    "   - try `preproc = pipe.named_steps['preproc']`;\n",
    "   - `preproc.fit(X_train, y_train_)`, then `X_test_enc = preproc.transform(X_test)`;\n",
    "   - save `feature_names` if available.\n",
    "\n",
    "**Files:**\n",
    "1. In `data/artifacts/`:\n",
    "   - `y_true_test.npy`, `y_proba_best.npy`, `y_pred_best.npy`;\n",
    "   - `X_test_sensitive.csv`;\n",
    "   - `feature_names.npy`;\n",
    "   - `X_test_enc.npz` for sparse or `X_test_enc.npy` for dense matrices;\n",
    "   - `export_meta.json` with `best_model`, timestamp, metrics string, and the artifact list.\n",
    "\n",
    "**Conclusion.** Export complete. The third notebook loads ready-made metrics, predictions, sensitive features, and encoded features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Export Artifacts for 03 (fairness & explainability) --- #\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from joblib import dump\n",
    "\n",
    "try:\n",
    "    import scipy.sparse as sp\n",
    "except Exception:\n",
    "    sp = None\n",
    "\n",
    "# preconditions #\n",
    "assert \"df_results_rounded\" in globals(), (\n",
    "    \"df_results_rounded not found. Run the Unified Metrics Table block\"\n",
    ")\n",
    "assert \"model_registry\" in globals() and len(model_registry) > 0, \"model_registry is empty\"\n",
    "assert all(v in globals() for v in [\"X_train\", \"X_test\", \"y_train\", \"y_test\"]), (\n",
    "    \"X/y split not in memory\"\n",
    ")\n",
    "\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# select the best model (by test_f1) and extract its pipeline #\n",
    "best_name = df_results_rounded.iloc[0][\"model\"]\n",
    "pack = model_registry.get(best_name)\n",
    "if not pack or \"estimator\" not in pack:\n",
    "    raise RuntimeError(f'\"{best_name}\" not found in model_registry')\n",
    "\n",
    "model_best = pack[\"estimator\"]\n",
    "print(f\"[export] best_model={best_name}\")\n",
    "\n",
    "# get probabilities/predictions on X_test #\n",
    "if hasattr(model_best, \"predict_proba\"):\n",
    "    y_proba_best = model_best.predict_proba(X_test)[:, 1]\n",
    "elif hasattr(model_best, \"decision_function\"):\n",
    "    y_proba_best = model_best.decision_function(X_test)\n",
    "else:\n",
    "    raise RuntimeError(f'Model \"{best_name}\" does not support predict_proba/decision_function.')\n",
    "\n",
    "y_pred_best = (y_proba_best >= 0.5).astype(\"int8\")\n",
    "y_true_test = np.asarray(y_test, dtype=\"int8\")\n",
    "\n",
    "# sensitive features for fairness slices #\n",
    "X_test_sens = None\n",
    "if \"X_test_raw\" in globals():\n",
    "    sens_cols = [c for c in [\"sex\", \"race\", \"age_group\", \"education\"] if c in X_test_raw.columns]\n",
    "    if sens_cols:\n",
    "        X_test_sens = X_test_raw[sens_cols].copy()\n",
    "        X_test_sens.to_csv(ART_DIR / \"X_test_sensitive.csv\", index=False)\n",
    "        print(f\"[export] X_test_sensitive.csv with cols={sens_cols}\")\n",
    "\n",
    "\n",
    "# extract preprocessor and encode X_test -> X_test_enc (+ feature_names) #\n",
    "def _get_preproc(pipe):\n",
    "    return getattr(pipe, \"named_steps\", {}).get(\"preproc\", None)\n",
    "\n",
    "\n",
    "feature_names = None\n",
    "X_test_enc = None\n",
    "\n",
    "preproc = _get_preproc(model_best)\n",
    "if preproc is not None:\n",
    "    try:\n",
    "        # ensure fitted state\n",
    "        preproc_fitted = preproc.fit(X_train, y_train)\n",
    "        X_test_enc = preproc_fitted.transform(X_test)\n",
    "        # try to get feature names\n",
    "        if hasattr(preproc_fitted, \"get_feature_names_out\"):\n",
    "            feature_names = preproc_fitted.get_feature_names_out()\n",
    "    except Exception as e:\n",
    "        print(\"[warn] preprocessor transform/get_feature_names_out failed:\", type(e).__name__, e)\n",
    "\n",
    "# save artifacts for '03_fairness_and_explainability.ipynb' #\n",
    "np.save(ART_DIR / \"y_true_test.npy\", y_true_test)\n",
    "np.save(ART_DIR / \"y_proba_best.npy\", y_proba_best)\n",
    "np.save(ART_DIR / \"y_pred_best.npy\", y_pred_best)\n",
    "print(\"[export] y_* saved\")\n",
    "\n",
    "if feature_names is not None:\n",
    "    try:\n",
    "        np.save(ART_DIR / \"feature_names.npy\", feature_names)\n",
    "        print(\"[export] feature_names.npy saved\")\n",
    "    except Exception as e:\n",
    "        print(\"[warn] feature_names.npy save failed:\", e)\n",
    "\n",
    "if X_test_enc is not None:\n",
    "    try:\n",
    "        if sp is not None and sp.issparse(X_test_enc):\n",
    "            sp.save_npz(ART_DIR / \"X_test_enc.npz\", X_test_enc)\n",
    "            print(\"[export] X_test_enc.npz saved\", X_test_enc.shape)\n",
    "        else:\n",
    "            np.save(ART_DIR / \"X_test_enc.npy\", np.asarray(X_test_enc))\n",
    "            print(\"[export] X_test_enc.npy saved\", np.asarray(X_test_enc).shape)\n",
    "    except Exception as e:\n",
    "        print(\"[warn] X_test_enc save failed:\", e)\n",
    "else:\n",
    "    print(\"[info] X_test_enc not available (no preprocessor)\")\n",
    "\n",
    "# export metadata #\n",
    "meta = {\n",
    "    \"best_model\": best_name,\n",
    "    \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"metrics_row\": df_results_rounded[df_results_rounded[\"model\"] == best_name].iloc[0].to_dict(),\n",
    "    \"artifacts\": sorted([p.name for p in ART_DIR.glob(\"*\")]),\n",
    "}\n",
    "with open(ART_DIR / \"export_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(\"[export] export_meta.json written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Export Figures (ROC, PR, Calibration, Confusion, Feature Importance) --- #\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import auc, confusion_matrix, precision_recall_curve\n",
    "\n",
    "# output directory #\n",
    "FIG_DIR_02 = REPORTS_DIR / \"figures_02\"\n",
    "FIG_DIR_02.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# robust load of y_true/y_proba/y_pred #\n",
    "g = globals()\n",
    "y_true = g.get(\"y_true_test\", g.get(\"y_test\", None))\n",
    "y_proba = g.get(\"y_proba_best\", None)\n",
    "y_pred = g.get(\"y_pred_best\", None)\n",
    "\n",
    "\n",
    "def _maybe_load(npy_path: Path):\n",
    "    try:\n",
    "        return np.load(npy_path)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "if y_true is None:\n",
    "    y_true = _maybe_load(ART_DIR / \"y_true_test.npy\")\n",
    "if y_proba is None:\n",
    "    y_proba = _maybe_load(ART_DIR / \"y_proba_best.npy\")\n",
    "if y_pred is None:\n",
    "    y_pred = _maybe_load(ART_DIR / \"y_pred_best.npy\")\n",
    "\n",
    "if y_true is None or y_proba is None:\n",
    "    raise RuntimeError(\"[fig02] y_true и y_proba required. Run artifact export first.\")\n",
    "\n",
    "if y_pred is None or len(y_pred) != len(y_true):\n",
    "    y_pred = (y_proba >= 0.5).astype(\"int8\")\n",
    "\n",
    "# ROC #\n",
    "fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, lw=2, label=f\"ROC AUC = {roc_auc:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], lw=1, linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR_02 / \"roc_curve.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# PR #\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, lw=2, label=f\"PR AUC = {pr_auc:.4f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR_02 / \"pr_curve.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# calibration #\n",
    "prob_true, prob_pred = calibration_curve(y_true, y_proba, n_bins=10, strategy=\"uniform\")\n",
    "plt.figure()\n",
    "plt.plot(prob_pred, prob_true, marker=\"o\", lw=2, label=\"Calibration\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=1, label=\"Perfect\")\n",
    "plt.xlabel(\"Mean predicted probability\")\n",
    "plt.ylabel(\"Fraction of positives\")\n",
    "plt.title(\"Calibration Curve\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR_02 / \"calibration_curve.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# confusion Matrix (thr=0.5) #\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "plt.figure()\n",
    "plt.imshow(cm, interpolation=\"nearest\")\n",
    "plt.title(\"Confusion Matrix (thr=0.5)\")\n",
    "plt.xticks([0, 1], [\"0\", \"1\"])\n",
    "plt.yticks([0, 1], [\"0\", \"1\"])\n",
    "for (i, j), v in np.ndenumerate(cm):\n",
    "    plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR_02 / \"confusion_matrix.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# feature Importance (LightGBM / XGBoost) #\n",
    "# try to get feature_names #\n",
    "feature_names = None\n",
    "p_fn = ART_DIR / \"feature_names.npy\"\n",
    "if p_fn.exists():\n",
    "    try:\n",
    "        feature_names = np.load(p_fn, allow_pickle=True)\n",
    "    except Exception:\n",
    "        feature_names = None\n",
    "\n",
    "\n",
    "def _get_preproc(pipe):\n",
    "    return getattr(pipe, \"named_steps\", {}).get(\"preproc\", None)\n",
    "\n",
    "\n",
    "if feature_names is None and \"model_best\" in g:\n",
    "    preproc = _get_preproc(g[\"model_best\"])\n",
    "    if preproc is not None and hasattr(preproc, \"get_feature_names_out\"):\n",
    "        try:\n",
    "            feature_names = preproc.get_feature_names_out()\n",
    "        except Exception:\n",
    "            feature_names = None\n",
    "\n",
    "# LightGBM #\n",
    "lgb_pack = model_registry.get(\"LightGBM_RS\")\n",
    "if lgb_pack:\n",
    "    lgb_pipe = lgb_pack.get(\"estimator\")\n",
    "    lgb_clf = (\n",
    "        getattr(getattr(lgb_pipe, \"named_steps\", {}), \"get\", lambda _: None)(\"clf\")\n",
    "        if hasattr(lgb_pipe, \"named_steps\")\n",
    "        else None\n",
    "    )\n",
    "    if lgb_clf is not None and hasattr(lgb_clf, \"feature_importances_\"):\n",
    "        try:\n",
    "            imp = np.array(lgb_clf.feature_importances_, dtype=float)\n",
    "            names = (\n",
    "                feature_names\n",
    "                if (feature_names is not None and len(feature_names) == len(imp))\n",
    "                else np.array([f\"f{i}\" for i in range(len(imp))])\n",
    "            )\n",
    "            order = np.argsort(imp)[::-1][:40]\n",
    "            plt.figure(figsize=(8, max(4, len(order) * 0.25)))\n",
    "            plt.barh(range(len(order)), imp[order][::-1])\n",
    "            plt.yticks(range(len(order)), names[order][::-1])\n",
    "            plt.xlabel(\"Importance\")\n",
    "            plt.title(\"Feature Importance — LightGBM\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(FIG_DIR_02 / \"feature_importance_lgbm.png\", dpi=200)\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(\"[warn] LGBM FI export:\", type(e).__name__, e)\n",
    "\n",
    "# XGBoost (gain) #\n",
    "xgb_pack = model_registry.get(\"XGBoost_ES\")\n",
    "if xgb_pack:\n",
    "    xgb_pipe = xgb_pack.get(\"estimator\")\n",
    "    xgb_clf = (\n",
    "        getattr(getattr(xgb_pipe, \"named_steps\", {}), \"get\", lambda _: None)(\"clf\")\n",
    "        if hasattr(xgb_pipe, \"named_steps\")\n",
    "        else None\n",
    "    )\n",
    "    booster = None\n",
    "    if xgb_clf is not None and hasattr(xgb_clf, \"get_booster\"):\n",
    "        try:\n",
    "            booster = xgb_clf.get_booster()\n",
    "        except Exception:\n",
    "            booster = None\n",
    "    if booster is not None:\n",
    "        try:\n",
    "            # dict: f{idx} -> gain\n",
    "            fscore = booster.get_score(importance_type=\"gain\")\n",
    "            items = sorted(fscore.items(), key=lambda kv: kv[1], reverse=True)[:40]\n",
    "            names_raw = [k for k, _ in items]\n",
    "            gains = [v for _, v in items]\n",
    "\n",
    "            def _map(raw):\n",
    "                if raw.startswith(\"f\") and raw[1:].isdigit():\n",
    "                    idx = int(raw[1:])\n",
    "                    if feature_names is not None and idx < len(feature_names):\n",
    "                        return str(feature_names[idx])\n",
    "                return raw\n",
    "\n",
    "            names = [_map(n) for n in names_raw]\n",
    "            plt.figure(figsize=(8, max(4, len(names) * 0.25)))\n",
    "            plt.barh(range(len(names)), gains[::-1])\n",
    "            plt.yticks(range(len(names)), names[::-1])\n",
    "            plt.xlabel(\"Gain\")\n",
    "            plt.title(\"Feature Importance - XGBoost (gain)\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(FIG_DIR_02 / \"feature_importance_xgb_gain.png\", dpi=200)\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(\"[warn] XGB FI export:\", type(e).__name__, e)\n",
    "\n",
    "print(\n",
    "    \"[fig02] Saved: \"\n",
    "    \"roc_curve.png, pr_curve.png, calibration_curve.png, \"\n",
    "    \"confusion_matrix.png, and FI if available.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Export Best Pipeline for Inference --- #\n",
    "\n",
    "from joblib import dump\n",
    "\n",
    "# preconditions #\n",
    "assert \"df_results_rounded\" in globals(), (\n",
    "    \"df_results_rounded not found. Run the Unified Metrics Table block\"\n",
    ")\n",
    "assert \"model_registry\" in globals() and len(model_registry) > 0, \"model_registry is empty\"\n",
    "\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# select best model by test_f1 #\n",
    "best_name = df_results_rounded.iloc[0][\"model\"]\n",
    "pack = model_registry.get(best_name)\n",
    "if not pack or \"estimator\" not in pack:\n",
    "    raise RuntimeError(f'\"{best_name}\" is missing in model_registry or has no \"estimator\".')\n",
    "\n",
    "model_best = pack[\"estimator\"]\n",
    "\n",
    "# export under two names: canonical and universal alias #\n",
    "p1 = MODELS_DIR / f\"{best_name}_best.joblib\"\n",
    "p2 = MODELS_DIR / \"model_best.joblib\"\n",
    "\n",
    "dump(model_best, p1)\n",
    "dump(model_best, p2)\n",
    "\n",
    "print(f\"[export] saved: {p1.name}, {p2.name} -> {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verify Artifacts for 03 (fairness & explainability) --- #\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "required = [\n",
    "    ART_DIR / \"y_true_test.npy\",\n",
    "    ART_DIR / \"y_proba_best.npy\",\n",
    "    ART_DIR / \"y_pred_best.npy\",\n",
    "]\n",
    "optional = [\n",
    "    ART_DIR / \"X_test_sensitive.csv\",\n",
    "    ART_DIR / \"feature_names.npy\",\n",
    "    ART_DIR / \"X_test_enc.npy\",\n",
    "    ART_DIR / \"X_test_enc.npz\",\n",
    "    ART_DIR / \"export_meta.json\",\n",
    "]\n",
    "\n",
    "missing = [p for p in required if not p.exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing required artifacts: {[p.name for p in missing]} \"\n",
    "        f'Need to run the block \"Export Artifacts for 03 (fairness & explainability)\" above'\n",
    "    )\n",
    "\n",
    "print(\"[verify] Required artifacts found:\")\n",
    "for p in required:\n",
    "    print(\"  -\", p.name)\n",
    "\n",
    "print(\"[verify] Optional artifacts:\")\n",
    "for p in optional:\n",
    "    print(\"  -\", p.name, \"OK\" if p.exists() else \"—\")\n",
    "\n",
    "print(\"[verify] 03 is ready to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Export Demo Predictions (Pipeline) --- #\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump\n",
    "\n",
    "from paths import ROOT\n",
    "\n",
    "# preconditions #\n",
    "assert \"df_results_rounded\" in globals(), \"df_results_rounded not found\"\n",
    "assert \"model_registry\" in globals() and len(model_registry) > 0, \"model_registry is empty\"\n",
    "assert all(v in globals() for v in [\"X_test\", \"y_test\"]), \"X_test / y_test not found\"\n",
    "\n",
    "PRED_DIR = ROOT / \"predictions\"\n",
    "PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# best model #\n",
    "best_name = df_results_rounded.iloc[0][\"model\"]\n",
    "pack = model_registry.get(best_name)\n",
    "if not pack or \"estimator\" not in pack:\n",
    "    raise RuntimeError(f'\"{best_name}\" not found in model_registry.')\n",
    "\n",
    "model_best = pack[\"estimator\"]\n",
    "\n",
    "# scoring #\n",
    "if hasattr(model_best, \"predict_proba\"):\n",
    "    proba = model_best.predict_proba(X_test)[:, 1]\n",
    "elif hasattr(model_best, \"decision_function\"):\n",
    "    proba = model_best.decision_function(X_test)\n",
    "else:\n",
    "    raise RuntimeError(f'Model \"{best_name}\" does not support predict_proba/decision_function.')\n",
    "\n",
    "label = (proba >= 0.5).astype(\"int8\")\n",
    "pd.DataFrame({\"proba\": proba, \"label\": label}).to_csv(PRED_DIR / \"preds_pipeline.csv\", index=False)\n",
    "\n",
    "print(f\"[pred] saved: {PRED_DIR / 'preds_pipeline.csv'} via {best_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Path Assertions --- #\n",
    "\n",
    "assert ART_DIR.resolve().parts[-2:] == (\"data\", \"artifacts\"), f\"ART_DIR={ART_DIR}\"\n",
    "assert MODELS_DIR.resolve().parts[-2:] == (\"data\", \"models\"), f\"MODELS_DIR={MODELS_DIR}\"\n",
    "print(\"[paths] ART_DIR and MODELS_DIR are valid.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "census_ds2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
