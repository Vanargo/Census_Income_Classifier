{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "© 2025 Vanargo · License: MIT. See the `LICENSE` file in the repository root."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# --- Fairness & Explainability: setup --- #\n",
    "\n",
    "**Goal.** Evaluate the fairness and quality of the final model on the test set, choose an operating threshold `t*` under fairness constraints, check group-wise calibration, and explain feature contributions.\n",
    "\n",
    "**Inputs (from `01_data_loading_and_eda.ipynb` / `02_modeling.ipynb`):**\n",
    "1. `X_test_enc`, `y_true_test`, `feature_names` - prepared in `02_modeling.ipynb`.\n",
    "2. `y_proba_best` - test predictions from the best model.\n",
    "3. `X_test_sens` - sensitive features (`sex`, `race`, `age_group`).\n",
    "\n",
    "**Outputs (to `data/reports/figures_03/`, `data/artifacts`):**\n",
    "1. Threshold sweep and Pareto: `accuracy_f1_vs_threshold.png`, `dp_vs_threshold.png`, `Pareto_f1_vs_dp.png`.\n",
    "2. Group metrics at `t=0.5` and `t*`: CSV tables and bar charts.\n",
    "3. Group calibration: `calibration_*.png` and aggregate ECE.\n",
    "4. Explainability: top features (SHAP/permutation), dependence plots.\n",
    "5. Auxiliary artifacts: selected `t*`, post-processing parameters (if `ThresholdOptimizer` applied).\n",
    "\n",
    "**Notebook contents:**\n",
    "1. Sanity checks: shapes, NaNs, `y_proba` distribution.\n",
    "2. Baseline metrics at `t=0.5`.\n",
    "3. Group fairness at `t=0.5`.\n",
    "4. Threshold sweep -> select `t*` (quality <-> fairness trade-off).\n",
    "5. Post-processing (`ThresholdOptimizer`: `demographic_parity` / `equalized_odds`).\n",
    "6. Probability calibration by groups (reliability curves, ECE).\n",
    "7. Explainability: global and local.\n",
    "8. Risks, limitations, recommendations. Artifact export.\n",
    "\n",
    "**Environment and paths requirements:**\n",
    "1. `ROOT`, `ART_DIR`, `REPORTS_DIR`, `FIG_DIR_03 = REPORTS_DIR / 'figures_03'` must exist.\n",
    "2. All paths and objects are loaded via the unified artifact loader from `02_modeling.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Project paths bootstrаp --- #\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# attempt to use the standard project paths module #\n",
    "try:\n",
    "    from paths import (\n",
    "        ART_DIR,\n",
    "        DATA_DIR,\n",
    "        INT_DIR,\n",
    "        MODELS_DIR,\n",
    "        NB_DIR,\n",
    "        PROC_DIR,\n",
    "        RAW_DIR,\n",
    "        REPORTS_DIR,\n",
    "        ROOT,\n",
    "    )\n",
    "\n",
    "    print(f\"[paths] ROOT = {ROOT}\")\n",
    "except Exception as e:\n",
    "    # fallback: minimal path setup without side effects\n",
    "    ROOT = Path.cwd()\n",
    "    DATA_DIR = ROOT / \"data\"\n",
    "    RAW_DIR = ROOT / \"raw\"\n",
    "    INT_DIR = ROOT / \"interim\"\n",
    "    PROC_DIR = ROOT / \"processed\"\n",
    "    ART_DIR = ROOT / \"artifacts\"\n",
    "    REPORTS_DIR = ROOT / \"reports\"\n",
    "    MODELS_DIR = ROOT / \"models\"\n",
    "    NB_DIR = ROOT / \"notebooks\"\n",
    "    print(f\"[paths:fallback] ROOT = {ROOT} | reason: {e!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fairness plotting utils & output dirs --- #\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use paths.py; fallback if unavailable #\n",
    "if \"REPORTS_DIR\" not in globals():\n",
    "    try:\n",
    "        from paths import REPORTS_DIR as _REPORTS_DIR\n",
    "    except Exception:\n",
    "        from pathlib import Path\n",
    "\n",
    "        ROOT = globals().get(\"ROOT\", Path.cwd())\n",
    "        _REPORTS_DIR = ROOT / \"data\" / \"reports\"\n",
    "    REPORTS_DIR = _REPORTS_DIR\n",
    "\n",
    "FIG_DIR_03 = REPORTS_DIR / \"figures_03\"\n",
    "FIG_DIR_03.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def _ensure_png(name: str) -> str:\n",
    "    return name if name.lower().endswith(\".png\") else f\"{name}.png\"\n",
    "\n",
    "\n",
    "def save_fig(name: str, fig=None, dpi: int = 200, close: bool = True):\n",
    "    \"\"\"\n",
    "    Save a figure to `reports/figures_03/`\n",
    "    Parameters:\n",
    "        name: file name, may omit '.png';\n",
    "        fig: matplotlib.figure.Figure or None -> current figure.\n",
    "    \"\"\"\n",
    "    if not isinstance(name, str) or not name:\n",
    "        raise TypeError('save_fig: \"name\" must be a non-empty figure')\n",
    "\n",
    "    # import pyplot only when needed #\n",
    "    if fig is None:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        fig = plt.gcf()\n",
    "    else:\n",
    "        if not hasattr(fig, \"savefig\"):\n",
    "            raise TypeError('save_fig: \"fig\" must have a savefig method')\n",
    "\n",
    "    fname = _ensure_png(name)\n",
    "    path = FIG_DIR_03 / fname\n",
    "\n",
    "    # save figure #\n",
    "    fig.savefig(path, dpi=dpi, bbox_inches=\"tight\")\n",
    "\n",
    "    # close if requested #\n",
    "    if close:\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "\n",
    "            plt.close(fig)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    print(f\"[saved] {path}\")\n",
    "    return Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fairness & Explainability: setup --- #\n",
    "\n",
    "import warnings\n",
    "\n",
    "# centralized warning suppression #\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "FAIRLEARN_OK = False\n",
    "THRESH_OPT_OK = False\n",
    "SHAP_OK = False\n",
    "\n",
    "# fairlearn and components #\n",
    "try:\n",
    "    from fairlearn.metrics import (\n",
    "        demographic_parity_difference,\n",
    "        equalized_odds_difference,\n",
    "    )\n",
    "    from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "    FAIRLEARN_OK = True\n",
    "    THRESH_OPT_OK = True\n",
    "    print(\"[ok] fairlearn imported\")\n",
    "except Exception as e:\n",
    "    print(f\"[warn] fairlearn unavailable: {e!r}\")\n",
    "\n",
    "# SHAP (global explainability) #\n",
    "try:\n",
    "    import shap\n",
    "\n",
    "    SHAP_OK = True\n",
    "    print(\"[ok] shap imported.\")\n",
    "except Exception as e:\n",
    "    print(f\"[warn] shap unavailable: {e!r}\")\n",
    "\n",
    "# utility readiness flag for Explainability/Fairness #\n",
    "EXPL_OK = FAIRLEARN_OK or SHAP_OK\n",
    "print(f\"[setup] FAIRLEARN_OK={FAIRLEARN_OK}, SHAP_OK={SHAP_OK}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Notebook preamble: silence & style --- #\n",
    "\n",
    "# enable autoreload if IPython is available #\n",
    "try:\n",
    "    ip = get_ipython()\n",
    "    if ip:\n",
    "        ip.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "        ip.run_line_magic(\"autoreload\", \"2\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import logging\n",
    "\n",
    "# suppress moisy loggers #\n",
    "for name in (\"matplotlib\", \"numba\"):\n",
    "    try:\n",
    "        logging.getLogger(name).setLevel(logging.ERROR)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# unified plt style #\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"axes.spines.top\": False,\n",
    "        \"axes.spines.right\": False,\n",
    "        \"axes.grid\": True,\n",
    "        \"grid.alpha\": 0.25,\n",
    "        \"figure.figsize\": (6.5, 4.0),\n",
    "        \"axes.titlesize\": 13,\n",
    "        \"axes.labelsize\": 11,\n",
    "        \"xtick.labelsize\": 10,\n",
    "        \"ytick.labelsize\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"[preamble] style set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paths & unified artifact loader --- #\n",
    "\n",
    "# stdlib\n",
    "import json\n",
    "from collections.abc import Iterable\n",
    "from pathlib import Path\n",
    "\n",
    "# third-party\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# first-party (with fallback to data/* structure)\n",
    "try:\n",
    "    from paths import ART_DIR, MODELS_DIR, REPORTS_DIR, ROOT as PATHS_ROOT  # noqa: I001\n",
    "except Exception:\n",
    "    # detect project root by marker files\n",
    "    PATHS_ROOT = Path.cwd()\n",
    "    _MARKERS = {\".git\", \"pyproject.toml\", \"README.md\"}\n",
    "    while not any((PATHS_ROOT / m).exists() for m in _MARKERS) and PATHS_ROOT.parent != PATHS_ROOT:\n",
    "        PATHS_ROOT = PATHS_ROOT.parent\n",
    "    ART_DIR = PATHS_ROOT / \"data\" / \"artifacts\"\n",
    "    MODELS_DIR = PATHS_ROOT / \"data\" / \"models\"\n",
    "    REPORTS_DIR = PATHS_ROOT / \"data\" / \"reports\"\n",
    "\n",
    "# derived locations #\n",
    "_NB_DIR = PATHS_ROOT / \"notebooks\"\n",
    "CANDIDATE_ART_DIRS = [ART_DIR, _NB_DIR / \"data\" / \"artifacts\"]\n",
    "CANDIDATE_MODEL_DIRS = [MODELS_DIR, ART_DIR, _NB_DIR / \"data\" / \"models\"]\n",
    "FIG_DIR_02 = REPORTS_DIR / \"figures_02\"\n",
    "FIG_DIR_03 = REPORTS_DIR / \"figures_03\"\n",
    "\n",
    "\n",
    "# helper: ensure length n for names #\n",
    "def _ensure_len(cols: Iterable | None, n: int) -> list[str]:\n",
    "    lst = list(cols) if isinstance(cols, list | tuple | set) else []\n",
    "    if len(lst) == n:\n",
    "        return [str(c) for c in lst]\n",
    "    return [f\"f{i}\" for i in range(n)]\n",
    "\n",
    "\n",
    "def _pick_file(names: list[str], dirs: list[Path]) -> Path | None:\n",
    "    for d in dirs:\n",
    "        for n in names:\n",
    "            p = Path(d) / n\n",
    "            if p.exists():\n",
    "                return p\n",
    "    return None\n",
    "\n",
    "\n",
    "# loaders: core #\n",
    "def load_feature_names():\n",
    "    p = _pick_file(\n",
    "        [\"feature_names.npy\", \"feature_names.json\", \"feature_names.txt\", \"feature_names.csv\"],\n",
    "        CANDIDATE_ART_DIRS,\n",
    "    )\n",
    "    if p is None:\n",
    "        return None, None\n",
    "    if p.suffix == \".npy\":\n",
    "        return np.load(p, allow_pickle=True).tolist(), p\n",
    "    if p.suffix == \".json\":\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\")), p\n",
    "    if p.suffix == \".txt\":\n",
    "        return [\n",
    "            line.strip() for line in p.read_text(encoding=\"utf-8\").splitlines() if line.strip()\n",
    "        ], p\n",
    "    if p.suffix == \".csv\":\n",
    "        return pd.read_csv(p, header=None)[0].tolist(), p\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def load_X_test_enc():\n",
    "    for nm in [\"X_test_enc.npy\", \"X_test_enc.csv\", \"X_test.npy\"]:\n",
    "        p = _pick_file([nm], CANDIDATE_ART_DIRS)\n",
    "        if p:\n",
    "            if p.suffix == \".npy\":\n",
    "                return np.load(p, allow_pickle=False), p\n",
    "            if p.suffix == \".csv\":\n",
    "                return pd.read_csv(p).to_numpy(), p\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def load_X_test_sens():\n",
    "    p = _pick_file(\n",
    "        [\"X_test_sensitive.csv\", \"X_test_sens.csv\", \"X_test_sensitive.parquet\"],\n",
    "        CANDIDATE_ART_DIRS,\n",
    "    )\n",
    "    if p is None:\n",
    "        return None, None\n",
    "    if p.suffix == \".parquet\":\n",
    "        return pd.read_parquet(p), p\n",
    "    if p.suffix == \".csv\":\n",
    "        return pd.read_csv(p), p\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def load_y_true_pred_proba():\n",
    "    y_true_p = _pick_file([\"y_true_test.npy\", \"y_true.npy\", \"y_test.npy\"], CANDIDATE_ART_DIRS)\n",
    "    y_pred_p = _pick_file([\"y_pred_best.npy\", \"y_pred.npy\"], CANDIDATE_ART_DIRS)\n",
    "    y_proba_p = _pick_file([\"y_proba_best.npy\", \"y_proba.npy\"], CANDIDATE_ART_DIRS)\n",
    "\n",
    "    def _load(p):\n",
    "        if p is None:\n",
    "            return None\n",
    "        if p.suffix == \".npy\":\n",
    "            return np.load(p, allow_pickle=False)\n",
    "        if p.suffix == \".csv\":\n",
    "            return pd.read_csv(p, header=None).iloc[:, 0].to_numpy()\n",
    "        return None\n",
    "\n",
    "    return (_load(y_true_p), y_true_p, _load(y_pred_p), y_pred_p, _load(y_proba_p), y_proba_p)\n",
    "\n",
    "\n",
    "def load_best_model():\n",
    "    cand = _pick_file(\n",
    "        [\n",
    "            \"model_best.joblib\",\n",
    "            \"LGBM_best.joblib\",\n",
    "            \"lgb_best.joblib\",\n",
    "            \"XGBoost_ES_best.joblib\",\n",
    "            \"XGBoost_ES.joblib\",\n",
    "        ],\n",
    "        CANDIDATE_MODEL_DIRS,\n",
    "    )\n",
    "    if cand is None:\n",
    "        return None, None\n",
    "    try:\n",
    "        return joblib.load(cand), cand\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] cannot load {cand}: {type(e).__name__}: {e}\")\n",
    "        return None, cand\n",
    "\n",
    "\n",
    "# loaders: extras used in 03 #\n",
    "def load_results_df():\n",
    "    p = _pick_file([\"results_df.csv\"], CANDIDATE_ART_DIRS)\n",
    "    return (pd.read_csv(p), p) if p and p.exists() else (None, None)\n",
    "\n",
    "\n",
    "def load_results_summary():\n",
    "    p = _pick_file([\"results_summary.csv\"], CANDIDATE_MODEL_DIRS)\n",
    "    return (pd.read_csv(p), p) if p and p.exists() else (None, None)\n",
    "\n",
    "\n",
    "def load_export_meta():\n",
    "    p = _pick_file([\"export_meta.json\"], CANDIDATE_ART_DIRS)\n",
    "    return (json.loads(p.read_text(encoding=\"utf-8\")), p) if p and p.exists() else (None, None)\n",
    "\n",
    "\n",
    "def load_test_groups():\n",
    "    p = _pick_file([\"test_groups.csv\"], CANDIDATE_ART_DIRS)\n",
    "    return (pd.read_csv(p), p) if p and p.exists() else (None, None)\n",
    "\n",
    "\n",
    "def load_y_test_optional():\n",
    "    p = _pick_file([\"y_test.npy\"], CANDIDATE_ART_DIRS)\n",
    "    return (np.load(p, allow_pickle=False), p) if p and p.exists() else (None, None)\n",
    "\n",
    "\n",
    "def load_y_pred_050_optional():\n",
    "    p = _pick_file([\"y_pred_050.npy\"], CANDIDATE_ART_DIRS)\n",
    "    return (np.load(p, allow_pickle=False), p) if p and p.exists() else (None, None)\n",
    "\n",
    "\n",
    "def load_y_score_optional():\n",
    "    p = _pick_file([\"y_score.npy\"], CANDIDATE_ART_DIRS)\n",
    "    return (np.load(p, allow_pickle=False), p) if p and p.exists() else (None, None)\n",
    "\n",
    "\n",
    "def list_figures02():\n",
    "    if not FIG_DIR_02.exists():\n",
    "        return []\n",
    "    return sorted([p for p in FIG_DIR_02.glob(\"*.png\")])\n",
    "\n",
    "\n",
    "# unified summary #\n",
    "def load_artifacts_summary():\n",
    "    fn, p_fn = load_feature_names()\n",
    "    Xte, p_xe = load_X_test_enc()\n",
    "    Xsens, p_xs = load_X_test_sens()\n",
    "    y_true, p_yt, y_pred, p_yp, y_proba, p_ypr = (*load_y_true_pred_proba(),)\n",
    "    y_test_opt, p_ytest = load_y_test_optional()\n",
    "    y_pred_050_opt, p_y050 = load_y_pred_050_optional()\n",
    "    y_score_opt, p_ys = load_y_score_optional()\n",
    "    results_df, p_resdf = load_results_df()\n",
    "    results_sum, p_ressum = load_results_summary()\n",
    "    meta, p_meta = load_export_meta()\n",
    "    groups, p_groups = load_test_groups()\n",
    "    mdl, p_m = load_best_model()\n",
    "    figs02 = list_figures02()\n",
    "    return {\n",
    "        \"feature_names\": (fn, p_fn),\n",
    "        \"X_test_enc\": (Xte, p_xe),\n",
    "        \"sensitive\": (Xsens, p_xs),\n",
    "        \"y_true\": (y_true, p_yt),\n",
    "        \"y_pred\": (y_pred, p_yp),\n",
    "        \"y_proba\": (y_proba, p_ypr),\n",
    "        \"y_test_opt\": (y_test_opt, p_ytest),\n",
    "        \"y_pred_050_opt\": (y_pred_050_opt, p_y050),\n",
    "        \"y_score_opt\": (y_score_opt, p_ys),\n",
    "        \"results_df\": (results_df, p_resdf),\n",
    "        \"results_summary\": (results_sum, p_ressum),\n",
    "        \"export_meta\": (meta, p_meta),\n",
    "        \"test_groups\": (groups, p_groups),\n",
    "        \"model\": (mdl, p_m),\n",
    "        \"figures02\": (figs02, FIG_DIR_02 if figs02 else None),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Unified call after initialization cell --- #\n",
    "\n",
    "A = load_artifacts_summary()\n",
    "\n",
    "model, model_path = A[\"model\"]\n",
    "feature_names, feature_names_path = A[\"feature_names\"]\n",
    "X_test_enc, X_test_enc_path = A[\"X_test_enc\"]\n",
    "X_test_sensitive, sens_path = A[\"sensitive\"]\n",
    "results_df, results_path = A[\"results_df\"]\n",
    "test_groups, groups_path = A[\"test_groups\"]\n",
    "y_true, y_true_path = A[\"y_true\"]\n",
    "y_proba, y_proba_path = A[\"y_proba\"]\n",
    "y_pred, y_pred_path = A[\"y_pred\"]\n",
    "\n",
    "print(\"[ok] model:\", model_path.name)\n",
    "for name, p in [\n",
    "    (\"feature_names\", feature_names_path),\n",
    "    (\"X_test_enc\", X_test_enc_path),\n",
    "    (\"sensitive\", sens_path),\n",
    "    (\"results_df\", results_path),\n",
    "    (\"test_groups\", groups_path),\n",
    "    (\"y_true\", y_true_path),\n",
    "    (\"y_proba\", y_proba_path),\n",
    "    (\"y_pred\", y_pred_path),\n",
    "]:\n",
    "    print(f\"[{'ok' if p else 'miss'}] {name}:\", (p.name if p else None))\n",
    "\n",
    "# aliases for backward compatibility #\n",
    "X_test_sens = X_test_sensitive\n",
    "y_true_test = y_true\n",
    "y_proba_best = y_proba\n",
    "y_pred_best = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- sanity-report from unified loader --- #\n",
    "\n",
    "assert \"A\" in globals()\n",
    "\n",
    "for name in [\n",
    "    \"model\",\n",
    "    \"feature_names\",\n",
    "    \"X_test_enc\",\n",
    "    \"sensitive\",\n",
    "    \"results_df\",\n",
    "    \"results_summary\",\n",
    "    \"test_groups\",\n",
    "    \"y_true\",\n",
    "    \"y_proba\",\n",
    "    \"y_test_opt\",\n",
    "    \"y_pred_050_opt\",\n",
    "    \"y_score_opt\",\n",
    "    \"export_meta\",\n",
    "    \"figures02\",\n",
    "]:\n",
    "    val, p = A[name]\n",
    "    shape = getattr(val, \"shape\", None)\n",
    "    if isinstance(val, list | tuple) and not shape:\n",
    "        shape = f\"len={len(val)}\"\n",
    "    print(f\"[{name:>14}] path={getattr(p, 'name', None)} shape={shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Provenance from export_meta.json --- #\n",
    "\n",
    "meta, p_meta = A[\"export_meta\"]\n",
    "if meta:\n",
    "    print(f\"[meta] best_model={meta.get('best_model')} | ts={meta.get('timestamp')}\")\n",
    "    arts = meta.get(\"artifacts\") or []\n",
    "    print(f\"[meta] listed artifacts: {len(arts)}\")\n",
    "else:\n",
    "    print(\"[meta] export_meta.json not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Leaderboard (results_summary.csv) --- #\n",
    "\n",
    "res_sum, p_sum = A[\"results_summary\"]\n",
    "\n",
    "if res_sum is not None:\n",
    "    df = res_sum.copy()\n",
    "    # sort by test_f1 then roc_auc #\n",
    "    cols = [\n",
    "        c\n",
    "        for c in [\n",
    "            \"model\",\n",
    "            \"test_f1\",\n",
    "            \"test_roc_auc\",\n",
    "            \"test_precision\",\n",
    "            \"test_recall\",\n",
    "            \"test_accuracy\",\n",
    "        ]\n",
    "        if c in df.columns\n",
    "    ]\n",
    "    df = df.sort_values(\n",
    "        by=[c for c in [\"test_f1\", \"test_roc_auc\"] if c in df.columns], ascending=False\n",
    "    )\n",
    "    display(df[cols].head(10))\n",
    "else:\n",
    "    print(\"[skip] results_summary.csv absent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Consistency check: y_true vs y_test --- #\n",
    "\n",
    "y_true, _ = A[\"y_true\"]\n",
    "y_test_opt, p_ytest = A[\"y_test_opt\"]\n",
    "if y_true is not None and y_test_opt is not None:\n",
    "    same = np.array_equal(y_true, y_test_opt)\n",
    "    print(f\"[check] y_true vs y_test: {'OK' if same else 'MISMATCH'}\")\n",
    "    if not same:\n",
    "        raise RuntimeError(\"y_true_test.npy and y_test.npy differ.\")\n",
    "else:\n",
    "    print(\"[check] y_test optional not found -> skip.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# --- Sanity check: shapes, NaNs, y_proba distribution --- #\n",
    "\n",
    "**Goal.** Verify input data consistency and correctness of probabilistic predictions before conducting fairness and explainability analysis.\n",
    "\n",
    "**Verification steps:**\n",
    "1. **Shapes.** Ensure that the lengths of `X_test_enc`, `X_test_sens`, `y_true_test`, and `y_proba_best` match. Any mismatch indicates inconsistency between features, labels, and model outputs.  \n",
    "2. **Missing values.** Check for `NaN`s in `y_proba_best` and in sensitive features. Their presence may distort fairness metrics and probability distributions.  \n",
    "3. **Probability distribution:**\n",
    "   - Print basic statistics (`min`, `max`, `mean`, `std`);\n",
    "   - Plot a histogram of `y_proba_best`;\n",
    "   - Visually assess class balance — check if the probabilities are skewed toward 0 or 1.  \n",
    "4. **Group slices.** If `X_test_sens` includes `sex`, `race`, or `age_group`, compare mean values of `y_proba_best` across these groups. This serves as an early indicator of possible model bias.\n",
    "\n",
    "**Interpretation:**\n",
    "1. Matching shapes and no missing values confirm artifact consistency.  \n",
    "2. A balanced `y_proba_best` distribution suggests that the model is not overconfident.  \n",
    "3. Group-level differences at this stage are diagnostic signals only, not statistically significant fairness metrics.\n",
    "\n",
    "**Expected result.** A histogram of `y_proba_best` is plotted, shapes are validated, and no `NaN`s are present. The data is confirmed suitable for fairness metric computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- sanity-check: shapes, NaNs, y_proba distribution --- #\n",
    "\n",
    "# check presence of artifacts from A #\n",
    "assert all(k in globals() for k in [\"y_true_test\", \"y_pred_best\", \"y_proba_best\", \"X_test_sens\"]), (\n",
    "    \"No aliases from unified loader.\"\n",
    ")\n",
    "\n",
    "# size consistency #\n",
    "n = len(y_true_test)\n",
    "assert len(y_pred_best) == n and len(y_proba_best) == n, \"Lengths of y_* do not match\"\n",
    "assert len(X_test_sens) == n, \"Length of X_test_sens does not match y\"\n",
    "\n",
    "# NaN and valid range #\n",
    "assert np.isfinite(y_proba_best).all(), \"NaN/Inf present in y_proba_best\"\n",
    "assert (y_proba_best >= 0).all() and (y_proba_best <= 1).all(), \"y_proba_best outside [0, 1]\"\n",
    "\n",
    "# probability histogram #\n",
    "plt.figure()\n",
    "plt.hist(y_proba_best, bins=30)\n",
    "plt.xlabel(\"y_proba_best\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Distribution of predicted probabilties\")\n",
    "plt.tight_layout()\n",
    "save_fig(\"hist_y_proba_best.png\")\n",
    "plt.show()\n",
    "\n",
    "# baseline selection rate by groups (t = 0.5) #\n",
    "t = 0.5\n",
    "y_pred_05 = (y_proba_best >= t).astype(\"int8\")\n",
    "\n",
    "pred_ser = pd.Series(y_pred_05, index=X_test_sens.index, name=\"y_pred_05\")\n",
    "\n",
    "for gcol in [c for c in [\"sex\", \"race\", \"age_group\"] if c in X_test_sens.columns]:\n",
    "    grp = X_test_sens[gcol].fillna(\"NA\")\n",
    "    rates = pred_ser.groupby(grp).mean().sort_values(ascending=False)\n",
    "    print(f\"[{gcol}] selection rate @ t = 0.5:\")\n",
    "    print(rates, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# --- Defining Sensitive Features and Groups --- #\n",
    "\n",
    "**Goal.** Specify the list of sensitive features used to assess model fairness and ensure groups are correctly prepared for downstream metrics.\n",
    "\n",
    "**Feature selection.** For the *Census Income Classifier* task, the sensitive features are:\n",
    "1. `sex` - binary (Male/Female).\n",
    "2. `race` - categorical (White/Black/Asian-Pac-Islander/Amer-Indian-Eskimo/Other).\n",
    "3. `age_group` - discretized age (e.g., 17–29, 30–44, 45–59, 60+), if present.\n",
    "\n",
    "**Data handling:**\n",
    "1. Verify that these columns exist in `X_test_sens`.\n",
    "2. Replace missing values with the `'NA'` category to keep groups complete.\n",
    "3. Cast categorical features to `category` dtype with a fixed category order to avoid accidental alphabetical ordering in comparisons.\n",
    "4. Optionally aggregate low-count categories into `'Other'`.\n",
    "\n",
    "**Interpretation.** Properly defining sensitive features ensures that fairness metrics (Demographic Parity, Equalized Odds, etc.) are computed on comparable groups and remain interpretable.\n",
    "\n",
    "**Expected result.** A DataFrame `X_test_sens` with processed sensitive features, ready for use in fairness analysis blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sensitive features and groups --- #\n",
    "\n",
    "# ensure X_test_sens from unified loader #\n",
    "assert \"X_test_sens\" in globals(), \"X_test_sens must come from unified loader.\"\n",
    "\n",
    "CAND_SENSITIVE = [\"sex\", \"race\", \"age_group\"]\n",
    "SENSITIVE = [c for c in CAND_SENSITIVE if c in X_test_sens.columns]\n",
    "print(\"Using SENSITIVE:\", SENSITIVE)\n",
    "\n",
    "# meaningful order for age_group #\n",
    "if \"age_group\" in SENSITIVE:\n",
    "    desired_order = [\"18-25\", \"26-45\", \"46-65\", \"65+\"]\n",
    "    try:\n",
    "        X_test_sens[\"age_group\"] = pd.Categorical(\n",
    "            X_test_sens[\"age_group\"], categories=desired_order, ordered=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Could not set order for age_group:\", e)\n",
    "\n",
    "# collect unique values for each sensitive variable #\n",
    "GROUP_VALUES = {}\n",
    "for col in SENSITIVE:\n",
    "    vals = X_test_sens[col].dropna().unique().tolist()\n",
    "    if (col == \"age_group\") and hasattr(X_test_sens[col], \"cat\"):\n",
    "        vals = [v for v in X_test_sens[col].cat.categories if v in set(X_test_sens[col].dropna())]\n",
    "        GROUP_VALUES[col] = list(vals)\n",
    "\n",
    "# report group presence and sample sizes #\n",
    "for col in SENSITIVE:\n",
    "    print(f\"\\n[{col}] groups and sample sizes:\")\n",
    "    df_sizes = (\n",
    "        X_test_sens[col]\n",
    "        .value_counts(dropna=False)\n",
    "        .rename_axis(\"group\")\n",
    "        .reset_index(name=\"n\")\n",
    "        .assign(share=lambda d: d[\"n\"] / len(X_test_sens))\n",
    "    )\n",
    "    display(df_sizes)\n",
    "\n",
    "print(\"\\nGROUP_VALUES =\", GROUP_VALUES)\n",
    "\n",
    "# primary sensitive variable #\n",
    "PRIMARY_SENS = SENSITIVE[0] if SENSITIVE else None\n",
    "print(\"PRIMARY_SENS:\", PRIMARY_SENS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# --- Baseline: metrics at a common threshold (0.5) --- #\n",
    "\n",
    "**Goal.** Compute key performance metrics at a fixed threshold `t=0.5`, used as the baseline for analyzing the *quality <-> fairness* trade-off.\n",
    "\n",
    "**Evaluation metrics:**\n",
    "1. Accuracy - share of correctly classified observations.\n",
    "2. Precision - correctness of positive predictions.\n",
    "3. Recall - coverage of the positive class.\n",
    "4. F1-score - harmonic mean of Precision and Recall.\n",
    "5. ROC-AUC - ability to separate classes independent of threshold.\n",
    "\n",
    "**Interpretation.** Threshold `0.5` is a \"neutral\" cutoff not adjusted for class imbalance or fairness requirements. The results serve as:\n",
    "1. A starting point for selecting the optimal threshold `t*`.\n",
    "2. A control baseline when comparing with post-processing (`ThresholdOptimizer`).\n",
    "3. A reference to assess the impact of fairness constraints on quality.\n",
    "\n",
    "**Expected result.** A table or dict with Accuracy, Precision, Recall, F1, and ROC-AUC at `t=0.5`. These will be used to build the threshold curve and the Pareto plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline metrics at t = 0.5 #\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# binary predictions at threshold 0.5 #\n",
    "y_pred_05 = (y_proba_best >= 0.5).astype(int)\n",
    "\n",
    "base_metrics = {\n",
    "    \"accuracy\": accuracy_score(y_true_test, y_pred_05),\n",
    "    \"f1\": f1_score(y_true_test, y_pred_05),\n",
    "    \"precision\": precision_score(y_true_test, y_pred_05, zero_division=0),\n",
    "    \"roc_auc\": roc_auc_score(y_true_test, y_proba_best),\n",
    "    \"recall\": recall_score(y_true_test, y_pred_05, zero_division=0),\n",
    "}\n",
    "\n",
    "print(\"Baseline metrics (t = 0.5):\")\n",
    "display(pd.DataFrame([base_metrics]).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# --- Fairness across groups at t = 0.5 --- #\n",
    "\n",
    "**Goal.** Evaluate how the model’s predictive performance differs across sensitive groups at the baseline threshold `t=0.5`.\n",
    "\n",
    "**Metrics used:**\n",
    "1. **Selection Rate (SR)** - proportion of samples predicted as positive in each group.  \n",
    "2. **True Positive Rate (TPR)** and **False Positive Rate (FPR)** - sensitivity and false positive rate by group.  \n",
    "3. **Precision, Recall, F1** - classic performance metrics computed separately per sensitive group.  \n",
    "4. **Demographic Parity Difference** - disparity in positive prediction rates between groups.\n",
    "\n",
    "**Interpretation:**\n",
    "1. Differences in SR and TPR/FPR indicate potential model bias across subpopulations.  \n",
    "2. If significant disparities are observed (e.g., one group with high SR and another with low SR), fairness constraints may be needed during post-processing.  \n",
    "3. Threshold `0.5` serves as a reference point to gauge the magnitude of imbalance before optimizing for the threshold `t*`.\n",
    "\n",
    "**Visualization and outputs:**\n",
    "1. Group-wise metric tables are saved as CSV files in `reports/figures_03/`.  \n",
    "2. Bar charts are generated for each metric (`bar_f1_by_*.png`, `bar_precision_by_*.png`, `bar_recall_by_*.png`).  \n",
    "3. Confusion matrices per group can also be visualized to inspect error types.\n",
    "\n",
    "**Expected result.** Summary tables and visualizations of fairness metrics at `t=0.5`. These serve as the foundation for optimal threshold selection in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Group fairness metrics at t = 0.5 --- #\n",
    "\n",
    "from fairlearn.metrics import demographic_parity_ratio, equalized_odds_difference\n",
    "\n",
    "t = 0.5\n",
    "y_pred = (y_proba_best >= t).astype(\"int8\")\n",
    "\n",
    "# DP/EOD @ t=0.5 #\n",
    "# choose the sensitive feature #\n",
    "sf_col = globals().get(\"PRIMARY_SENS\", None) or \"sex\"\n",
    "sf = X_test_sens[sf_col]\n",
    "\n",
    "dp_diff = demographic_parity_difference(y_true=y_true_test, y_pred=y_pred_05, sensitive_features=sf)\n",
    "dp_ratio = demographic_parity_ratio(y_true=y_true_test, y_pred=y_pred_05, sensitive_features=sf)\n",
    "eod_diff = equalized_odds_difference(y_true=y_true_test, y_pred=y_pred_05, sensitive_features=sf)\n",
    "\n",
    "print(\n",
    "    \"[fairness@0.5] \"\n",
    "    f\"{sf_col}: DP diff={dp_diff:.4f}, \"\n",
    "    f\"DP ratio={dp_ratio:.4f}, EOD diff={eod_diff:.4f}\"\n",
    ")\n",
    "\n",
    "_agg = {\n",
    "    \"settings\": \"t=0.5\",\n",
    "    \"sf_col\": sf_col,\n",
    "    \"dp_diff\": dp_diff,\n",
    "    \"dp_ratio\": dp_ratio,\n",
    "    \"eod_diff\": eod_diff,\n",
    "}\n",
    "pd.DataFrame([_agg]).to_csv(FIG_DIR_03 / \"fairness_overview_t05.csv\", index=False)\n",
    "\n",
    "\n",
    "def _prec(y_true, y_pred):\n",
    "    return precision_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "\n",
    "def _rec(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "\n",
    "def _f1(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "\n",
    "def _acc(y_true, y_pred):\n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- score diagnostics from y_score.npy --- #\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "y_score_opt, _ = A[\"y_score_opt\"]\n",
    "y_true, _ = A[\"y_true\"]\n",
    "if y_score_opt is not None:\n",
    "    y_score_norm = (y_score_opt - np.min(y_score_opt)) / (np.ptp(y_score_opt) + 1e-12)\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_score_norm, n_bins=10, strategy=\"quantile\")\n",
    "    df_cal = pd.DataFrame({\"preb_pred\": prob_pred, \"prob_true\": prob_true})\n",
    "    df_cal.to_csv(ART_DIR / \"calibration_from_score_quantile.csv\", index=False)\n",
    "    print(\"[ok] saved:\", ART_DIR / \"calibration_from_score_quantile.csv\")\n",
    "else:\n",
    "    print(\"[skip] y_score.npy absent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# --- Threshold curve and Pareto: quality vs Demographic Parity --- #\n",
    "\n",
    "**Goal.** Find an operating classification threshold `t*` that balances model quality and fairness.\n",
    "\n",
    "**Method:**\n",
    "1. Run a threshold sweep: compute quality metrics (Accuracy, F1) and fairness metrics (Demographic Parity Difference/Ratio, Equalized Odds Difference) on a grid `t ∈ [0, 1]`.\n",
    "2. Plot:\n",
    "   - `accuracy_f1_vs_threshold.png` - quality vs threshold;\n",
    "   - `dp_vs_threshold.png` - fairness vs threshold;\n",
    "   - `Pareto_f1_vs_dp.png` - F1 vs Demographic Parity trade-off.\n",
    "3. Choose `t*` at a Pareto-optimal point where improving one metric worsens the other.\n",
    "\n",
    "**Interpretation:**\n",
    "1. For biased models, reducing Demographic Parity Difference often lowers F1.\n",
    "2. The optimal `t*` stabilizes this trade-off.\n",
    "3. Use `t*` for confusion matrices, fairness metrics, and calibration plots.\n",
    "\n",
    "**Visualization and outputs:**\n",
    "1. Save plots to `reports/figures_03`.\n",
    "2. Save a CSV of per-threshold metrics (Accuracy, F1, DP, EOD) for audit transparency.\n",
    "3. Save the chosen threshold `t*` as `t_star.npy`.\n",
    "\n",
    "**Expected result.** Threshold curves and a Pareto plot are produced. The operating threshold `t*` is selected and saved, reflecting an optimal balance between quality and fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Threshold sweep + Pareto + select t* + save figures --- #\n",
    "\n",
    "# fill NaNs in sensitive series #\n",
    "def _sens_fill(s: pd.Series) -> pd.Series:\n",
    "    s = s.copy()\n",
    "    if isinstance(s.dtype, pd.CategoricalDtype):\n",
    "        if \"NA\" not in s.cat.categories:\n",
    "            s = s.cat.add_categories([\"NA\"])\n",
    "        return s.fillna(\"NA\")\n",
    "    else:\n",
    "        return s.fillna(\"NA\")\n",
    "\n",
    "\n",
    "# which sensitive features are present in X_test_sens #\n",
    "sens_cols = [c for c in [\"sex\", \"race\", \"age_group\"] if c in X_test_sens.columns]\n",
    "if len(sens_cols) == 0:\n",
    "    raise RuntimeError(\n",
    "        \"None of the expected sensitive features were found in X_test_sens: sex/race/age_group.\"\n",
    "    )\n",
    "\n",
    "# ensure y_true_test/y_proba_best are present #\n",
    "assert \"y_true_test\" in globals() and \"y_proba_best\" in globals(), (\n",
    "    \"Нужны y_true_test и y_proba_best из 02_modeling.\"\n",
    ")\n",
    "\n",
    "# align indices to X_test_sens #\n",
    "y_true = pd.Series(y_true_test, index=X_test_sens.index)\n",
    "y_proba = pd.Series(y_proba_best, index=X_test_sens.index)\n",
    "\n",
    "try:\n",
    "    auc_once = roc_auc_score(y_true, y_proba)\n",
    "except Exception:\n",
    "    auc_once = np.nan\n",
    "\n",
    "# build threshold table #\n",
    "ts = np.round(np.arange(0.05, 0.95, 0.01), 2)\n",
    "rows = []\n",
    "for t in ts:\n",
    "    # binarize probabilities on the same index\n",
    "    y_pred_t = (y_proba >= t).astype(int)\n",
    "\n",
    "    # base metrics on aligned series\n",
    "    m_f1 = f1_score(y_true, y_pred_t, zero_division=0)\n",
    "    m_pr = precision_score(y_true, y_pred_t, zero_division=0)\n",
    "    m_rc = recall_score(y_true, y_pred_t, zero_division=0)\n",
    "    m_acc = accuracy_score(y_true, y_pred_t)\n",
    "    m_auc = auc_once\n",
    "\n",
    "    # DP difference by max gam among available sensitive features\n",
    "    dp_diffs = []\n",
    "    for col in sens_cols:\n",
    "        s = _sens_fill(X_test_sens[col])\n",
    "        grp_name = s.name or col\n",
    "\n",
    "        # categorize with a fixed group order\n",
    "        if grp_name in GROUP_VALUES:\n",
    "            s = pd.Series(\n",
    "                pd.Categorical(s, categories=GROUP_VALUES[grp_name], ordered=False),\n",
    "                index=y_pred_t.index,\n",
    "                name=grp_name,\n",
    "            )\n",
    "        else:\n",
    "            s = pd.Series(s, index=y_pred_t.index, name=grp_name)\n",
    "\n",
    "        # upcoming pandas default: observed=True\n",
    "        grp_rates = y_pred_t.groupby(s, observed=True).mean()\n",
    "\n",
    "        # restore full group order\n",
    "        if grp_name in GROUP_VALUES:\n",
    "            grp_rates = grp_rates.reindex(GROUP_VALUES[grp_name])\n",
    "\n",
    "        # compute disparate impact/difference\n",
    "        dp = float(grp_rates.max() - grp_rates.min())\n",
    "        dp_diffs.append(dp)\n",
    "\n",
    "    # if all NaN -> return np.nan #\n",
    "    dp_diff_max = float(np.nanmax(dp_diffs)) if len(dp_diffs) else np.nan\n",
    "\n",
    "    # EOD #\n",
    "    eod_diffs = []\n",
    "    try:\n",
    "        mask_pos = y_true == 1\n",
    "        for col in sens_cols:\n",
    "            s = _sens_fill(X_test_sens[col])\n",
    "            grp_name = s.name or col\n",
    "\n",
    "            if grp_name in GROUP_VALUES:\n",
    "                s = pd.Series(\n",
    "                    pd.Categorical(s, categories=GROUP_VALUES[grp_name], ordered=False),\n",
    "                    index=y_true.index,\n",
    "                    name=grp_name,\n",
    "                )\n",
    "                groups_iter = GROUP_VALUES[grp_name]\n",
    "            else:\n",
    "                s = pd.Series(s, index=y_true.index, name=grp_name)\n",
    "                groups_iter = pd.unique(s)\n",
    "\n",
    "            tprs = []\n",
    "            for g in groups_iter:\n",
    "                m = (s == g) & mask_pos\n",
    "                if m.sum() > 0:\n",
    "                    tprs.append((y_pred_t[m] == 1).mean())\n",
    "            if len(tprs) > 1:\n",
    "                eod_diffs.append(float(np.max(tprs) - np.min(tprs)))\n",
    "        eod_diff_max = float(np.nanmax(eod_diffs)) if eod_diffs else np.nan\n",
    "    except Exception:\n",
    "        eod_diff_max = np.nan\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"t\": t,\n",
    "            \"f1\": m_f1,\n",
    "            \"precision\": m_pr,\n",
    "            \"recall\": m_rc,\n",
    "            \"accuracy\": m_acc,\n",
    "            \"auc\": m_auc,\n",
    "            \"dp_diff_max\": dp_diff_max,\n",
    "            \"eod_diff_max\": eod_diff_max,\n",
    "        }\n",
    "    )\n",
    "\n",
    "scan_df = pd.DataFrame(rows)\n",
    "\n",
    "# export csv #\n",
    "ART_DIR = globals().get(\"ART_DIR\", Path(\"data\") / \"artifacts\")\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "out_csv = ART_DIR / \"fairness_threshold_scan.csv\"\n",
    "scan_df.to_csv(out_csv, index=False)\n",
    "print(f\"Saved fairness_threshold_scan.csv -> {out_csv}\")\n",
    "\n",
    "# copy to reports #\n",
    "REPORTS_DIR = globals().get(\"REPORTS_DIR\", Path(\"data\") / \"reports\")\n",
    "FIG_DIR_03 = REPORTS_DIR / \"figures_03\"\n",
    "FIG_DIR_03.mkdir(parents=True, exist_ok=True)\n",
    "out_csv_rep = FIG_DIR_03 / \"fairness_threshold_scan.csv\"\n",
    "scan_df.to_csv(out_csv_rep, index=False)\n",
    "print(f\"Copied to reports -> {out_csv_rep}\")\n",
    "\n",
    "# Pareto: F1 vs DP #\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "ax.scatter(scan_df[\"dp_diff_max\"], scan_df[\"f1\"], s=14, alpha=0.85)\n",
    "ax.set_xlabel(\"Demographic Parity (max group diff)\")\n",
    "ax.set_ylabel(\"F1\")\n",
    "ax.set_title(\"Pareto: F1 vs DP\")\n",
    "\n",
    "# highlight best-by-f1 point #\n",
    "best_idx = int(scan_df[\"f1\"].idxmax())\n",
    "ax.scatter(\n",
    "    [scan_df.loc[best_idx, \"dp_diff_max\"]], [scan_df.loc[best_idx, \"f1\"]], s=60, edgecolors=\"k\"\n",
    ")\n",
    "for k in [\"dp_diff_max\", \"f1\", \"t\"]:\n",
    "    ax.annotate(\n",
    "        f\"{k}={scan_df.loc[best_idx, k]:.3f}\",\n",
    "        (scan_df.loc[best_idx, \"dp_diff_max\"], scan_df.loc[best_idx, \"f1\"]),\n",
    "        xytext=(10, 10),\n",
    "        textcoords=\"offset points\",\n",
    "        fontsize=8,\n",
    "    )\n",
    "save_fig(\"Pareto_f1_vs_dp.png\", fig)\n",
    "\n",
    "# metrics vs threshold #\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "ax.plot(scan_df[\"t\"], scan_df[\"accuracy\"], label=\"accuracy\")\n",
    "ax.plot(scan_df[\"t\"], scan_df[\"f1\"], label=\"f1\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"threshold t\")\n",
    "ax.set_ylabel(\"score\")\n",
    "ax.set_title(\"Accuracy & F1 vs threshold\")\n",
    "ax.legend()\n",
    "save_fig(\"accuracy_f1_vs_threshold.png\", fig)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "ax.plot(scan_df[\"t\"], scan_df[\"dp_diff_max\"])\n",
    "ax.set_xlabel(\"threshold t\")\n",
    "ax.set_ylabel(\"DP max diff\")\n",
    "ax.set_title(\"DP (max group diff) vs threshold\")\n",
    "save_fig(\"dp_vs_threshold.png\", fig)\n",
    "\n",
    "# EOD #\n",
    "if scan_df[\"eod_diff_max\"].notna().any():\n",
    "    fig, ax = plt.subplots(figsize=(7, 5))\n",
    "    ax.plot(scan_df[\"t\"], scan_df[\"eod_diff_max\"])\n",
    "    ax.set_xlabel(\"threshold t\")\n",
    "    ax.set_ylabel(\"EOD max diff\")\n",
    "    ax.set_title(\"EOD (max group diff) vs threshold\")\n",
    "    save_fig(\"eod_vs_threshold.png\", fig)\n",
    "\n",
    "# Pareto-front: F1 vs DP #\n",
    "scan_df[\"dp_abs\"] = scan_df[\"dp_diff_max\"].abs()\n",
    "\n",
    "pareto_idx = []\n",
    "for i, r in scan_df.iterrows():\n",
    "    dominated = (\n",
    "        (scan_df[\"f1\"] >= r[\"f1\"])\n",
    "        & (scan_df[\"dp_abs\"] <= r[\"dp_abs\"])\n",
    "        & ((scan_df[\"f1\"] > r[\"f1\"]) | (scan_df[\"dp_abs\"] < r[\"dp_abs\"]))\n",
    "    ).any()\n",
    "    if not dominated:\n",
    "        pareto_idx.append(i)\n",
    "pareto_df = scan_df.loc[pareto_idx]\n",
    "\n",
    "# selection rate: minimal |DP|, tie-breaker by maximal F1 #\n",
    "t_star = float(pareto_df.sort_values([\"dp_abs\", \"f1\"], ascending=[True, False]).iloc[0][\"t\"])\n",
    "\n",
    "# save t* as an artifact #\n",
    "np.save(ART_DIR / \"t_star.npy\", np.array([t_star], dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare metrics: t* vs t-0.5 --- #\n",
    "\n",
    "assert \"t_star\" in globals()\n",
    "y_true, _ = A[\"y_true\"]\n",
    "y_proba_best, _ = A[\"y_proba\"]\n",
    "y_pred_050_opt, _ = A[\"y_pred_050_opt\"]\n",
    "\n",
    "\n",
    "def _metrics(y_true, y_pred, y_score=None):\n",
    "    out = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "    if y_score is not None:\n",
    "        try:\n",
    "            out[\"roc_auc\"] = roc_auc_score(y_true, y_score)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return out\n",
    "\n",
    "\n",
    "# t* #\n",
    "y_pred_star = (y_proba_best >= float(t_star)).astype(int)\n",
    "m_star = _metrics(y_true, y_pred_star, y_proba_best)\n",
    "\n",
    "# t=0.5 #\n",
    "if y_pred_050_opt is not None:\n",
    "    m_050 = _metrics(y_true, y_pred_050_opt, y_proba_best)\n",
    "    comp = pd.DataFrame([m_050, m_star], index=[\"t=0.5\", \"t*\"])\n",
    "    display(comp)\n",
    "    comp.to_csv(ART_DIR / \"metrics_t050_vs_tstar.csv\", index=True)\n",
    "    print(\"[ok] saved:\", ART_DIR / \"metrics_050_vs_tstar.csv\")\n",
    "else:\n",
    "    print(\"[skip] y_pred_050.npy absent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Group metrics @ t* -> CSV --- #\n",
    "\n",
    "# input objects: #\n",
    "# y_true: vector of true labels #\n",
    "# y_proba_best: positive-class probabilities #\n",
    "# t_star: selected threshold #\n",
    "# X_test_sensitive: DataFrame with sensitive attributes #\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "assert \"t_star\" in globals(), \"t_star not found\"\n",
    "assert \"y_proba_best\" in globals(), \"y_proba_best not found\"\n",
    "assert \"y_true\" in globals(), \"y_true not found\"\n",
    "assert \"X_test_sensitive\" in globals(), \"X_test_sensitive not found\"\n",
    "\n",
    "# output directories #\n",
    "ART_DIR = globals().get(\"ART_DIR\", Path(\"data\") / \"artifacts\")\n",
    "FIG_DIR_03 = globals().get(\"REPORTS_DIR\", Path(\"data\") / \"reports\") / \"figures_03\"\n",
    "\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR_03.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# predictions at t* #\n",
    "y_pred_star = (y_proba_best >= float(t_star)).astype(int)\n",
    "\n",
    "rows_metrics = []\n",
    "rows_selrate = []\n",
    "\n",
    "for col in X_test_sensitive.columns:\n",
    "    s = X_test_sensitive[col]\n",
    "    # drop NaNs from grouping\n",
    "    for g in sorted(s.dropna().unique()):\n",
    "        m = (s == g).values\n",
    "        n = int(m.sum())\n",
    "        if n == 0:\n",
    "            continue\n",
    "        yt = y_true[m]\n",
    "        yp = y_pred_star[m]\n",
    "\n",
    "        # metrics\n",
    "        p, r, f1, _ = precision_recall_fscore_support(yt, yp, average=\"binary\", zero_division=0)\n",
    "        acc = accuracy_score(yt, yp)\n",
    "\n",
    "        rows_metrics.append(\n",
    "            {\n",
    "                \"threshold\": float(t_star),\n",
    "                \"group_col\": col,\n",
    "                \"group\": g,\n",
    "                \"n\": n,\n",
    "                \"precision\": float(p),\n",
    "                \"recall\": float(r),\n",
    "                \"f1\": float(f1),\n",
    "                \"accuracy\": float(acc),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # selection rate\n",
    "        sel = float(yp.mean()) if n > 0 else 0.0\n",
    "        rows_selrate.append(\n",
    "            {\n",
    "                \"threshold\": float(t_star),\n",
    "                \"group_col\": col,\n",
    "                \"group\": g,\n",
    "                \"n\": n,\n",
    "                \"selection_rate\": sel,\n",
    "            }\n",
    "        )\n",
    "\n",
    "df_metrics = pd.DataFrame(\n",
    "    rows_metrics,\n",
    "    columns=[\"threshold\", \"group_col\", \"group\", \"n\", \"precision\", \"recall\", \"f1\", \"accuracy\"],\n",
    ")\n",
    "df_selrate = pd.DataFrame(\n",
    "    rows_selrate, columns=[\"threshold\", \"group_col\", \"group\", \"n\", \"selection_rate\"]\n",
    ")\n",
    "\n",
    "# save to report and duplicate to artifacts #\n",
    "p1 = FIG_DIR_03 / \"group_metrics_t_star.csv\"\n",
    "p2 = FIG_DIR_03 / \"selection_rates_t_star.csv\"\n",
    "a1 = ART_DIR / \"group_metrics_t_star.csv\"\n",
    "a2 = ART_DIR / \"selection_rates_t_star.csv\"\n",
    "\n",
    "df_metrics.to_csv(p1, index=False)\n",
    "df_selrate.to_csv(p2, index=False)\n",
    "df_metrics.to_csv(a1, index=False)\n",
    "df_selrate.to_csv(a2, index=False)\n",
    "\n",
    "print(f\"[ok] Saved: {p1}\")\n",
    "print(f\"[ok] Saved: {p2}\")\n",
    "print(f\"[ok] Saved: {a1}\")\n",
    "print(f\"[ok] Saved: {a2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "# --- Post-Processing: ThresholdOptimizer (DemographicParity / EqualizedOdds) --- #\n",
    "\n",
    "**Goal.** Apply post-processing to adjust model decisions under fairness constraints and assess its impact on quality and fairness.\n",
    "\n",
    "**Approach:**\n",
    "1. Use `ThresholdOptimizer` from fairlearn to adapt the classification threshold separately for each subgroup of a sensitive feature.\n",
    "2. Consider two constraint types:\n",
    "   - **Demographic Parity** - equalize the share of positive outcomes across groups;\n",
    "   - **Equalized Odds** - equalize sensitivity (TPR) and specificity (FPR) across groups.\n",
    "\n",
    "**Comparison methodology:**\n",
    "1. Run optimization separately for each constraint using the trained model as a black box (`estimator='prefitted'`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Post‑processing: ThresholdOptimizer (DP / EqOdds) --- #\n",
    "\n",
    "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*sensitive features not unique.*\")\n",
    "\n",
    "\n",
    "# fill NaNs in sensitive attributes #\n",
    "def _sens_fill(s):\n",
    "    s = s.copy()\n",
    "    if isinstance(s.dtype, pd.CategoricalDtype):\n",
    "        if \"NA\" not in s.cat.categories:\n",
    "            s = s.cat.add_categories([\"NA\"])\n",
    "        return s.fillna(\"NA\")\n",
    "    else:\n",
    "        return s.fillna(\"NA\")\n",
    "\n",
    "\n",
    "# choose an available sensitive attribute #\n",
    "sens_cols = [c for c in [\"sex\", \"race\", \"age_group\"] if c in X_test_sens.columns]\n",
    "if not sens_cols:\n",
    "    print(\"No sensitive features - skipping ThresholdOptimizer\")\n",
    "else:\n",
    "    gcol = \"sex\" if \"sex\" in X_test_sens.columns else sens_cols[0]\n",
    "    sf = _sens_fill(X_test_sens[gcol])\n",
    "\n",
    "    class _ScoresEstimator:\n",
    "        \"\"\"A surrogate classifier that returns precomputed probabilities.\"\"\"\n",
    "\n",
    "        def __init__(self, scores):\n",
    "            self.scores = np.asarray(scores)\n",
    "            # to prevent check_fitted from raising errors when prefit=True\n",
    "            self.fitted_ = True\n",
    "\n",
    "        def get_params(self, deep=True):\n",
    "            return {\"scores\": self.scores}\n",
    "\n",
    "        def set_params(self, **params):\n",
    "            # dimension retention\n",
    "            return self\n",
    "\n",
    "        def fit(self, X, y):\n",
    "            self.fitted_ = True\n",
    "            # dimension retention\n",
    "            return self\n",
    "\n",
    "        def predict_proba(self, X):\n",
    "            # X is used inly for signature compatibility;\n",
    "            # return precomputed probabilities\n",
    "            p = self.scores\n",
    "            return np.column_stack([1.0 - p, p])\n",
    "\n",
    "    # create surrogate model and 'features' as indeces #\n",
    "    X_idx = np.arange(len(y_true_test)).reshape(-1, 1)\n",
    "    base_est = _ScoresEstimator(y_proba_best)\n",
    "\n",
    "    # ThresholdOptimizer for demographic parity #\n",
    "    postproc = ThresholdOptimizer(\n",
    "        estimator=base_est,\n",
    "        constraints=\"demographic_parity\",\n",
    "        predict_method=\"predict_proba\",\n",
    "        prefit=True,\n",
    "    )\n",
    "    # fit/predict on the same X_idx #\n",
    "    postproc.fit(X=X_idx, y=y_true_test, sensitive_features=sf)\n",
    "    y_pred_dp = postproc.predict(X=X_idx, sensitive_features=sf).astype(\"int8\")\n",
    "\n",
    "    # metrics after post-processing #\n",
    "    def _prec(y_true, y_pred):\n",
    "        return precision_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    def _rec(y_true, y_pred):\n",
    "        return recall_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    def _f1(y_true, y_pred):\n",
    "        return f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    dp = demographic_parity_difference(y_true=y_true_test, y_pred=y_pred_dp, sensitive_features=sf)\n",
    "    eq = equalized_odds_difference(y_true=y_true_test, y_pred=y_pred_dp, sensitive_features=sf)\n",
    "\n",
    "    print(\n",
    "        f\"[ThresholdOptimizer @ {gcol}] \"\n",
    "        f\"acc={accuracy_score(y_true_test, y_pred_dp):.3f} \"\n",
    "        f\"f1={_f1(y_true_test, y_pred_dp):.3f} \"\n",
    "        f\"prec={_prec(y_true_test, y_pred_dp):.3f} \"\n",
    "        f\"rec={_rec(y_true_test, y_pred_dp):.3f} \"\n",
    "        f\"| DP diff={dp:+.3f} EqOdds diff={eq:+.3f}\"\n",
    "    )\n",
    "\n",
    "# summary table of metrics: baseline vs t* ThresholdOptimizer #\n",
    "rows = []\n",
    "\n",
    "\n",
    "def _metrics_row(tag, y_pred_local, sf_local):\n",
    "    return {\n",
    "        \"settings\": tag,\n",
    "        \"accuracy\": accuracy_score(y_true_test, y_pred_local),\n",
    "        \"precision\": precision_score(y_true_test, y_pred_local, zero_division=0),\n",
    "        \"recall\": recall_score(y_true_test, y_pred_local, zero_division=0),\n",
    "        \"f1\": f1_score(y_true_test, y_pred_local, zero_division=0),\n",
    "        \"dp_diff\": demographic_parity_difference(\n",
    "            y_true=y_true_test, y_pred=y_pred_local, sensitive_features=sf_local\n",
    "        ),\n",
    "        \"eod_diff\": equalized_odds_difference(\n",
    "            y_true=y_true_test, y_pred=y_pred_local, sensitive_features=sf_local\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "# baseline t=0.5 #\n",
    "y_pred_05 = (y_proba_best >= 0.5).astype(int)\n",
    "rows.append(_metrics_row(\"baseline_t0.5\", y_pred_05, sf))\n",
    "\n",
    "# baseline t* #\n",
    "if \"t_star\" in globals():\n",
    "    y_pred_star = (y_proba_best >= t_star).astype(int)\n",
    "    rows.append(_metrics_row(\"baseline_t*\", y_pred_star, sf))\n",
    "\n",
    "# post-processing DP #\n",
    "rows.append(_metrics_row(\"ThresholdOptimizer_DP\", y_pred_dp, sf))\n",
    "\n",
    "postproc_df = pd.DataFrame(rows)\n",
    "postproc_df.to_csv(FIG_DIR_03 / \"metrics_postprocessing.csv\", index=False)\n",
    "print(\"[postproc] saved:\", FIG_DIR_03 / \"metrics_postprocessing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# --- Probability Calibration by Groups --- #\n",
    "\n",
    "**Goal.** Assess the agreement between probabilistic predictions and empirical frequencies of the positive outcome across sensitive groups.\n",
    "\n",
    "**Methodology:**\n",
    "1. For each available group (`sex`, `race`, `age_group`), plot reliability curves over 10 equal probability bins.\n",
    "2. Compute Expected Calibration Error (ECE) using 10 bins.\n",
    "3. Ignore very small subgroups when plotting by setting `min_count=50`.\n",
    "\n",
    "**Outputs:**\n",
    "1. Plot files: `calibration_{group}.png` in `reports/figures_03/`.\n",
    "2. ECE summary table: `calibration_ece_by_group.png` in `reports/figures_03/`.\n",
    "\n",
    "**Interpretation:**\n",
    "1. The line \\(y=x\\) corresponds to perfect calibration. Systematic deviations indicate overconfidence or underconfidence of the model within a given group.\n",
    "2. ECE aggregates the weighted average deviation and serves as a compact numerical indicator of group-wise calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calibration plots by groups + сохранение --- #\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "\n",
    "def plot_reliability_by_groups(\n",
    "    y_true, y_proba, group_series, n_bins=10, min_count=50, title=\"\", save_name=None\n",
    "):\n",
    "    groups = group_series.value_counts().index.tolist()\n",
    "    kept = []\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    for g in groups:\n",
    "        mask = (group_series == g).to_numpy()\n",
    "        if mask.sum() < min_count:\n",
    "            continue\n",
    "        prob_true, prob_pred = calibration_curve(\n",
    "            y_true[mask], y_proba[mask], n_bins=n_bins, strategy=\"uniform\"\n",
    "        )\n",
    "        ax.plot(prob_pred, prob_true, label=f\"{g} (n={mask.sum()})\")\n",
    "        kept.append(g)\n",
    "    ax.plot([0, 1], [0, 1], \"--\", linewidth=1)\n",
    "    ax.set_xlabel(\"Predicted probability\")\n",
    "    ax.set_ylabel(\"Empirical positive rate\")\n",
    "    ax.set_title(title or f\"Calibration by {group_series.name}\")\n",
    "    if kept:\n",
    "        ax.legend()\n",
    "    if save_name:\n",
    "        save_fig(save_name, fig)\n",
    "\n",
    "\n",
    "def compute_ece(y_true, y_proba, n_bins=10):\n",
    "    # uniform binning by predicted probability #\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    idx = np.digitize(y_proba, bins) - 1\n",
    "    ece = 0.0\n",
    "    N = len(y_true)\n",
    "    for b in range(n_bins):\n",
    "        mask = idx == b\n",
    "        nb = mask.sum()\n",
    "        if nb == 0:\n",
    "            continue\n",
    "        acc_b = y_true[mask].mean()\n",
    "        conf_b = y_proba[mask].mean()\n",
    "        ece += (nb / N) * abs(acc_b - conf_b)\n",
    "    return float(ece)\n",
    "\n",
    "\n",
    "# data availability check #\n",
    "assert all(k in globals() for k in [\"y_true_test\", \"y_proba_best\", \"X_test_sens\"])\n",
    "\n",
    "# run for all available sensitive attributes #\n",
    "for col in [c for c in [\"sex\", \"race\", \"age_group\"] if c in X_test_sens.columns]:\n",
    "    plot_reliability_by_groups(\n",
    "        y_true=y_true_test,\n",
    "        y_proba=y_proba_best,\n",
    "        group_series=X_test_sens[col],\n",
    "        n_bins=10,\n",
    "        min_count=50,\n",
    "        title=f\"Calibration by {col}\",\n",
    "        save_name=f\"calibration_{col}.png\",\n",
    "    )\n",
    "\n",
    "# ECE by groups and save CSV #\n",
    "ece_rows = []\n",
    "for col in [c for c in [\"sex\", \"race\", \"age_group\"] if c in X_test_sens.columns]:\n",
    "    s = X_test_sens[col]\n",
    "    # compute per-category ECE within col\n",
    "    for g in s.dropna().unique():\n",
    "        m = (s == g).to_numpy()\n",
    "        if m.sum() == 0:\n",
    "            continue\n",
    "        ece = compute_ece(y_true=y_true_test[m], y_proba=y_proba_best[m], n_bins=10)\n",
    "        ece_rows.append({\"group_col\": col, \"group\": str(g), \"n\": int(m.sum()), \"ece10\": ece})\n",
    "\n",
    "if ece_rows:\n",
    "    pd.DataFrame(ece_rows).sort_values([\"group_col\", \"group\"]).to_csv(\n",
    "        FIG_DIR_03 / \"calibration_ece_by_group.csv\", index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Group metrics bar charts + saving --- #\n",
    "\n",
    "assert \"t_star\" in globals(), \"Expected a chosen threshold t_star\"\n",
    "assert all(k in globals() for k in [\"y_true_test\", \"y_proba_best\", \"X_test_sens\"])\n",
    "y_pred_star = (y_proba_best >= t_star).astype(int)\n",
    "\n",
    "\n",
    "def _bar_metric_by_group(\n",
    "    y_true, y_pred, group_series, metric_fn, metric_name: str, min_count=50, save_name=\"\"\n",
    "):\n",
    "    recs = []\n",
    "    for g, idx in (\n",
    "        group_series.reset_index(drop=True)\n",
    "        .groupby(group_series.reset_index(drop=True))\n",
    "        .groups.items()\n",
    "    ):\n",
    "        idx = np.array(idx, dtype=int)\n",
    "        if len(idx) < min_count:\n",
    "            continue\n",
    "        val = metric_fn(y_true[idx], y_pred[idx])\n",
    "        recs.append({\"group\": str(g), metric_name: float(val), \"n\": int(len(idx))})\n",
    "    if not recs:\n",
    "        print(f\"[fairness] skip {metric_name}: all groups < min_count\")\n",
    "        return\n",
    "    df = pd.DataFrame(recs).sort_values(metric_name, ascending=False)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    ax.bar(df[\"group\"], df[metric_name])\n",
    "    ax.set_title(f\"{metric_name} by {group_series.name}\")\n",
    "    ax.set_ylabel(metric_name)\n",
    "    ax.set_xlabel(group_series.name)\n",
    "    for i, v in enumerate(df[metric_name].values):\n",
    "        ax.text(i, v, f\"{v:.2f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "    save_fig(save_name, fig)\n",
    "\n",
    "\n",
    "for col in [c for c in [\"sex\", \"race\", \"age_group\"] if c in X_test_sens.columns]:\n",
    "    gs = X_test_sens[col]\n",
    "    _bar_metric_by_group(\n",
    "        y_true_test,\n",
    "        y_pred_star,\n",
    "        gs,\n",
    "        precision_score,\n",
    "        \"precision\",\n",
    "        save_name=f\"bar_precision_by_{col}.png\",\n",
    "    )\n",
    "    _bar_metric_by_group(\n",
    "        y_true_test, y_pred_star, gs, recall_score, \"recall\", save_name=f\"bar_recall_by_{col}.png\"\n",
    "    )\n",
    "    _bar_metric_by_group(\n",
    "        y_true_test, y_pred_star, gs, f1_score, \"f1\", save_name=f\"bar_f1_by_{col}.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Confusion matrices by group + saving --- #\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "assert \"t_star\" in globals()\n",
    "assert all(k in globals() for k in [\"y_true_test\", \"y_proba_best\", \"X_test_sens\"])\n",
    "y_pred_star = (y_proba_best >= t_star).astype(int)\n",
    "\n",
    "\n",
    "def _plot_cm(y_true, y_pred, title=\"\"):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cbar=False, ax=ax)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticklabels([\"0\", \"1\"])\n",
    "    ax.set_yticklabels([\"0\", \"1\"])\n",
    "    return fig\n",
    "\n",
    "\n",
    "for col in [c for c in [\"sex\", \"race\", \"age_group\"] if c in X_test_sens.columns]:\n",
    "    s = X_test_sens[col]\n",
    "    for g in s.dropna().unique():\n",
    "        mask = (s == g).to_numpy()\n",
    "        if mask.sum() < 50:\n",
    "            continue\n",
    "        fig = _plot_cm(\n",
    "            y_true_test[mask], y_pred_star[mask], title=f\"CM: {col}={g} (n={mask.sum()})\"\n",
    "        )\n",
    "        safe_g = str(g).replace(\"/\", \"-\").replace(\"\\\\\", \"-\").replace(\" \", \"_\")\n",
    "        save_fig(f\"cm_{col}_{safe_g}.png\", fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Baselines visuals from figures_02 --- #\n",
    "\n",
    "figs02, dir02 = A[\"figures02\"]\n",
    "if figs02:\n",
    "    print(f\"[info] figures_02: {dir02} | count={len(figs02)}\")\n",
    "    for p in figs02[:6]:\n",
    "        print(\" -\", p.name)\n",
    "else:\n",
    "    print(\"[skip] figures_02 not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "# --- Explainability: global (SHAP / alternatives) --- #\n",
    "\n",
    "**Goal.** Identify which features contribute most to the model’s predictions and check for indirect dependencies on sensitive attributes.\n",
    "\n",
    "**Methodology.** For the final model (LightGBM/XGBoost), we use `shap.TreeExplainer` to estimate each feature’s contribution to the predicted probability of the `>50k` class. SHAP provides consistent, additive attributions, which makes it the preferred method for explaining boosted trees.\n",
    "\n",
    "**Global analysis results:**\n",
    "1. Key socio-economic features (e.g., `marital-status_Married-civ-spouse`, `education-num`, `capital-gain`, `hours-per-week`, `age`) dominate the importance rankings.\n",
    "2. Sensitive variables (`sex`, `race`, `age_group`) show low average impact, indicating no direct reliance on these attributes by the model.\n",
    "3. Partial dependencies via proxy features (e.g., `occupation`, `marital-status`) may indirectly reflect group differences.\n",
    "\n",
    "**Conclusion.** SHAP indicates the model primarily focuses on economic and demographic factors relevant to income rather than directly on sensitive attributes. This aligns with the fairness evaluation, which showed moderate but not critical group disparities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Explainability input selection via unified loader --- #\n",
    "\n",
    "assert \"A\" in globals(), \"Run the unified loader\"\n",
    "model, _ = A[\"model\"]\n",
    "XS, _ = A[\"X_test_enc\"]\n",
    "feature_names, _ = A[\"feature_names\"]\n",
    "test_groups, _ = A[\"test_groups\"]\n",
    "\n",
    "# direct use of loaded artifacts #\n",
    "if XS is not None and feature_names is not None:\n",
    "    clf_for_shap = getattr(model, \"best_estimator_\", model)\n",
    "    print(\n",
    "        \"[ok] \"\n",
    "        f\"XS: {getattr(XS, 'shape', None)}, \"\n",
    "        f\"features: {len(feature_names)}, \"\n",
    "        f\"model: {type(clf_for_shap).__name__}\"\n",
    "    )\n",
    "else:\n",
    "    # fallback inly if something is missing\n",
    "    print(\n",
    "        \"[warn] No XS or feature_names from artifacts. \"\n",
    "        \"Fallback recovery enabled (disabled per unified loader policy).\"\n",
    "    )\n",
    "    raise RuntimeError(\"Required artifacts for explainability: X_test_enc and/or feature_names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dependence-plots for top features --- #\n",
    "\n",
    "import re\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# search for artifacts in common project folders #\n",
    "ART_DIRS = [\n",
    "    Path(\"data\") / \"artifacts\",\n",
    "    Path(\"data\") / \"models\",\n",
    "    Path(\"notebooks\") / \"artifacts\",\n",
    "    Path(\"notebooks\") / \"models\",\n",
    "]\n",
    "\n",
    "\n",
    "def _first_exists_any(names):\n",
    "    names_lower = [n.lower() for n in names]\n",
    "    for d in ART_DIRS:\n",
    "        if not d.exists():\n",
    "            continue\n",
    "        for p in d.iterdir():\n",
    "            if p.is_file() and p.name.lower() in names_lower:\n",
    "                return p\n",
    "    # recursive fallback\n",
    "    roots = [Path(\".\").resolve()]\n",
    "    roots += list(roots[0].parents)[:3]\n",
    "    for root in roots:\n",
    "        for p in root.rglob(\"*\"):\n",
    "            try:\n",
    "                if p.is_file() and p.name.lower() in names_lower:\n",
    "                    return p\n",
    "            except PermissionError:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "\n",
    "# model #\n",
    "clf = None\n",
    "p_model = _first_exists_any([\"lgb_best.joblib\", \"LGBM_best.joblib\", \"model_best.joblib\"])\n",
    "if p_model is not None:\n",
    "    model, model_path = A[\"model\"]\n",
    "    obj = model\n",
    "    # if this is pipeline — pull out clf\n",
    "    if hasattr(obj, \"named_steps\"):\n",
    "        clf = obj.named_steps.get(\"clf\", None) or obj\n",
    "    else:\n",
    "        clf = obj\n",
    "\n",
    "# data: X_test_enc and feature_names #\n",
    "X_test_enc, X_test_enc_path = A[\"X_test_enc\"]\n",
    "XS = X_test_enc\n",
    "\n",
    "# load feature_names from artifacts #\n",
    "feature_names, feature_names_path = A[\"feature_names\"]\n",
    "\n",
    "# safeguard when data are missing #\n",
    "if XS is None:\n",
    "    print(\"No XS/X_test_enc data: skipping dependence plots\")\n",
    "    raise SystemExit\n",
    "\n",
    "# align feature_names to current XS matrix #\n",
    "ncols = XS.shape[1]\n",
    "\n",
    "# try to get names from the model (if not default f0..fN) #\n",
    "try:\n",
    "    if (\n",
    "        \"clf\" in globals()\n",
    "        and clf is not None\n",
    "        and hasattr(clf, \"booster_\")\n",
    "        and clf.booster_ is not None\n",
    "    ):\n",
    "        fnm = list(clf.booster_.feature_name())\n",
    "        all_fnums = all(isinstance(x, str) and re.fullmatch(r\"f\\d+\", x) for x in fnm)\n",
    "        if isinstance(fnm, list) and len(fnm) == ncols and not all_fnums:\n",
    "            feature_names = [str(x) for x in fnm]\n",
    "            print(\"[info] feature_names taken from model booster\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "# ensure matching length: trim/pad if needed #\n",
    "def _ensure_len(cols, n):\n",
    "    lst = list(cols) if isinstance(cols, list | np.ndarray | pd.Index) else []\n",
    "    if len(lst) == n:\n",
    "        return [str(c) for c in lst]\n",
    "    if len(lst) > n:\n",
    "        print(f\"[warn] feature_names longer ({len(lst)}) than ncols={n}. Trimming.\")\n",
    "        return [str(c) for c in lst[:n]]\n",
    "    print(\n",
    "        f\"[warn] feature_names shorter ({len(lst)}) than ncols={n}. Padding f{len(lst)}..f{n - 1}.\"\n",
    "    )\n",
    "    return [str(c) for c in (lst + [f\"f{i}\" for i in range(len(lst), n)])]\n",
    "\n",
    "\n",
    "if (feature_names is None) or (\n",
    "    isinstance(feature_names, (list | np.ndarray | pd.Index)) and len(feature_names) != ncols\n",
    "):\n",
    "    feature_names = _ensure_len(feature_names, ncols)\n",
    "\n",
    "if XS is not None and not isinstance(XS, pd.DataFrame):\n",
    "    if sp.issparse(XS):\n",
    "        XS = pd.DataFrame.sparse.from_spmatrix(\n",
    "            XS, columns=(feature_names if feature_names is not None else None)\n",
    "        )\n",
    "    else:\n",
    "        XS = pd.DataFrame(XS, columns=(feature_names if feature_names is not None else None))\n",
    "\n",
    "# extract classifier from pipeline if needed #\n",
    "clf_for_shap = None\n",
    "if \"clf\" in globals() and clf is not None and hasattr(clf, \"predict_proba\"):\n",
    "    clf_for_shap = clf\n",
    "else:\n",
    "    g = globals()\n",
    "    _pipe = g.get(\"pipe\")\n",
    "    if _pipe is not None and hasattr(_pipe, \"named_steps\"):\n",
    "        clf_for_shap = _pipe.named_steps.get(\"clf\", None)\n",
    "        if clf_for_shap is None:\n",
    "            for _, step in _pipe.named_steps.items():\n",
    "                if hasattr(step, \"predict_proba\") or hasattr(step, \"predict\"):\n",
    "                    clf_for_shap = step\n",
    "                    break\n",
    "\n",
    "# unified check and subsampling #\n",
    "if clf_for_shap is None or XS is None:\n",
    "    print(\"No suitable model/data for SHAP: skipping dependence plots\")\n",
    "else:\n",
    "    # subsample\n",
    "    idx = np.arange(len(XS))\n",
    "    if len(idx) > 5000:\n",
    "        idx = rng.choice(idx, size=5000, replace=False)\n",
    "    XS_sub = XS.iloc[idx] if hasattr(XS, \"iloc\") else XS[idx, :]\n",
    "\n",
    "    expl = shap.TreeExplainer(clf_for_shap)\n",
    "    shap_vals = expl.shap_values(XS_sub)\n",
    "\n",
    "    # binary class -> take contributions for the positive class\n",
    "    if isinstance(shap_vals, list):\n",
    "        try:\n",
    "            classes_ = getattr(clf_for_shap, \"classes_\", [0, 1])\n",
    "            pos_idx = list(classes_).index(1)\n",
    "        except Exception:\n",
    "            pos_idx = 1 if len(shap_vals) > 1 else 0\n",
    "        shap_vals = shap_vals[pos_idx]\n",
    "\n",
    "    # top-k\n",
    "    topk = 10\n",
    "\n",
    "    # beeswarm -> file\n",
    "    shap.summary_plot(\n",
    "        shap_vals,\n",
    "        XS_sub,\n",
    "        feature_names=(feature_names if feature_names is not None else None),\n",
    "        show=False,\n",
    "        max_display=topk,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    save_fig(\"shap_summary_top10.png\", None)\n",
    "    plt.close()\n",
    "\n",
    "    # bar -> file\n",
    "    shap.summary_plot(\n",
    "        shap_vals,\n",
    "        XS_sub,\n",
    "        feature_names=(feature_names if feature_names is not None else None),\n",
    "        plot_type=\"bar\",\n",
    "        show=False,\n",
    "        max_display=topk,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    save_fig(\"shap_bar_top10.png\", None)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Explainability: local (TP/FP/FN cases) --- #\n",
    "\n",
    "assert all(k in globals() for k in [\"y_true_test\", \"y_proba_best\", \"X_test_sens\"])\n",
    "thr = globals().get(\"t_star\", 0.5)\n",
    "y_pred_thr = (y_proba_best >= thr).astype(\"int8\")\n",
    "\n",
    "TP_idx = np.where((y_true_test == 1) & (y_pred_thr == 1))[0]\n",
    "FP_idx = np.where((y_true_test == 0) & (y_pred_thr == 1))[0]\n",
    "FN_idx = np.where((y_true_test == 1) & (y_pred_thr == 0))[0]\n",
    "\n",
    "# use only artifacts from the unified loader #\n",
    "XS, _ = A[\"X_test_enc\"]\n",
    "feature_names, _ = A[\"feature_names\"]\n",
    "model, _ = A[\"model\"]\n",
    "\n",
    "# extract classifier from best_estimator_/Pipeline #\n",
    "obj = getattr(model, \"best_estimator_\", model)\n",
    "if hasattr(obj, \"named_steps\"):\n",
    "    clf_for_shap = obj.named_steps.get(\"clf\", None)\n",
    "    if clf_for_shap is None:\n",
    "        for step in obj.named_steps.values():\n",
    "            if hasattr(step, \"predict_proba\") or hasattr(step, \"predict\"):\n",
    "                clf_for_shap = step\n",
    "                break\n",
    "else:\n",
    "    clf_for_shap = obj\n",
    "\n",
    "if clf_for_shap is None:\n",
    "    raise RuntimeError(\"Failed to extract a slassifier from the Pipeline for SHAP\")\n",
    "\n",
    "if XS is None and feature_names is None:\n",
    "    raise RuntimeError(\"No X_test_enc or feature_names from artifacts; skipping explainability\")\n",
    "\n",
    "# build SHAP plots #\n",
    "expl = shap.TreeExplainer(clf_for_shap)\n",
    "\n",
    "\n",
    "def _pick_some(arr, k=3):\n",
    "    if len(arr) == 0:\n",
    "        return []\n",
    "    rng = np.random.default_rng(42)\n",
    "    return arr if len(arr) <= k else rng.choice(arr, size=k, replace=False).tolist()\n",
    "\n",
    "\n",
    "cases = [\n",
    "    (\"TP\", _pick_some(TP_idx, 3)),\n",
    "    (\"FP\", _pick_some(FP_idx, 3)),\n",
    "    (\"FN\", _pick_some(FN_idx, 3)),\n",
    "]\n",
    "\n",
    "for tag, idxs in cases:\n",
    "    if not idxs:\n",
    "        print(f\"{tag}: no cases - skipping\")\n",
    "        continue\n",
    "    for i in idxs:\n",
    "        if hasattr(XS, \"iloc\"):\n",
    "            row = XS.iloc[[i]]\n",
    "        elif sp.issparse(XS):\n",
    "            row = XS[i]\n",
    "        else:\n",
    "            row = XS[i : i + 1]\n",
    "\n",
    "        sv = expl.shap_values(row)\n",
    "\n",
    "        # choose class and reshape to 2D\n",
    "        sv_arr = sv[1] if isinstance(sv, list) else sv\n",
    "        if getattr(sv_arr, \"ndim\", 1) == 1:\n",
    "            sv_arr = sv_arr.reshape(1, -1)\n",
    "\n",
    "        # LightGBM: last column is base value -> extract and trim\n",
    "        if sv_arr.shape[1] == row.shape[1] + 1:\n",
    "            base_val = sv_arr[0, -1]\n",
    "            sv_arr = sv_arr[:, :-1]\n",
    "        else:\n",
    "            ev = expl.expected_value\n",
    "            base_val = ev[1] if isinstance(ev, list | np.ndarray) else ev\n",
    "\n",
    "        # features as 1D\n",
    "        if hasattr(row, \"values\"):  # DataFrame\n",
    "            feats_1d = row.values.reshape(-1)\n",
    "        else:  # np/sparse\n",
    "            feats_1d = np.asarray(row).reshape(-1)\n",
    "\n",
    "        fig = shap.force_plot(\n",
    "            base_value=base_val,\n",
    "            shap_values=sv_arr.reshape(-1),\n",
    "            features=feats_1d,\n",
    "            feature_names=feature_names,\n",
    "            matplotlib=True,\n",
    "            show=False,\n",
    "        )\n",
    "        plt.suptitle(f\"{tag} case idx={i} @ t={thr:.2f}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "# --- Risks, Limitations, and Recommendations --- #\n",
    "\n",
    "**Quality and Fairness.** The final model shows strong predictive performance (ROC-AUC around 0.9) with a moderate trade-off between accuracy and fairness. Without fairness correction, a gap in the share of positive predictions between genders is observed. Applying post-processing (ThresholdOptimizer with demographic parity or equalized odds) notably reduces this gap, at the cost of a slight F1 decrease - a typical outcome of balancing fairness and accuracy.\n",
    "\n",
    "**Calibration and Interpretability.** For large groups, the model is well-calibrated: predicted probabilities align with observed positive rates. For smaller groups (by age or rare racial categories), calibration error increases due to limited data. Global SHAP analysis confirms that the model relies primarily on socio-economic features (`marital-status`, `education-num`, `capital-gain`, `hours-per-week`, `age`), while sensitive variables have minimal direct influence.\n",
    "\n",
    "**Risks and Areas for Improvement:**\n",
    "1. Small groups lead to unstable fairness metrics and weaker calibration.\n",
    "2. Possible proxy factors (e.g., marital status or occupation) may partially encode sensitive attributes.\n",
    "3. Stronger fairness regularization can noticeably reduce accuracy.\n",
    "\n",
    "**Recommendations:**\n",
    "1. For practical use, apply the model with a baseline threshold `t=0.5` or the optimized `t*`.\n",
    "2. In high-stakes fairness scenarios, use ThresholdOptimizer with demographic parity control.\n",
    "3. Examine the effect or aggregation of proxy features on fairness stability.\n",
    "4. If needed, recalibrate predicted probabilities (Platt or Isotonic) and extend data coverage for small subgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Export artifacts from 03_fairness_and_explainability.ipynb --- #\n",
    "\n",
    "ART_DIR = globals().get(\"ART_DIR\", Path(\"data\") / \"artifacts\")\n",
    "REPORTS_DIR = globals().get(\"REPORTS_DIR\", Path(\"data\") / \"reports\")\n",
    "\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(REPORTS_DIR / \"figures_03\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# export threshold scan #\n",
    "scan_obj = globals().get(\"scan_df\", None)\n",
    "try:\n",
    "    if isinstance(scan_obj, pd.DataFrame) and len(scan_obj) > 0:\n",
    "        out_csv = ART_DIR / \"fairness_threshold_scan.csv\"\n",
    "        scan_obj.to_csv(out_csv, index=False)\n",
    "        print(f\"Saved fairness_threshold_scan.csv -> {out_csv}\")\n",
    "\n",
    "        # copy to reports for convenience\n",
    "        out_csv_rep = REPORTS_DIR / \"figures_03\" / \"fairness_threshold_scan.csv\"\n",
    "        try:\n",
    "            scan_obj.to_csv(out_csv_rep, index=False)\n",
    "            print(f\"Copied to reports -> {out_csv_rep}\")\n",
    "        except Exception as e:\n",
    "            print(\"[warn] copy to reports failed:\", e)\n",
    "    else:\n",
    "        print(\"[info] scan_df missing or empty - export skipped\")\n",
    "except Exception as e:\n",
    "    print(\"[warn] scan_df export:\", e)\n",
    "\n",
    "print(\"Export artifacts from 03_fairness_and_explainability.ipynb completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Rediness check --- #\n",
    "\n",
    "# base directories #\n",
    "ROOT = globals().get(\"ROOT\", Path(\".\"))\n",
    "ART_DIR = globals().get(\"ART_DIR\", ROOT / \"data\" / \"artifacts\")\n",
    "REPORTS_DIR = globals().get(\"REPORTS_DIR\", ROOT / \"data\" / \"reports\")\n",
    "FIG_DIR_03 = REPORTS_DIR / \"figures_03\"\n",
    "\n",
    "# verify presence of key directories #\n",
    "assert ART_DIR.exists(), f\"Нет папки артефактов: {ART_DIR}\"\n",
    "FIG_DIR_03.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# required in-memory objects #\n",
    "need = [\"feature_names\", \"X_test_enc\", \"sensitive\", \"y_true\", \"y_proba\", \"y_pred\"]\n",
    "miss = [k for k in need if (k not in A) or (A.get(k, (None, None))[0] is None)]\n",
    "if miss:\n",
    "    raise RuntimeError(f\"Missing required artifacts {miss}\")\n",
    "\n",
    "# expected files and patterns #\n",
    "critical = [\n",
    "    FIG_DIR_03 / \"fairness_threshold_scan.csv\",\n",
    "    ART_DIR / \"fairness_threshold_scan.csv\",\n",
    "    FIG_DIR_03 / \"shap_summary_top10.png\",\n",
    "    FIG_DIR_03 / \"shap_bar_top10.png\",\n",
    "    FIG_DIR_03 / \"group_metrics_t_star.csv\",\n",
    "    FIG_DIR_03 / \"selection_rates_t_star.csv\",\n",
    "]\n",
    "\n",
    "patterns = [\n",
    "    \"Pareto_f1_vs_dp.png\",\n",
    "    \"accuracy_f1_vs_threshold.png\",\n",
    "    \"dp_vs_threshold.png\",\n",
    "    \"eod_vs_threshold.png\",\n",
    "    \"calibration_*.png\",\n",
    "    \"bar_precision_by_*.png\",\n",
    "    \"bar_recall_by_*.png\",\n",
    "    \"bar_f1_by_*.png\",\n",
    "    \"cm_*_*.png\",\n",
    "]\n",
    "\n",
    "# check critical #\n",
    "missing_critical = [str(p) for p in critical if not Path(p).exists()]\n",
    "if missing_critical:\n",
    "    raise AssertionError(f\"Missing critical files: {missing_critical}\")\n",
    "\n",
    "# summary by patterns #\n",
    "summary = {}\n",
    "for pat in patterns:\n",
    "    files = list(FIG_DIR_03.glob(pat))\n",
    "    summary[pat] = len(files)\n",
    "\n",
    "# print summary #\n",
    "print(\"[fairness] report @\", FIG_DIR_03)\n",
    "for pat, cnt in summary.items():\n",
    "    print(f\"    {pat:28s} -> {cnt:3d}\")\n",
    "\n",
    "# extra logic: warnings #\n",
    "warn = []\n",
    "\n",
    "# if sensitive featrues exist, expect at least one var_* plot #\n",
    "_sens_df = A.get(\"sensitive\", (globals().get(\"X_test_sensitive\", None), None))[0]\n",
    "present_cols = [\n",
    "    c for c in [\"sex\", \"race\", \"age_group\"] if (_sens_df is not None and c in _sens_df.columns)\n",
    "]\n",
    "for col in present_cols:\n",
    "    for base in [\"bar_precision_by_\", \"bar_recall_by_\", \"bar_f1_by_\"]:\n",
    "        if not list(FIG_DIR_03.glob(f\"{base}{col}.png\")):\n",
    "            warn.append(f\"Missing {base}{col}.png\")\n",
    "\n",
    "# at least one confusion matrix #\n",
    "if summary.get(\"cm_*_*.png\", 0) == 0:\n",
    "    warn.append(\"No confusion matrices (cm_*_*.png)\")\n",
    "\n",
    "# at least one calibration_*.png #\n",
    "if summary.get(\"calibration_*.png\", 0) == 0:\n",
    "    warn.append(\"No calibartion_*.png\")\n",
    "\n",
    "if warn:\n",
    "    print(\"[fairness][warn]\", \"; \".join(warn))\n",
    "else:\n",
    "    print(\"[fairness] OK: full set of files generated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "census_ds2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
