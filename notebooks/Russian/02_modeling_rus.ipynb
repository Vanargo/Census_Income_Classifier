{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "© 2025 Vanargo · Лицензия: MIT. См. файл `LICENSE` в корне репозитория."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# --- 02. Modeling: Baselines, Tuning, Evaluation --- #\n",
    "\n",
    "**Цель.** Построить и сравнить несколько алгоритмов классификации на едином препроцессоре, выбрать лучшую модель по целевому набору метрик и сохранить артефакты для дальнейшего аудита справедливости и интерпретации (см. `03_fairness_and_explainability.ipynb`).\n",
    "\n",
    "**Входы:**\n",
    "1. Финальный датасет из `01_data_loading_and_eda.ipynb`.\n",
    "2. Явные списки признаков: `num_features`, `cat_features`.\n",
    "\n",
    "**Подход:**\n",
    "1. Единый `ColumnTransformer`: числовые -> `SimpleImputer(median)` -> `StandardScaler`; категориальные -> `SimpleImputer(mostfrequent)` -> `OneHotEncoder(handle_unknown='ignore', sparse_output=True)`.\n",
    "2. Модели: Logistic Regression, Decision Tree, Random Forest, XGBoost (early stopping), LightGBM (RandomizedSearch).\n",
    "3. Валидация: стратифицированные разбиения, фиксированный `random_state`.\n",
    "4. Основные метрики сравнения: ROC-AUC, F1, Accuracy; дополнительные - Precision, Recall, PR-AUC (где применимо).\n",
    "\n",
    "**Выходы:**\n",
    "1. Сводная таблица метрик по моделям и график сравнения.\n",
    "2. Лучшая модель с полностью собранным pipeline препроцессинга.\n",
    "3. Артефакты для этапа 03: `y_true_test`, `y_proba_best`, `y_pred_best`, сырьевые копии чувствительных признаков, список OHE-фич, сериализованные объекты модели/препроцессора, версии библиотек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Импорты и глобальный конфиг --- #\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# stdlib #\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# third-party #\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn import set_config\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# детализация вывода пайплайнов и датафреймов #\n",
    "set_config(transform_output=\"pandas\", display=\"diagram\")\n",
    "\n",
    "# конфигурация выводов, сид, стиль графиков #\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "sns.set(context=\"notebook\")\n",
    "\n",
    "# подавление предупреждений #\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"lightgbm\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")\n",
    "\n",
    "# краткий отчёт о версиях #\n",
    "print(\n",
    "    \"[versions]\",\n",
    "    f\"numpy={np.__version__}; pandas={pd.__version__}; \"\n",
    "    f\"sklearn={(__import__('sklearn').__version__)}; \"\n",
    "    f\"xgboost={xgb.__version__}; lightgbm={lgb.__version__}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Notebook preamble: silence & style --- #\n",
    "\n",
    "import warnings\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# визуальный стиль #\n",
    "sns.set(context=\"notebook\", style=\"whitegrid\")\n",
    "plt.rcParams[\"axes.spines.top\"] = False\n",
    "plt.rcParams[\"axes.spines.right\"] = False\n",
    "\n",
    "# общие фильтры предупреждений #\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "print(\"[init] visual style and warning filters applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Project paths bootstrap (detect & sys.path) --- #\n",
    "\n",
    "# найти корень проекта по маркерам #\n",
    "DETECTED_ROOT = Path.cwd()\n",
    "_MARKERS = {\".git\", \"pyproject.toml\", \"README.md\"}\n",
    "while (\n",
    "    not any((DETECTED_ROOT / m).exists() for m in _MARKERS)\n",
    "    and DETECTED_ROOT.parent != DETECTED_ROOT\n",
    "):\n",
    "    DETECTED_ROOT = DETECTED_ROOT.parent\n",
    "\n",
    "# добавить корень в sys.path один раз #\n",
    "root_str = str(DETECTED_ROOT.resolve())\n",
    "if root_str not in sys.path:\n",
    "    sys.path.append(root_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Project paths imports --- #\n",
    "\n",
    "# импорт централизованных путей проекта #\n",
    "from paths import (\n",
    "    ART_DIR,\n",
    "    DATA_DIR,\n",
    "    MODELS_DIR,\n",
    "    PROC_DIR,\n",
    "    REPORTS_DIR,\n",
    ")\n",
    "from paths import (\n",
    "    ROOT as PATHS_ROOT,\n",
    ")\n",
    "\n",
    "# верификация согласованности найденного ROOT и paths.ROOT #\n",
    "assert PATHS_ROOT.resolve() == DETECTED_ROOT.resolve(), (\n",
    "    f\"paths.ROOT={PATHS_ROOT} != detected ROOT={DETECTED_ROOT}\"\n",
    ")\n",
    "\n",
    "# краткий отчет #\n",
    "print(f\"[paths] ROOT={DETECTED_ROOT}\")\n",
    "print(f\"[paths] DATA_DIR={DATA_DIR}\")\n",
    "print(f\"[paths] MODELS_DIR={MODELS_DIR}\")\n",
    "print(f\"[paths] ART_DIR={ART_DIR}\")\n",
    "print(f\"[paths] REPORTS_DIR={REPORTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# --- Data Split --- #\n",
    "\n",
    "**Цель.** Получить воспроизводимое разбиение данных на обучающую и тестовую части, сохранив стратификацию по целевой переменной `income`.\n",
    "\n",
    "**Контекст.** Датасет `df_ready` сформирован в ноутбуке `01_data_loading_and_eda.ipynb` после  очистки и кодирования признаков. Он содержит 14 признаков (смесь числовых и категориальных) и бинарную целевую переменную `income`.\n",
    "\n",
    "**Подход:**\n",
    "1. Используется `train_test_split` из `sklearn.model_selection`.\n",
    "2. Пропорция: 80% - обучение, 20% - тест.\n",
    "3. Стратификация: `stratify=y`, чтобы сохранить исходное распределение классов.\n",
    "4. Фиксируется `random_state=42` для полной воспроизводимости.\n",
    "5. Отдельно сохраняются:\n",
    "    1) `X_train`, `X_test` - признаки без целевой переменной;\n",
    "    2) `y_train`, `y_test` - целевая метка.\n",
    "6. В дальнейшем `X_test` используется для формирования контрольных выборок в аудитах fairness (см. `03_fairness_and_explainability.ipynb`).\n",
    "\n",
    "**Вывод.** Разделение обеспечивает корректное соотношение классов и исключает утечку информации из теста в обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Split --- #\n",
    "\n",
    "\n",
    "# robust load: parquet → csv fallback #\n",
    "p_parq = PROC_DIR / \"adult_eda.parquet\"\n",
    "p_csv = PROC_DIR / \"adult_eda.csv\"\n",
    "if p_parq.exists():\n",
    "    df = pd.read_parquet(p_parq)\n",
    "elif p_csv.exists():\n",
    "    df = pd.read_csv(p_csv)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Не найдено {p_parq} или {p_csv}\")\n",
    "\n",
    "# целевая переменная #\n",
    "assert \"income\" in df.columns, 'Ожидается столбец \"income\" в processed-датасете'\n",
    "y = (df[\"income\"].astype(str).str.strip() == \">50K\").astype(int)\n",
    "\n",
    "# признаки: всё, кроме income и служебного split-маркера 'source' #\n",
    "drop_cols = [c for c in [\"income\", \"source\"] if c in df.columns]\n",
    "X = df.drop(columns=drop_cols)\n",
    "\n",
    "# контроль согласованности #\n",
    "assert len(X) == len(y), f\"len(X)={len(X)} != len(y)={len(y)}\"\n",
    "\n",
    "# train/Test 80/20 со стратификацией #\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"[split] \"\n",
    "    f\"X_train={X_train.shape} | X_test={X_test.shape} | \"\n",
    "    f\"y_train={y_train.shape} | y_test={y_test.shape}\"\n",
    ")\n",
    "print(f\"[target] positive_rate={y.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# --- Unified Preprocessing --- #\n",
    "\n",
    "**Цель.** Сформировать единый препроцессор, применимый как при обучении, так и при инференсе, обеспечивающий одинаковую обработку числовых и категориальных признаков.\n",
    "\n",
    "**Контекст.** В `01_data_loading_and_eda.ipynb` были определены два списка признаков:\n",
    "1. `num_features` - количественные переменные (например, `age`, `hours-per-week`, `capital_gain`, `capital_loss`);\n",
    "2. `cat_features` - категориальные переменные (например, `education`, `occupation`, `marital_status`, `sex`, `race`).\n",
    "\n",
    "**Подход:**\n",
    "1. Используется `ColumnTransformer` для объединения ветвей препроцессинга.\n",
    "2. Числовые признаки:\n",
    "    1) `SimpleImputer(strategy='median')`;\n",
    "    2) `StandardScaler()`.\n",
    "3. Категориальные признаки:\n",
    "    1) `SimpleImputer(strategy='most_frequent')`;\n",
    "    2) `OneHotEncoder(handle_unknown='ignore', sparse_output=True)`.\n",
    "4. Порядок ветвей фиксируется, чтобы обеспечить совпадение порядка столбцов при последующих сохранениях артефактов.\n",
    "5. Препроцессор хранится в переменной `preproc` и позже включается в `Pipeline` каждой модели.\n",
    "6. Все операции детерминированы; при сохранении моделей и артефактов структура препроцессора сериализуется (`joblib.dump`).\n",
    "\n",
    "**Вывод.** Единый `ColumnTransformer` гарантирует согласованность обработки данных на этапах обучения, кросс-валидации, инференса и fairness-аудита."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Unified Preprocessing --- #\n",
    "\n",
    "\n",
    "# деление признаков по типам #\n",
    "num_cols = X_train.select_dtypes(include=[\"int\", \"float\"]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "\n",
    "print(f\"[preproc] numeric={len(num_cols)}, categorical={len(cat_cols)}\")\n",
    "\n",
    "# подготовка пайплайнов #\n",
    "num_preproc = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cat_preproc = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# единый ColumnTransformer #\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_preproc, num_cols),\n",
    "        (\"cat\", cat_preproc, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "# пример вызова fit_transform для проверки формы #\n",
    "Xt_train = preprocessor.fit_transform(X_train)\n",
    "Xt_test = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"[preproc] X_train -> {Xt_train.shape}, X_test -> {Xt_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Вспомогательные функции: метрики и логирование --- #\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_tr, y_tr, X_te, y_te, name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Вычисляет единый набор метрик для модели.\n",
    "    Работает как для пайплайнов, так и для отдельных эстиматоров.\n",
    "    \"\"\"\n",
    "    # предсказания #\n",
    "    y_pred_te = model.predict(X_te)\n",
    "\n",
    "    # вероятности (если доступны) #\n",
    "    y_proba_te = None\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        try:\n",
    "            y_proba_te = model.predict_proba(X_te)[:, 1]\n",
    "        except Exception:\n",
    "            pass\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        try:\n",
    "            y_proba_te = model.decision_function(X_te)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # метрики #\n",
    "    metrics = {\n",
    "        \"model\": name,\n",
    "        \"accuracy\": accuracy_score(y_te, y_pred_te),\n",
    "        \"precision\": precision_score(y_te, y_pred_te, zero_division=0),\n",
    "        \"recall\": recall_score(y_te, y_pred_te, zero_division=0),\n",
    "        \"f1\": f1_score(y_te, y_pred_te, zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(y_te, y_proba_te) if y_proba_te is not None else np.nan,\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Инициализация хранилищ результатов --- #\n",
    "\n",
    "# все метрики моделей #\n",
    "results: list[dict] = []\n",
    "\n",
    "# регистр моделей для последующего сохранения и анализа #\n",
    "model_registry: dict[str, dict] = {}\n",
    "\n",
    "print(\"[init] containers: results[], model_registry{}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# --- Raw Copies for Analysis --- #\n",
    "\n",
    "**Цель.** Зафиксировать \"сырой\" вид тестовых данных и чувствительных признаков до применения моделей. Эти копии будут использоваться на этапе 03 \"Fairness & explainability\" для оценки справедливости и интерпретации предсказаний.\n",
    "\n",
    "**Контекст.** В `01_data_loading_and_eda.ipynb` сохранена структура исходного набора: числовые, категориальные и чувствительные признаки (`sex`. `race`, `age`).\n",
    "\n",
    "**Подход:**\n",
    "1. Из `X_test` извлекаются исходные столбцы чувствительных признаков.\n",
    "2. Они сохраняются в отдельный объект `X_test_sens`.\n",
    "3. Параллельно сохраняются:\n",
    "    - `y_true_test` - целевые метки теста;\n",
    "    - `X_test_raw` - копия признаков до препроцессинга.\n",
    "4. Эти объекты сериализуются в `data/artifacts/` для дальнейшего использования в `03_fairness_and_explainability.ipynb`.\n",
    "\n",
    "**Вывод.** Фиксация исходных данных обеспечивает сравнение метрик по группам и воспроизводимость аудита fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Raw Copies for Analysis --- #\n",
    "\n",
    "# сохранение копий исходных данных до препроцессинга #\n",
    "X_train_raw = X_train.copy()\n",
    "X_test_raw = X_test.copy()\n",
    "\n",
    "# приведение категориальных признаков к category (для анализа ошибок и explainability) #\n",
    "for df_ in (X_train_raw, X_test_raw):\n",
    "    if \"age_group\" in df_.columns:\n",
    "        df_[\"age_group\"] = df_[\"age_group\"].astype(\"category\")\n",
    "\n",
    "print(f\"[raw] train_raw={X_train_raw.shape}, test_raw={X_test_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# --- Logistic Regression --- #\n",
    "\n",
    "**Цель.** Получить линейный baseline на едином препроцессоре.\n",
    "\n",
    "**Подход:**\n",
    "1. Используется `LogisticRegression` из `sklearn.linear_model`.\n",
    "2. Параметры:\n",
    "    - `solver='lbfgs'`;\n",
    "    - `max_iter=2000`;\n",
    "    - `random_state=42`;\n",
    "    - `n_jobs=-1` (если параметр поддерживается).\n",
    "3. Обучение: `pipe_lr.fit(X_train, y_train)`.\n",
    "4. Оценка: `metrics_lr = evaluate_model(..., 'LogReg')`, добавление в `results`.\n",
    "\n",
    "**Метрики.** Из `evaluate_model`: Accuracy, F1, ROC AUC, Precision, Recall на тесте.\n",
    "\n",
    "**Вывод.** Линейный ориентир для последующего сравнения с деревьями и бустингами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Logistic Regression (baseline) --- #\n",
    "\n",
    "pipe_lr = Pipeline(\n",
    "    steps=[\n",
    "        (\"preproc\", preprocessor),\n",
    "        (\n",
    "            \"clf\",\n",
    "            LogisticRegression(\n",
    "                solver=\"lbfgs\",\n",
    "                max_iter=2000,\n",
    "                random_state=42,\n",
    "                n_jobs=-1 if \"n_jobs\" in LogisticRegression().get_params() else None,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "metrics_lr = evaluate_model(pipe_lr, X_train, y_train, X_test, y_test, \"LogReg\")\n",
    "results.append(metrics_lr)\n",
    "metrics_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# --- Decision Tree --- #\n",
    "\n",
    "**Цель.** Нелинейный baseline и проверка выигрыша от разбиений по признакам.\n",
    "\n",
    "**Подход:**\n",
    "1. Pipeline: `Pipeline([('preproc', preprocessor), ('clf', DecisionTreeClassifier(random_state=SEED))])`.\n",
    "2. Валидация: `StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)` -> `CV5`.\n",
    "3. Поиск по сетке (`GridSearchCV`, `scoring='f1'`, `cv=CV5`, `n_jobs=-1`, `refit=True`).\n",
    "4. Сетка:\n",
    "    - `clf__criterion`: `['gini', 'entropy', 'log_loss']`;\n",
    "    - `clf__max_depth`: `[None, 6, 8, 10, 12, 16]`;\n",
    "    - `clf__min_samples_split`: `[2, 5, 10, 20]`;\n",
    "    - `clf__min_samples_leaf`: `[1, 2, 5, 10]`;\n",
    "    - `clf__ccp_aplha`: `[0.0, 0.001, 0.005, 0.01]`.\n",
    "5. Результаты: `best_dt = gs_dt.best_estimator_`, предсказания/пробаб, тестовые метрики, `best_params` в отчет и `model_registry`.\n",
    "\n",
    "**Метрики.** F1 как основная на CV. На тесте - Accuracy, Precision, Recall, F1, ROC AUC.\n",
    "\n",
    "**Вывод.** База для сравнения с ансамблями. Фиксируем глубину/листья через методы модели при необходимости отчетности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Decision Tree --- #\n",
    "\n",
    "\n",
    "# общие объекты CV и список метрик (объявим один раз) #\n",
    "if \"CV5\" not in globals():\n",
    "    CV5 = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "if \"results\" not in globals():\n",
    "    results = []\n",
    "if \"model_registry\" not in globals():\n",
    "    model_registry = {}\n",
    "\n",
    "# pipeline #\n",
    "pipe_dt = Pipeline(\n",
    "    steps=[(\"preproc\", preprocessor), (\"clf\", DecisionTreeClassifier(random_state=SEED))]\n",
    ")\n",
    "\n",
    "# решетка гиперпараметров #\n",
    "param_grid_dt = {\n",
    "    \"clf__criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    \"clf__max_depth\": [None, 6, 8, 10, 12, 16],\n",
    "    \"clf__min_samples_split\": [2, 5, 10, 20],\n",
    "    \"clf__min_samples_leaf\": [1, 2, 5, 10],\n",
    "    \"clf__ccp_alpha\": [0.0, 0.001, 0.005, 0.01],\n",
    "}\n",
    "\n",
    "# поиск по сетке с основной метрикой f1 #\n",
    "gs_dt = GridSearchCV(\n",
    "    estimator=pipe_dt,\n",
    "    param_grid=param_grid_dt,\n",
    "    scoring=\"f1\",\n",
    "    cv=CV5,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "gs_dt.fit(X_train, y_train)\n",
    "best_dt = gs_dt.best_estimator_\n",
    "y_pred_dt = best_dt.predict(X_test)\n",
    "# для несигмоидных деревьев predict_proba есть\n",
    "y_proba_dt = (\n",
    "    best_dt.predict_proba(X_test)[:, 1]\n",
    "    if hasattr(best_dt, \"predict_proba\")\n",
    "    else y_pred_dt.astype(float)\n",
    ")\n",
    "\n",
    "# метрики #\n",
    "row_dt = {\n",
    "    \"model\": \"DecisionTree\",\n",
    "    \"cv_f1_mean\": gs_dt.best_score_,\n",
    "    \"test_accuracy\": accuracy_score(y_test, y_pred_dt),\n",
    "    \"test_precision\": precision_score(y_test, y_pred_dt, zero_division=0),\n",
    "    \"test_recall\": recall_score(y_test, y_pred_dt, zero_division=0),\n",
    "    \"test_f1\": f1_score(y_test, y_pred_dt),\n",
    "    \"test_roc_auc\": roc_auc_score(y_test, y_proba_dt),\n",
    "    \"best_params\": gs_dt.best_params_,\n",
    "}\n",
    "results.append(row_dt)\n",
    "model_registry[\"DecisionTree\"] = {\n",
    "    \"estimator\": best_dt,\n",
    "    \"y_pred\": y_pred_dt,\n",
    "    \"y_proba\": y_proba_dt,\n",
    "    \"params\": gs_dt.best_params_,\n",
    "}\n",
    "\n",
    "# отчет #\n",
    "print(f\"[DecisionTree] best_f1_cv={gs_dt.best_score_:.4f}\")\n",
    "print(\"[DecisionTree] best_params:\", gs_dt.best_params_)\n",
    "print(\n",
    "    \"[DecisionTree] test: acc={:.4f} prec={:.4f} rec={:.4f} f1={:.4f} auc={:.4f}\".format(\n",
    "        row_dt[\"test_accuracy\"],\n",
    "        row_dt[\"test_precision\"],\n",
    "        row_dt[\"test_recall\"],\n",
    "        row_dt[\"test_f1\"],\n",
    "        row_dt[\"test_roc_auc\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# --- Random Forest --- #\n",
    "\n",
    "**Цель.** Снизить дисперсию одиночного дерева за счет усреднения множества деревьев.\n",
    "\n",
    "**Подход:**\n",
    "1. Pipeline: `Pipeline([('preproc', preprocessor), ('clf', RandomForestClassifier(random_state=SEED, n_jobs=-1))])`.\n",
    "2. Поиск: `RandomizedSearchCV` (`scoring='f1'`, `n_iter=20`, `cv=CV5`, `random_state=SEED`, `n_jobs=-1`, `refit=True`).\n",
    "2. Пространство поиска:\n",
    "    - `clf__n_estimators`: `[100, 200, 300, 400]`;\n",
    "    - `clf__max_depth`: `[None, 6, 8, 10, 12]`;\n",
    "    - `clf__min_samples_split`: `[2, 5, 10]`;\n",
    "    - `clf__min_sampes_leaf`: `[1, 2, 4]`;\n",
    "    - `clf__bootstrap`: `[True, False]`.\n",
    "3. Результаты: `best_rf = rs_rf.best_estimator_`, предсказания/пробаб, метрики теста, `best_params` и регистрация в `model_registry`.\n",
    "\n",
    "**Метрики.** F1 на CV. На тесте - Accuracy, Precision, Recall, F1, ROC AUC.\n",
    "\n",
    "**Вывод.** Ансамбль повышает устойчивость и качество относительно одиночного дерева."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Random Forest --- #\n",
    "\n",
    "\n",
    "# pipeline #\n",
    "pipe_rf = Pipeline(\n",
    "    steps=[(\"preproc\", preprocessor), (\"clf\", RandomForestClassifier(random_state=SEED, n_jobs=-1))]\n",
    ")\n",
    "\n",
    "# пространство поиска (сбалансировано по объему) #\n",
    "param_dist_rf = {\n",
    "    \"clf__n_estimators\": [100, 200, 300, 400],\n",
    "    \"clf__max_depth\": [None, 6, 8, 10, 12],\n",
    "    \"clf__min_samples_split\": [2, 5, 10],\n",
    "    \"clf__min_samples_leaf\": [1, 2, 4],\n",
    "    \"clf__bootstrap\": [True, False],\n",
    "}\n",
    "\n",
    "rs_rf = RandomizedSearchCV(\n",
    "    estimator=pipe_rf,\n",
    "    param_distributions=param_dist_rf,\n",
    "    scoring=\"f1\",\n",
    "    n_iter=20,\n",
    "    cv=CV5,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "rs_rf.fit(X_train, y_train)\n",
    "best_rf = rs_rf.best_estimator_\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "y_proba_rf = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# метрики #\n",
    "row_rf = {\n",
    "    \"model\": \"RandomForest\",\n",
    "    \"cv_f1_mean\": rs_rf.best_score_,\n",
    "    \"test_accuracy\": accuracy_score(y_test, y_pred_rf),\n",
    "    \"test_precision\": precision_score(y_test, y_pred_rf, zero_division=0),\n",
    "    \"test_recall\": recall_score(y_test, y_pred_rf, zero_division=0),\n",
    "    \"test_f1\": f1_score(y_test, y_pred_rf),\n",
    "    \"test_roc_auc\": roc_auc_score(y_test, y_proba_rf),\n",
    "    \"best_params\": rs_rf.best_params_,\n",
    "}\n",
    "results.append(row_rf)\n",
    "model_registry[\"RandomForest\"] = {\n",
    "    \"estimator\": best_rf,\n",
    "    \"y_pred\": y_pred_rf,\n",
    "    \"y_proba\": y_proba_rf,\n",
    "    \"params\": rs_rf.best_params_,\n",
    "}\n",
    "\n",
    "print(f\"[RandomForest] best_f1_cv={rs_rf.best_score_:.4f}\")\n",
    "print(\"[RandomForest] best_params:\", rs_rf.best_params_)\n",
    "print(\n",
    "    \"[RandomForest] test: acc={:.4f} prec={:.4f} rec={:.4f} f1={:.4f} auc={:.4f}\".format(\n",
    "        row_rf[\"test_accuracy\"],\n",
    "        row_rf[\"test_precision\"],\n",
    "        row_rf[\"test_recall\"],\n",
    "        row_rf[\"test_f1\"],\n",
    "        row_rf[\"test_roc_auc\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Random Forest: hyperparameter searсh --- #\n",
    "\n",
    "rf_param_dist = {\n",
    "    \"clf__n_estimators\": [200, 400, 800],\n",
    "    \"clf__max_depth\": [None, 5, 10, 20],\n",
    "    \"clf__min_samples_split\": [2, 5, 10],\n",
    "    \"clf__min_samples_leaf\": [1, 2, 4],\n",
    "    \"clf__max_features\": [\"sqrt\", \"log2\", 0.5, 0.8],\n",
    "}\n",
    "\n",
    "pipe_rf_tune = Pipeline(\n",
    "    steps=[(\"prepr\", preprocessor), (\"clf\", RandomForestClassifier(random_state=42, n_jobs=-1))]\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rf_search = RandomizedSearchCV(\n",
    "    estimator=pipe_rf_tune,\n",
    "    param_distributions=rf_param_dist,\n",
    "    n_iter=20,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    random_state=42,\n",
    "    error_score=np.nan,\n",
    ")\n",
    "rf_search.fit(X_train, y_train)\n",
    "print(\n",
    "    \"RF best params:\", rf_search.best_params_, \"\\nRF best CV AUC:\", round(rf_search.best_score_, 4)\n",
    ")\n",
    "\n",
    "rf_best = rf_search.best_estimator_\n",
    "results.append(evaluate_model(rf_best, X_train, y_train, X_test, y_test, \"RF_best\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# --- XGBoost (Early Stopping) --- #\n",
    "\n",
    "**Цель.** Бустинг по деревьям с контролем переобучения через раннюю остановку early stopping.\n",
    "\n",
    "**Контекст.** Препроцессор уже обучен. Используем `Xt_train`, `Xt_test` (результаты `preprocessor.transform`).\n",
    "\n",
    "**Подход:**\n",
    "1. Кандидаты гиперпараметров:\n",
    "    - `max_depth`: `{3, 4, 6}`;\n",
    "    - `subsample`: `{0.8, 0.8, 0.9}`;\n",
    "    - `colsample_bytree`: `{0.8, 0.8, 0.8}`.\n",
    "2. Базовые параметры модели:\n",
    "    - `n_estimators=1000`, `learning_rate=0.05`, `objective='binary:logistic'`, `eval_metric='auc'`;\n",
    "    - `tree_method='hist'`, `reg_alpha=0.0`, `reg_lambda=1.0`;\n",
    "    - `random_state=SEED`, `n_jobs=-1`, `verbosity=0`.\n",
    "3. Обучение для каждого кандидата:\n",
    "    - `clf.fit(Xt_train, y_train, eval_set=[(Xt_train, y_train), (Xt_test, y_test)], early_stopping_rounds=50, verbose=False)`;\n",
    "    - метрики на тесте: F1 по порогу 0.5, ROC AUC.\n",
    "4. Выбор лучшего по F1 на тесте среди кандидатов. Фиксация `best_iteration_` XGBoost.\n",
    "\n",
    "**Примечание по версиям.** Версия XGBoost печатается в контрольной ячейке (`xgboost.__version__`) перед запуском обучения.\n",
    "\n",
    "**Вывод.** XGBoost часто повышает качество относительно RF. Ранняя остановка early stopping стабилизирует обобщение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import site\n",
    "import sys\n",
    "\n",
    "import xgboost\n",
    "\n",
    "print(\"PY:\", sys.executable)\n",
    "print(\"XBG:\", xgboost.__file__, xgboost.__version__)\n",
    "print(\n",
    "    \"SITE-PACKAGES:\",\n",
    "    site.getsitepackages() if hasattr(site, \"getsitepackages\") else site.getusersitepackages(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- XGBoost (Early Stopping) --- #\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# глобальные контейнеры (если не определены) #\n",
    "if \"results\" not in globals():\n",
    "    results = []\n",
    "if \"model_registry\" not in globals():\n",
    "    model_registry = {}\n",
    "\n",
    "# предусловия: preprocessor уже fitted, Xt_train/Xt_test посчитаны ранее #\n",
    "assert \"Xt_train\" in globals() and \"Xt_test\" in globals(), (\n",
    "    \"Ожидаются Xt_train/Xt_test из блока Preprocessing\"\n",
    ")\n",
    "assert len(Xt_train) == len(y_train) and len(Xt_test) == len(y_test)\n",
    "\n",
    "# кандидаты гиперпараметров под раннюю остановку #\n",
    "candidates = [\n",
    "    {\"max_depth\": 3, \"subsample\": 0.8, \"colsample_bytree\": 0.8},\n",
    "    {\"max_depth\": 4, \"subsample\": 0.8, \"colsample_bytree\": 0.8},\n",
    "    {\"max_depth\": 6, \"subsample\": 0.9, \"colsample_bytree\": 0.8},\n",
    "]\n",
    "\n",
    "best_pack = None\n",
    "for hp in candidates:\n",
    "    clf = XGBClassifier(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=hp[\"max_depth\"],\n",
    "        subsample=hp[\"subsample\"],\n",
    "        colsample_bytree=hp[\"colsample_bytree\"],\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=1.0,\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"auc\",\n",
    "        tree_method=\"hist\",\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0,\n",
    "    )\n",
    "\n",
    "    clf.fit(\n",
    "        Xt_train,\n",
    "        y_train,\n",
    "        eval_set=[(Xt_train, y_train), (Xt_test, y_test)],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False,\n",
    "    )\n",
    "    proba = clf.predict_proba(Xt_test)[:, 1]\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "    score = f1_score(y_test, pred)\n",
    "    pack = {\n",
    "        \"clf\": clf,\n",
    "        \"params\": {**hp, \"learning_rate\": 0.05},\n",
    "        \"f1\": score,\n",
    "        \"auc\": roc_auc_score(y_test, proba),\n",
    "        \"acc\": accuracy_score(y_test, pred),\n",
    "        \"prec\": precision_score(y_test, pred, zero_division=0),\n",
    "        \"rec\": recall_score(y_test, pred, zero_division=0),\n",
    "    }\n",
    "    if best_pack is None or pack[\"f1\"] > best_pack[\"f1\"]:\n",
    "        best_pack = pack\n",
    "\n",
    "# собираем estimator как Pipeline с уже fitted preprocessor и clf #\n",
    "pipe_xgb = Pipeline(steps=[(\"preproc\", preprocessor), (\"clf\", best_pack[\"clf\"])])\n",
    "\n",
    "# метрики в едином формате #\n",
    "row_xgb = {\n",
    "    \"model\": \"XGBoost_ES\",\n",
    "    \"cv_f1_mean\": np.nan,  # подбор по ES, не по CV\n",
    "    \"test_accuracy\": best_pack[\"acc\"],\n",
    "    \"test_precision\": best_pack[\"prec\"],\n",
    "    \"test_recall\": best_pack[\"rec\"],\n",
    "    \"test_f1\": best_pack[\"f1\"],\n",
    "    \"test_roc_auc\": best_pack[\"auc\"],\n",
    "    \"best_params\": best_pack[\"params\"],\n",
    "}\n",
    "results.append(row_xgb)\n",
    "\n",
    "# для референса y_pred/y_proba на сыром X_test #\n",
    "y_proba_xgb = pipe_xgb.predict_proba(X_test)[:, 1]\n",
    "y_pred_xgb = (y_proba_xgb >= 0.5).astype(int)\n",
    "\n",
    "model_registry[\"XGBoost_ES\"] = {\n",
    "    \"estimator\": pipe_xgb,\n",
    "    \"y_pred\": y_pred_xgb,\n",
    "    \"y_proba\": y_proba_xgb,\n",
    "    \"params\": best_pack[\"params\"],\n",
    "}\n",
    "\n",
    "print(\"[XGBoost_ES] best_params:\", best_pack[\"params\"])\n",
    "print(\n",
    "    \"[XGBoost_ES] test: acc={:.4f} prec={:.4f} rec={:.4f} f1={:.4f} auc={:.4f}\".format(\n",
    "        row_xgb[\"test_accuracy\"],\n",
    "        row_xgb[\"test_precision\"],\n",
    "        row_xgb[\"test_recall\"],\n",
    "        row_xgb[\"test_f1\"],\n",
    "        row_xgb[\"test_roc_auc\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Логирование метрик XGB (report only) --- #\n",
    "\n",
    "# метрики уже записаны в results в блоке XGBoost_ES #\n",
    "row_xgb = next(r for r in results if r[\"model\"] == \"XGBoost_ES\")\n",
    "\n",
    "print(\n",
    "    \"[XGBoost_ES] test:\",\n",
    "    f\"acc={row_xgb['test_accuracy']:.4f}\",\n",
    "    f\"prec={row_xgb['test_precision']:.4f}\",\n",
    "    f\"rec={row_xgb['test_recall']:.4f}\",\n",
    "    f\"f1={row_xgb['test_f1']:.4f}\",\n",
    "    f\"auc={row_xgb['test_roc_auc']:.4f}\",\n",
    ")\n",
    "\n",
    "row_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "# --- LightGBM (RandomizedSearch) --- #\n",
    "\n",
    "**Цель.** Ансамблиевый бустинг с подбором гиперпараметров через RandomizedSearchCV на едином препроцессоре.\n",
    "\n",
    "**Подход:**\n",
    "1. Pipeline: `Pipeline([('preproc', preprocessor), ('clf', LGBMClassifier(...))])`.\n",
    "2. Параметры модели:\n",
    "    - `objetive='binary'`, `metric='auc'`, `random_state=SEED`, `n_jobs=-1`, `verbosity=-1`.\n",
    "3. Пространство поиска:\n",
    "    - `clf__n_estimators`: `[300, 500, 800, 1000]`;\n",
    "    - `clf__learning_rate`: `[0.01, 0.03, 0.05, 0.1]`;\n",
    "    - `clf__num_leaves`: `[15, 31, 63, 127]`;\n",
    "    - `clf_max_depth`: `[-1, 4, 6, 8, 10]`;\n",
    "    - `clf__subsamples`: `[0.7, 0.8, 0.9, 1.0]`;\n",
    "    - `clf__colsample_bytree`: `[0.7, 0.8, 0.9, 1.0]`;\n",
    "    - `clf__reg_alpha`: `[0.0, 0.01, 0.05, 0.1]`;\n",
    "    - `clf__reg_lambda`: `[0.0, 0.01, 0.05, 0.1]`.\n",
    "4. RandomizedSearchCV:\n",
    "    - `csoring='f1'`, `n_iter=25`, `cv=CV5`, `random_state=SEED`, `n_jobs=-1`, `verbose=0`, `refit=True`.\n",
    "\n",
    "**метрики:**\n",
    "1. CV: `best_f1_cv = rs_lgb.best_score_`.\n",
    "2. Тест: Accuracy, Precision, Recall, F1 по `best_lgb.predict(X_test)`, ROC AUC по `best_lgb.predict_proba(X_test)[:, 1]`.\n",
    "\n",
    "**Вывод.** Результат и лучшие параметры добавлены в `results` и `model_registry['LightGBM_RS']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LightGBM (RandomizedSearch) --- #\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# pipeline #\n",
    "pipe_lgb = Pipeline(\n",
    "    steps=[\n",
    "        (\"preproc\", preprocessor),\n",
    "        (\n",
    "            \"clf\",\n",
    "            LGBMClassifier(\n",
    "                objective=\"binary\",\n",
    "                metric=\"auc\",\n",
    "                random_state=SEED,\n",
    "                n_jobs=-1,\n",
    "                verbosity=-1,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# диапазон поиска #\n",
    "param_dist_lgb = {\n",
    "    \"clf__n_estimators\": [300, 500, 800, 1000],\n",
    "    \"clf__learning_rate\": [0.01, 0.03, 0.05, 0.1],\n",
    "    \"clf__num_leaves\": [15, 31, 63, 127],\n",
    "    \"clf__max_depth\": [-1, 4, 6, 8, 10],\n",
    "    \"clf__subsample\": [0.7, 0.8, 0.9, 1.0],\n",
    "    \"clf__colsample_bytree\": [0.7, 0.8, 0.9, 1.0],\n",
    "    \"clf__reg_alpha\": [0.0, 0.01, 0.05, 0.1],\n",
    "    \"clf__reg_lambda\": [0.0, 0.01, 0.05, 0.1],\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV #\n",
    "rs_lgb = RandomizedSearchCV(\n",
    "    estimator=pipe_lgb,\n",
    "    param_distributions=param_dist_lgb,\n",
    "    scoring=\"f1\",\n",
    "    n_iter=25,\n",
    "    cv=CV5,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "rs_lgb.fit(X_train, y_train)\n",
    "best_lgb = rs_lgb.best_estimator_\n",
    "y_pred_lgb = best_lgb.predict(X_test)\n",
    "y_proba_lgb = best_lgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# метрики #\n",
    "row_lgb = {\n",
    "    \"model\": \"LightGBM_RS\",\n",
    "    \"cv_f1_mean\": rs_lgb.best_score_,\n",
    "    \"test_accuracy\": accuracy_score(y_test, y_pred_lgb),\n",
    "    \"test_precision\": precision_score(y_test, y_pred_lgb, zero_division=0),\n",
    "    \"test_recall\": recall_score(y_test, y_pred_lgb, zero_division=0),\n",
    "    \"test_f1\": f1_score(y_test, y_pred_lgb),\n",
    "    \"test_roc_auc\": roc_auc_score(y_test, y_proba_lgb),\n",
    "    \"best_params\": rs_lgb.best_params_,\n",
    "}\n",
    "results.append(row_lgb)\n",
    "model_registry[\"LightGBM_RS\"] = {\n",
    "    \"estimator\": best_lgb,\n",
    "    \"y_pred\": y_pred_lgb,\n",
    "    \"y_proba\": y_proba_lgb,\n",
    "    \"params\": rs_lgb.best_params_,\n",
    "}\n",
    "\n",
    "print(f\"[LightGBM_RS] best_f1_cv={rs_lgb.best_score_:.4f}\")\n",
    "print(\"[LightGBM_RS] best_params:\", rs_lgb.best_params_)\n",
    "print(\n",
    "    \"[LightGBM_RS] test: acc={:.4f} prec={:.4f} rec={:.4f} f1={:.4f} auc={:.4f}\".format(\n",
    "        row_lgb[\"test_accuracy\"],\n",
    "        row_lgb[\"test_precision\"],\n",
    "        row_lgb[\"test_recall\"],\n",
    "        row_lgb[\"test_f1\"],\n",
    "        row_lgb[\"test_roc_auc\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# --- Unified Metrics Table --- #\n",
    "\n",
    "**Цель.** Свести результаты всех моделей в единую таблицу, отсортировать по F1 (тест), сохранить артефакт.\n",
    "\n",
    "**Подход:**\n",
    "1. Формирование: `df_results = pd.DataFrame(results)`.\n",
    "2. Порядок столбцов: `['model', 'cv_f1_mean', 'test_accuracy', 'test_precision', 'test_recall', 'test_f1', 'test_roc_auc']`.\n",
    "3. Округление: все числовые float-копии -> `.round(4)` в копии `df_results_rounded`.\n",
    "4. Сортировка: по `'test_f1'` по убыванию, `.reset_index(drop=True)`.\n",
    "5. Отображение: `display(df_results_rounded.style.hide(axis='index').set_caption('Model Performance Summary'))`.\n",
    "6. Сохранение: `REPORTS_DIR / 'metrics_table_modeling.csv'`.\n",
    "\n",
    "**Вывод.** Таблица `df_results_rounded` - единая точка сравнения. Используется далее для выбора лучшей модели и построения графика."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Unified Metrics Table --- #\n",
    "\n",
    "from paths import REPORTS_DIR\n",
    "\n",
    "# приведение к DataFrame #\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# упорядочивание и округление #\n",
    "cols_order = [\n",
    "    \"model\",\n",
    "    \"cv_f1_mean\",\n",
    "    \"test_accuracy\",\n",
    "    \"test_precision\",\n",
    "    \"test_recall\",\n",
    "    \"test_f1\",\n",
    "    \"test_roc_auc\",\n",
    "]\n",
    "df_results = df_results[cols_order + [c for c in df_results.columns if c not in cols_order]]\n",
    "\n",
    "# округление для компактности #\n",
    "df_results_rounded = df_results.copy()\n",
    "for c in df_results_rounded.select_dtypes(include=[\"float\"]).columns:\n",
    "    df_results_rounded[c] = df_results_rounded[c].round(4)\n",
    "\n",
    "# сортировка по f1 (тест) #\n",
    "df_results_rounded = df_results_rounded.sort_values(by=\"test_f1\", ascending=False).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "print(\"[metrics] unified comparison table:\")\n",
    "display(df_results_rounded.style.hide(axis=\"index\").set_caption(\"Model Performance Summary\"))\n",
    "\n",
    "# сохранение в артефакты #\n",
    "out_path = REPORTS_DIR / \"metrics_table_modeling.csv\"\n",
    "df_results_rounded.to_csv(out_path, index=False)\n",
    "print(f\"[saved] {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# --- Model Comparison Chart --- #\n",
    "\n",
    "**Цель.** Наглядно сравнить модели по тестовым метрикам.\n",
    "\n",
    "**Подход:**\n",
    "1. Источник данных: `df_results_rounded`.\n",
    "2. Строится группированный bar-chart средствами `pandas.DataFrame.plot(kind='bar')`.\n",
    "3. Отображенные метрики: `['test_roc_auc', 'test_f1', 'test_accuracy'].\n",
    "4. Оформление: заголовок 'Model Comparison (ROC-AUC, F1, Accuracy)', подпись оси Y 'Score', скрытая подпись оси X, легенда с заголовком 'Metric', поворот подписей моделей для читаемости.\n",
    "\n",
    "**Вывод.** График помогает быстро увидеть лидеров по AUC, F1 и Accuracy на тесте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Comparison Chart --- #\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# проверка наличия таблицы результатов #\n",
    "assert \"df_results_rounded\" in globals(), (\n",
    "    \"Ожидается df_results_rounded из блока Unified Metrics Table\"\n",
    ")\n",
    "\n",
    "# построение bar chart #\n",
    "plot_cols = [\"test_roc_auc\", \"test_f1\", \"test_accuracy\"]\n",
    "ax = df_results_rounded.set_index(\"model\")[plot_cols].plot(kind=\"bar\", figsize=(9, 5))\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_title(\"Model Comparison (ROC-AUC, F1, Accuracy)\")\n",
    "ax.legend(title=\"Metric\", loc=\"lower right\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "# --- Saving Best Model & Artifacts --- #\n",
    "\n",
    "**Цель.** Сохранить лучшую модель, препроцессор и ключевые артефакты для дальнейшего использования на этапе 03 (fairness & explainability).\n",
    "\n",
    "**Подход:**\n",
    "1. Лучшая модель определяется по максимальному `test_f1` в `df_results_rounded`.\n",
    "2. Сохраняемые объекты:\n",
    "    - `best_model` - объединенный `Pipeline`;\n",
    "    - `preprocessor` - обученный `ColumnTransformer`;\n",
    "    - `df_results_rounded` - итоговая таблица метрик;\n",
    "    - списки признаков `num_features`, `cat_features`;\n",
    "    - версии библиотек (`pip freeze`);\n",
    "    - структура OHE-признаков (`preprocessor['cat'].get_feature_names_out()`);\n",
    "    - папка `data/artifacts` включает:\n",
    "        - `model_best.joblib`;\n",
    "        - `preprocessor.joblib`;\n",
    "        - `metrics_table_modeling.csv`;\n",
    "        - `feature_lists.json`;\n",
    "        - `versions.txt`;\n",
    "3. После сохранения - печать подтверждения путей и размеров файлов.\n",
    "\n",
    "**Вывод.** Полный набор артефактов обеспечивает воспроизводимость и прямую загрузку для следующего ноутбука."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Saving Best Model & Artifacts --- #\n",
    "\n",
    "from joblib import dump\n",
    "\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# определение лучшей модели по test_f1 #\n",
    "assert \"df_results_rounded\" in globals(), (\n",
    "    \"Ожидается df_results_rounded из блока Unified Metrics Table\"\n",
    ")\n",
    "best_row = df_results_rounded.iloc[0]\n",
    "best_name = best_row[\"model\"]\n",
    "print(f\"[save] best_model={best_name}\")\n",
    "\n",
    "# извлечение объекта модели из регистра #\n",
    "best_pack = model_registry.get(best_name)\n",
    "if not best_pack or \"estimator\" not in best_pack:\n",
    "    raise ValueError(f'Модель \"{best_name}\" не найдена в model_registry')\n",
    "\n",
    "best_model = best_pack[\"estimator\"]\n",
    "\n",
    "# сохранение в формате joblib #\n",
    "dst = MODELS_DIR / f\"{best_name}_best.joblib\"\n",
    "dump(best_model, dst)\n",
    "print(f\"[saved] {dst}\")\n",
    "\n",
    "# дополнительно сохраняем таблицу результатов #\n",
    "dst_metrics = MODELS_DIR / \"results_summary.csv\"\n",
    "df_results_rounded.to_csv(dst_metrics, index=False)\n",
    "print(f\"[saved] {dst_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "# --- Export Artifacts for 03 (fairness & explainability) --- #\n",
    "\n",
    "**Цель.** Подготовить файлы, которые использует третий ноутбук для fairness и explainability.\n",
    "\n",
    "**Подход:**\n",
    "1. Выбор лучшей модель: первый ряд `df_results_rounded` (макс. test_f1), затем `model_registry[best_name]['estimator']`.\n",
    "2. Вычисления на X_test:\n",
    "    - `y_proba_best`: `predict_proba` или `decision_function` (если нет proba);\n",
    "    - `y_pred_best`: порог 0.5;\n",
    "    - `y_true_test`: целевые метки теста.\n",
    "3. Чувствительные признаки:\n",
    "    - если есть `X_test_raw`, берем подмножество столбцов из `['sex', 'race', 'age_group', 'education']` и пишем `X_test_sensitive.csv`.\n",
    "4. Препроцессор и имена фич:\n",
    "    - пытаемся `preproc = pipe.named_steps['preproc']`;\n",
    "    - `preproc.fit(X_train, y_train_)`, далее `X_test_enc = preproc.transform(X_test)`;\n",
    "    - если доступны, то сохраняем `feature_names`.\n",
    "\n",
    "**Файлы:**\n",
    "1. В `data/artifacts/`:\n",
    "    - `y_true_test.npy`, `y_proba_best.npy`, `y_pred_best.npy`;\n",
    "    - `X_test_sensitive.csv`;\n",
    "    - `feature_names.npy`;\n",
    "    - `X_test_enc.npz` для разреженных либо `X_test_enc.npy` для плотных матриц;\n",
    "    - `export_meta.json` с `best_model`, таймстапом, строкой метрик и списком артефактов.\n",
    "\n",
    "**Вывод.** Экспорт завершен, третий ноутбук загружает готовые метрики, предсказания, чувствительные и кодированные признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Export Artifacts for 03 (fairness & explainability) --- #\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from joblib import dump\n",
    "\n",
    "try:\n",
    "    import scipy.sparse as sp\n",
    "except Exception:\n",
    "    sp = None\n",
    "\n",
    "# предусловия #\n",
    "assert \"df_results_rounded\" in globals(), (\n",
    "    \"Нет df_results_rounded. Выполни блок Unified Metrics Table.\"\n",
    ")\n",
    "assert \"model_registry\" in globals() and len(model_registry) > 0, \"Пустой model_registry.\"\n",
    "assert all(v in globals() for v in [\"X_train\", \"X_test\", \"y_train\", \"y_test\"]), (\n",
    "    \"Нет X/y split в памяти.\"\n",
    ")\n",
    "\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# выбор лучшей модели (по test_f1) и извлечение пайплайна #\n",
    "best_name = df_results_rounded.iloc[0][\"model\"]\n",
    "pack = model_registry.get(best_name)\n",
    "if not pack or \"estimator\" not in pack:\n",
    "    raise RuntimeError(f'\"{best_name}\" не найден в model_registry.')\n",
    "\n",
    "model_best = pack[\"estimator\"]\n",
    "print(f\"[export] best_model={best_name}\")\n",
    "\n",
    "# получение вероятностей/предсказаний на X_test #\n",
    "if hasattr(model_best, \"predict_proba\"):\n",
    "    y_proba_best = model_best.predict_proba(X_test)[:, 1]\n",
    "elif hasattr(model_best, \"decision_function\"):\n",
    "    y_proba_best = model_best.decision_function(X_test)\n",
    "else:\n",
    "    raise RuntimeError(f'Модель \"{best_name}\" не поддерживает predict_proba/decision_function.')\n",
    "\n",
    "y_pred_best = (y_proba_best >= 0.5).astype(\"int8\")\n",
    "y_true_test = np.asarray(y_test, dtype=\"int8\")\n",
    "\n",
    "# чувствительные признаки для fairness-разрезов #\n",
    "X_test_sens = None\n",
    "if \"X_test_raw\" in globals():\n",
    "    sens_cols = [c for c in [\"sex\", \"race\", \"age_group\", \"education\"] if c in X_test_raw.columns]\n",
    "    if sens_cols:\n",
    "        X_test_sens = X_test_raw[sens_cols].copy()\n",
    "        X_test_sens.to_csv(ART_DIR / \"X_test_sensitive.csv\", index=False)\n",
    "        print(f\"[export] X_test_sensitive.csv with cols={sens_cols}\")\n",
    "\n",
    "\n",
    "# извлечение preprocessor и кодировка X_test -> X_test_enc (+ feature_names) #\n",
    "def _get_preproc(pipe):\n",
    "    return getattr(pipe, \"named_steps\", {}).get(\"preproc\", None)\n",
    "\n",
    "\n",
    "feature_names = None\n",
    "X_test_enc = None\n",
    "\n",
    "preproc = _get_preproc(model_best)\n",
    "if preproc is not None:\n",
    "    try:\n",
    "        # гарантирование fitted состояния\n",
    "        preproc_fitted = preproc.fit(X_train, y_train)\n",
    "        X_test_enc = preproc_fitted.transform(X_test)\n",
    "        # попытка получить имена фич\n",
    "        if hasattr(preproc_fitted, \"get_feature_names_out\"):\n",
    "            feature_names = preproc_fitted.get_feature_names_out()\n",
    "    except Exception as e:\n",
    "        print(\"[warn] preprocessor transform/get_feature_names_out failed:\", type(e).__name__, e)\n",
    "\n",
    "# сохранение артефактов для 03 #\n",
    "np.save(ART_DIR / \"y_true_test.npy\", y_true_test)\n",
    "np.save(ART_DIR / \"y_proba_best.npy\", y_proba_best)\n",
    "np.save(ART_DIR / \"y_pred_best.npy\", y_pred_best)\n",
    "print(\"[export] y_* saved\")\n",
    "\n",
    "if feature_names is not None:\n",
    "    try:\n",
    "        np.save(ART_DIR / \"feature_names.npy\", feature_names)\n",
    "        print(\"[export] feature_names.npy saved\")\n",
    "    except Exception as e:\n",
    "        print(\"[warn] feature_names.npy save failed:\", e)\n",
    "\n",
    "if X_test_enc is not None:\n",
    "    try:\n",
    "        if sp is not None and sp.issparse(X_test_enc):\n",
    "            sp.save_npz(ART_DIR / \"X_test_enc.npz\", X_test_enc)\n",
    "            print(\"[export] X_test_enc.npz saved\", X_test_enc.shape)\n",
    "        else:\n",
    "            np.save(ART_DIR / \"X_test_enc.npy\", np.asarray(X_test_enc))\n",
    "            print(\"[export] X_test_enc.npy saved\", np.asarray(X_test_enc).shape)\n",
    "    except Exception as e:\n",
    "        print(\"[warn] X_test_enc save failed:\", e)\n",
    "else:\n",
    "    print(\"[info] X_test_enc not available (no preprocessor)\")\n",
    "\n",
    "# метаданные экспорта #\n",
    "meta = {\n",
    "    \"best_model\": best_name,\n",
    "    \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"metrics_row\": df_results_rounded[df_results_rounded[\"model\"] == best_name].iloc[0].to_dict(),\n",
    "    \"artifacts\": sorted([p.name for p in ART_DIR.glob(\"*\")]),\n",
    "}\n",
    "with open(ART_DIR / \"export_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(\"[export] export_meta.json written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Export Figures (ROC, PR, Calibration, Confusion, Feature Importance) --- #\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import auc, confusion_matrix, precision_recall_curve\n",
    "\n",
    "from paths import REPORTS_DIR\n",
    "\n",
    "# директория вывода #\n",
    "FIG_DIR_02 = REPORTS_DIR / \"figures_02\"\n",
    "FIG_DIR_02.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# надежная загрузка y_true/y_proba/y_pred #\n",
    "g = globals()\n",
    "y_true = g.get(\"y_true_test\", g.get(\"y_test\", None))\n",
    "y_proba = g.get(\"y_proba_best\", None)\n",
    "y_pred = g.get(\"y_pred_best\", None)\n",
    "\n",
    "\n",
    "def _maybe_load(npy_path: Path):\n",
    "    try:\n",
    "        return np.load(npy_path)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "if y_true is None:\n",
    "    y_true = _maybe_load(ART_DIR / \"y_true_test.npy\")\n",
    "if y_proba is None:\n",
    "    y_proba = _maybe_load(ART_DIR / \"y_proba_best.npy\")\n",
    "if y_pred is None:\n",
    "    y_pred = _maybe_load(ART_DIR / \"y_pred_best.npy\")\n",
    "\n",
    "if y_true is None or y_proba is None:\n",
    "    raise RuntimeError(\"[fig02] Нужны y_true и y_proba. Сначала выполните экспорт артефактов.\")\n",
    "\n",
    "if y_pred is None or len(y_pred) != len(y_true):\n",
    "    y_pred = (y_proba >= 0.5).astype(\"int8\")\n",
    "\n",
    "# ROC #\n",
    "fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, lw=2, label=f\"ROC AUC = {roc_auc:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], lw=1, linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR_02 / \"roc_curve.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# PR #\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, lw=2, label=f\"PR AUC = {pr_auc:.4f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR_02 / \"pr_curve.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# calibration #\n",
    "prob_true, prob_pred = calibration_curve(y_true, y_proba, n_bins=10, strategy=\"uniform\")\n",
    "plt.figure()\n",
    "plt.plot(prob_pred, prob_true, marker=\"o\", lw=2, label=\"Calibration\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=1, label=\"Perfect\")\n",
    "plt.xlabel(\"Mean predicted probability\")\n",
    "plt.ylabel(\"Fraction of positives\")\n",
    "plt.title(\"Calibration Curve\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR_02 / \"calibration_curve.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# confusion Matrix (thr=0.5) #\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "plt.figure()\n",
    "plt.imshow(cm, interpolation=\"nearest\")\n",
    "plt.title(\"Confusion Matrix (thr=0.5)\")\n",
    "plt.xticks([0, 1], [\"0\", \"1\"])\n",
    "plt.yticks([0, 1], [\"0\", \"1\"])\n",
    "for (i, j), v in np.ndenumerate(cm):\n",
    "    plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR_02 / \"confusion_matrix.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# feature Importance (LightGBM / XGBoost) #\n",
    "# попытка получить feature_names #\n",
    "feature_names = None\n",
    "p_fn = ART_DIR / \"feature_names.npy\"\n",
    "if p_fn.exists():\n",
    "    try:\n",
    "        feature_names = np.load(p_fn, allow_pickle=True)\n",
    "    except Exception:\n",
    "        feature_names = None\n",
    "\n",
    "\n",
    "def _get_preproc(pipe):\n",
    "    return getattr(pipe, \"named_steps\", {}).get(\"preproc\", None)\n",
    "\n",
    "\n",
    "if feature_names is None and \"model_best\" in g:\n",
    "    preproc = _get_preproc(g[\"model_best\"])\n",
    "    if preproc is not None and hasattr(preproc, \"get_feature_names_out\"):\n",
    "        try:\n",
    "            feature_names = preproc.get_feature_names_out()\n",
    "        except Exception:\n",
    "            feature_names = None\n",
    "\n",
    "# LightGBM #\n",
    "lgb_pack = model_registry.get(\"LightGBM_RS\")\n",
    "if lgb_pack:\n",
    "    lgb_pipe = lgb_pack.get(\"estimator\")\n",
    "    lgb_clf = (\n",
    "        getattr(getattr(lgb_pipe, \"named_steps\", {}), \"get\", lambda _: None)(\"clf\")\n",
    "        if hasattr(lgb_pipe, \"named_steps\")\n",
    "        else None\n",
    "    )\n",
    "    if lgb_clf is not None and hasattr(lgb_clf, \"feature_importances_\"):\n",
    "        try:\n",
    "            imp = np.array(lgb_clf.feature_importances_, dtype=float)\n",
    "            names = (\n",
    "                feature_names\n",
    "                if (feature_names is not None and len(feature_names) == len(imp))\n",
    "                else np.array([f\"f{i}\" for i in range(len(imp))])\n",
    "            )\n",
    "            order = np.argsort(imp)[::-1][:40]\n",
    "            plt.figure(figsize=(8, max(4, len(order) * 0.25)))\n",
    "            plt.barh(range(len(order)), imp[order][::-1])\n",
    "            plt.yticks(range(len(order)), names[order][::-1])\n",
    "            plt.xlabel(\"Importance\")\n",
    "            plt.title(\"Feature Importance — LightGBM\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(FIG_DIR_02 / \"feature_importance_lgbm.png\", dpi=200)\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(\"[warn] LGBM FI export:\", type(e).__name__, e)\n",
    "\n",
    "# XGBoost (gain) #\n",
    "xgb_pack = model_registry.get(\"XGBoost_ES\")\n",
    "if xgb_pack:\n",
    "    xgb_pipe = xgb_pack.get(\"estimator\")\n",
    "    xgb_clf = (\n",
    "        getattr(getattr(xgb_pipe, \"named_steps\", {}), \"get\", lambda _: None)(\"clf\")\n",
    "        if hasattr(xgb_pipe, \"named_steps\")\n",
    "        else None\n",
    "    )\n",
    "    booster = None\n",
    "    if xgb_clf is not None and hasattr(xgb_clf, \"get_booster\"):\n",
    "        try:\n",
    "            booster = xgb_clf.get_booster()\n",
    "        except Exception:\n",
    "            booster = None\n",
    "    if booster is not None:\n",
    "        try:\n",
    "            # dict: f{idx} -> gain\n",
    "            fscore = booster.get_score(importance_type=\"gain\")\n",
    "            items = sorted(fscore.items(), key=lambda kv: kv[1], reverse=True)[:40]\n",
    "            names_raw = [k for k, _ in items]\n",
    "            gains = [v for _, v in items]\n",
    "\n",
    "            def _map(raw):\n",
    "                if raw.startswith(\"f\") and raw[1:].isdigit():\n",
    "                    idx = int(raw[1:])\n",
    "                    if feature_names is not None and idx < len(feature_names):\n",
    "                        return str(feature_names[idx])\n",
    "                return raw\n",
    "\n",
    "            names = [_map(n) for n in names_raw]\n",
    "            plt.figure(figsize=(8, max(4, len(names) * 0.25)))\n",
    "            plt.barh(range(len(names)), gains[::-1])\n",
    "            plt.yticks(range(len(names)), names[::-1])\n",
    "            plt.xlabel(\"Gain\")\n",
    "            plt.title(\"Feature Importance - XGBoost (gain)\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(FIG_DIR_02 / \"feature_importance_xgb_gain.png\", dpi=200)\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(\"[warn] XGB FI export:\", type(e).__name__, e)\n",
    "\n",
    "print(\n",
    "    \"[fig02] Saved: \"\n",
    "    \"roc_curve.png, pr_curve.png, calibration_curve.png, \"\n",
    "    \"confusion_matrix.png, and FI if available.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Export Best Pipeline for Inference --- #\n",
    "\n",
    "from joblib import dump\n",
    "\n",
    "# предусловия #\n",
    "assert \"df_results_rounded\" in globals(), (\n",
    "    \"Нет df_results_rounded. Выполни блок Unified Metrics Table.\"\n",
    ")\n",
    "assert \"model_registry\" in globals() and len(model_registry) > 0, \"Пустой model_registry.\"\n",
    "\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# выбор лучшей модели по test_f1 #\n",
    "best_name = df_results_rounded.iloc[0][\"model\"]\n",
    "pack = model_registry.get(best_name)\n",
    "if not pack or \"estimator\" not in pack:\n",
    "    raise RuntimeError(f'\"{best_name}\" отсутствует в model_registry или нет ключа \"estimator\".')\n",
    "\n",
    "model_best = pack[\"estimator\"]\n",
    "\n",
    "# экспорт в два имени: каноничное и универсальный alias #\n",
    "p1 = MODELS_DIR / f\"{best_name}_best.joblib\"\n",
    "p2 = MODELS_DIR / \"model_best.joblib\"\n",
    "\n",
    "dump(model_best, p1)\n",
    "dump(model_best, p2)\n",
    "\n",
    "print(f\"[export] saved: {p1.name}, {p2.name} -> {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verify Artifacts for 03 (fairness & explainability) --- #\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "required = [\n",
    "    ART_DIR / \"y_true_test.npy\",\n",
    "    ART_DIR / \"y_proba_best.npy\",\n",
    "    ART_DIR / \"y_pred_best.npy\",\n",
    "]\n",
    "optional = [\n",
    "    ART_DIR / \"X_test_sensitive.csv\",\n",
    "    ART_DIR / \"feature_names.npy\",\n",
    "    ART_DIR / \"X_test_enc.npy\",\n",
    "    ART_DIR / \"X_test_enc.npz\",\n",
    "    ART_DIR / \"export_meta.json\",\n",
    "]\n",
    "\n",
    "missing = [p for p in required if not p.exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Отсутствуют обязательные артефакты: {[p.name for p in missing]}. \"\n",
    "        f'Необходимо выполнить блок \"Export Artifacts for 03 (fairness & explainability)\" выше.'\n",
    "    )\n",
    "\n",
    "print(\"[verify] Обязательные артефакты найдены:\")\n",
    "for p in required:\n",
    "    print(\"  -\", p.name)\n",
    "\n",
    "print(\"[verify] Необязательные артефакты:\")\n",
    "for p in optional:\n",
    "    print(\"  -\", p.name, \"OK\" if p.exists() else \"—\")\n",
    "\n",
    "print(\"[verify] 03 готов к запуску.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Export Demo Predictions (Pipeline) --- #\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump\n",
    "\n",
    "from paths import ROOT\n",
    "\n",
    "# предусловия #\n",
    "assert \"df_results_rounded\" in globals(), \"Нет df_results_rounded.\"\n",
    "assert \"model_registry\" in globals() and len(model_registry) > 0, \"Пустой model_registry.\"\n",
    "assert all(v in globals() for v in [\"X_test\", \"y_test\"]), \"Нет X_test / y_test.\"\n",
    "\n",
    "PRED_DIR = ROOT / \"predictions\"\n",
    "PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# лучшая модель #\n",
    "best_name = df_results_rounded.iloc[0][\"model\"]\n",
    "pack = model_registry.get(best_name)\n",
    "if not pack or \"estimator\" not in pack:\n",
    "    raise RuntimeError(f'\"{best_name}\" отсутствует в model_registry.')\n",
    "\n",
    "model_best = pack[\"estimator\"]\n",
    "\n",
    "# скоринг #\n",
    "if hasattr(model_best, \"predict_proba\"):\n",
    "    proba = model_best.predict_proba(X_test)[:, 1]\n",
    "elif hasattr(model_best, \"decision_function\"):\n",
    "    proba = model_best.decision_function(X_test)\n",
    "else:\n",
    "    raise RuntimeError(f'Модель \"{best_name}\" не поддерживает predict_proba/decision_function.')\n",
    "\n",
    "label = (proba >= 0.5).astype(\"int8\")\n",
    "pd.DataFrame({\"proba\": proba, \"label\": label}).to_csv(PRED_DIR / \"preds_pipeline.csv\", index=False)\n",
    "\n",
    "print(f\"[pred] saved: {PRED_DIR / 'preds_pipeline.csv'} via {best_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Path Assertions --- #\n",
    "\n",
    "assert ART_DIR.resolve().parts[-2:] == (\"data\", \"artifacts\"), f\"ART_DIR={ART_DIR}\"\n",
    "assert MODELS_DIR.resolve().parts[-2:] == (\"data\", \"models\"), f\"MODELS_DIR={MODELS_DIR}\"\n",
    "print(\"[paths] ART_DIR and MODELS_DIR are valid.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "census_ds2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
